	Too vague.  Begs the issue of when an agent has goals, e.g. does a thermostat really have a goal.	Too long, not really a definition.	Too long, not really a definition	Goals	Not sure how to decide if something has foresight, or if that should be a defining characteristic.	Both vague and narrow	Might as well say "ability to do a lot of stuff"	
								
This is a simple definition that captures most of the work on AI	Hinges too much on "success", when success is also contingent on the capabilities (action repertoire) of the "body" which the "intelligence" is controlling.	too specific	not elegant	not sure I am crazy about the computational resource part	too vague	"best possible" is unachievable in most scenarios	still a bit vague, but it's ok	"guidance and service to humans" seems again too narrow for a definition. A definition of AI needs to be broader than that.
It defines AI as a field of research.	The abilities in one domain do not translate to abilities in another. 	Just change intelligence to consciousness and the statement becomes valid.  	Cognitive abilities might be based on humans, but not limited by them. I would take issue with the physical world aspect, since that becomes an issue with trying to define what a robot is.  	I don't even know what any of that means. A goal-based, cross-domain intelligence with efficiency measured against hardware (computational) constraints? 	In high school we learned (not learnt) that you can't define a word using its self as part of the definition. 	Rational is another way of saying "your actions will be second-guessed by a committee." And "best possible action" is a local maxim problem. We can't define  the field of AI based on the kinds of outcomes we expect of these systems. 	A single algorithm? This is the problem with a bottom-up approach to AI, trying to fit the problem with the solution already at hand.  	OMG. Unbiased guidance and service to humans... The few, older attempts at the start sounded rational, but there is a clear social progression over time. The hindrance to consensus seems less about a technical understanding of the basic research, and more about perceived implications. 
								
"Perceive" and "act" are too general, "reason" needs further clarification.	Too permissive.				I'm not sure that foresight is strictly necessary as part of a definition of intelligence.	I mostly agree with this, but it's important that "rational" isn't taken to mean "totally ignores factors that are hard to precisely quantify." "Best possible action" isn't well-defined in most domains.		More of an application than a definition.
								
	Sufficiently flexible but adaptive linguistically. 	Scientific and still clear. 				Rationality is a bad qualifier. 		
		Intelligence does not need an environment.				Is true for non-intelligent agents, too.		
This is clearly a significant part of what AI is, though short of definitive.	This is clearly a significant part of what intelligence is, though short of definitive.	This is clearly a significant part of what intelligence is, though short of definitive.	The goal of AI is at least the range of human capability.	confusing, esp. part about normalization	too vague		"matching" is too specific	"unbiased" not part of the definition of machine intelligence, nor is service to humans
This ignores the question of why an entity benefits from perceiving and reasoning.	I think this gets at the heart of what constitutes intelligent behavior: intelligence is not an end in itself but a means to an end. The end is survival.		Far from being a definition of intelligence, this sounds like a reasonable definition of the goal of artificial intelligence, without recognizing the problematic differences between human and machine "intelligence"		Ignores the fact that functioning "appropriately" here means appropriate to human needs, not the machine's needs. Machines don't have needs and we have no idea how to imbue them with needs.	What is the point of rational action?	Why does the machine care to "meet complex information-processing challenges"?	Optimal for whom? The machine or the human designing it?
Key for me is an AI should operate in a dynamic, noisy environment. This means being able to perceive that environment and reason about it. Acting is optional. It may be just analyzing and advising, but still be an AI.	See previous answer.		That looks more like AGI, so is more a subset of AI. A machine could be considered intelligent in a particular domain, without being able to work on a variety of problems. 			Not necessarily best possible action but rather sufficiently optimized action. 		Getting unbiased guidance is probably not feasible 
								
Perception, reasoning, and action is too limited of a definition.  For example we act to achieve goals and meet needs.  	Restricting the definition to 'ability' to achieve goals is not a good measure of intelligence.   For example it does not look at 'adaptability'.  A system may achieve a certain set of goals using a simple algorithm thus score 'high intelligence' but is unable to adapt to new situations.  It also does not address the time span at which goals are met: sometimes it is intelligent to 'not perceive immediate goals' for the sake of long term flourishing (meeting of all needs). It we only look at a certain time window we will see someone binge eating and using drugs as 'intelligent' due to the dopamine release and 'in the moment' goal meeting.	Each of these Each of these questions addresses a problem I had with the previous...   Anyways - the problem with this definition is that it focus too much on adaptability.  Our goal is not to 'adapt' but to 'function and adapt'. 	Intelligence should not be limited to the needs and goals of 'humans' or 'human' capabilities.  Humans are changing (in the long run) and our needs and goals will change as we evolve genetically and culturally.  There may be other intelligent species (aliens, dolphins, etc.) out there with different capabilities, just as there are different subsets of humans with different capabilities.  I do agree on the 'not needing optimal behavior'.  None of us are optimal.	This sounds like intelligence is the ability to 'improve/adapt' when they state 'relative to prior distributions'.  Intelligence is not just about adapting the current system but the system itself.  We don't say a person loses their intelligence when they converge on a computational system which doesn't adapt for a while. 		Intelligent beings are not always rational.  With humans this is a byproduct of our intelligence.  Sometimes irrationality actually leads to more intelligent system.  For example with the genetic algorithm we need to increase mutation rates and introduce sub par solutions to get out of local minimums.	There is no need for intelligence to involve 'multiple domains'.  An intelligent system can involve one or many domains.  Human intelligence involves multiple domains as we have multiple needs and goals which are complex and multi-faceted. 	Not at all.  Machine intelligence can serve no human need or goal.  We will just tend to make intelligent machines which do so.
Seems incomplete - also, intelligence is not a study. You mean the field of AI - in which case "computations" is too restricted. What about non-computational aspects?	Contemplation e.g. is not covered but is an important part of decision making. Goal-based action is an important principle nevertheless (if you want to measure intelligence or something akin to it). Learning is also not included (explicitly).	This includes learning (from experience), which I like. But I find any limitation of the concept of intelligence to rationality dissatisfying. Relativism does not really save it for me. "Working definitions" are useful I suppose but they are like definitions that are missing (crucial) limbs...	Much more like it though "computer system" is too limited - in my view to speak of "goal" must include and allow for the possibility of not computer-based systems. otherwise, computational intelligence would be what you're looking for, not AI. The hard won extension to the science of the artificial would be lost.	I cannot understand this well enough to judge it. Too complicated to be true. Frankfurt School type definition...	I like it quite a bit - minimal, doesn't bite off more than it can chew. Is it operational? Perhaps not, because the devil's word here is "with foresight" - hard to measure if not impossible. 	Too Kantian though even more minimal.	Passes the buck to complexity science - still appealing, but does not solve much. The last sub clause is interesting though - the inclusion of natural and abstract domains is inspiring.	Machines as slaves (servicing machines) - not satisfying. Very limited. "Range of circumstances" less clear than the previous definition.
			Human competence is not a necessary condition for intelligence.					
						And how to know what is the best action in a situation?	I think humans have insights. Something beyond the logic, not limited to a linear sequence of events.	
								
I agree with this, but it seems insufficient: it just pushes the problem on to the word "reason".  And if a thermostat is AI, and our goal is achieved, then I would guess your whole definitional enterprise is undermined.	Again, I agree with this, but it seems insufficient.  Where do the goals come from?  Are there also a wide range of goals?	This has a lot more words but seems to be saying even less.  I have no idea what the final sentence even means.  And the second sentence is all tautological: how could an AI use infinite resources, or pause time, or even not learn from experience.	I don't like how it depends upon "humans" in its definition, although this maybe is in fact unneeded as the following sentences don't depend upon it.  I don't know what "different types of knowledge and learning" are.  But I agree it needs "broad competence" and "optimal behavior" is a red herring.	This is a nice formalized technical definition.  The kind of thing that you could use to prove impossibility results for intelligence.  But as the one definition intended to be a guide for the field and understood outside of the field.  This can't do that.	I like the shorter definitions.  They say less things to disagree with or are tautological for the purpose of being wordy.  However they suffer from just shifting the definitional problem... in this case to the word "appropriately".	How can intelligence ignore learning and only focus on the behavior that is the goal of learning?	Another definitional shift to "common sense" (what does that mean?).  But I do like the definition if you just drop that reference altogether. 	Optimal outcomes seems unnecessary.  Guidance and service to humans is a role that machine intelligence should play, but to say it’s the definition doesn't make sense to me.
As it is not restricted to perceive, reason, and act but has got multiple perspective to it.	Even dumbest human is an intelligent being.					Best possible action might not yield results		
This definition is not particularly useful, which might be an issue of not needing a definition at all. Then again, focusing solely on 'computations' will inevitably disenfranchise good research. Interestingly enough, the latter half sounds like it’s taken from a Robotics course; it too risks putting too much focus on part of the field.	Intelligence and the ability to act upon the results of your cognitive/computational processes are two very different things. Don't disagree strongly because intelligence is related to the ability to generalize.	This definition presents an animal, and in that it is accurate, but animal intelligence might disagree with "relative rationality". 	We need machines that solve specific hard problems as good as possible. Replacing humans in their entirety in not necessarily a desirable goal. If you want to limit AI to the latter, then you have successfully excluded the vast majority of current research under the umbrella term 'AI'. 	Is this an already active subfield? Otherwise it reads as a silly relabeling of Machine Learning related metric. 	This comes close, but again this is not something most AI researchers are working towards.	Intelligence is not about rational action as broadly understood. 'Rational action' is not well defined anyway, and could risk becoming a tautology in AI research. 	"What is it?" - "Ah, it's like this thing over here?" - "What's that like?" - "Aaaah" 	All decisions are by definition biased.
		Intelligence should be more than just "adapting". Being able to define what could be an Artificial Intelligence should allow to define what should be an Artificial Madness	This definition means that we are able to find all the human cognitive capabilities. 					
								
								Too ambitious (optimality).
								
	Intelligence could imply survival which does not necessarily encapsulate the above definitions. 	Concept of knowledge is not clearly defined.						
Too high-level. Also misses key elements like learning.	By this definition none of the current successful AI systems are intelligent.	Captures the essential properties. See my definition. The processing capacity limitation is not essential.	Apples and oranges. AI and human cognition do not need to be comparable in any scientific sense.	Similar to 3 but unnecessarily more mathy.	Too vague. 	Rationality is important but there are many other factors.	Similar to 3 but unnecessarily compares to humans	"Unbiased"? with respect to what? This is not a good definition.
								
Is it really computation? Also, perceive, reason, and act seem to be an oversimplification.	How the agent comes up with its own goals is also important but neglected in this question.	Humans are irrational and good creative things can result from that. Thinking of humans as purely rational limits the potential of human.				This will make the behavior very rigid.	Is it really information processing? Define information. If it is Shannon information, it does not have "meaning", as defined by Shannon, and AI needs meaning.	
								
It is (kind of) similar to human intelligence in the sense, a person's intellect is what he/she perceives then reason and take the action to the external stimulus.								
								
This captures much of the functional goal of replicating human IQ	This links knowing the solution to implementing the solution. Achievement is distinct from IQ	Cast as a working definition, this is nicely constrained and observable	This explicitly limits itself to building a human-type intelligence. One can imagine intelligences that lack the ability to process certain kinds of information (for instance social) but they wouldn't be human like. An important problem is that this definition doesn't require the intelligence to break down in the modular fashion in which human IQ can fail.	Relative to prior is a nice constraint to highlight the information processing component of goal achievement under novelty	too vague	too demanding	 too vague	defined nicely as an artificial assistant
			A rich and complex definition.	Intelligence is well explained.	Intelligence is not well described in this definition .	This definition contains a weak explanation of intelligence.	This definition contain a rich supply of features of intelligence.	This definition privileges aiding humans; not machine intelligence in and of itself.
			It needs to complete a relative success of the task to be retried, to observe if it truly learned, and becomes more efficient in its problem solving.					
	"wide range" is ambiguous. 	This one is a little better than the others, but "rationality" is another term which has very little meaning. There are plenty of animals and humans which are very intelligent, but not necessarily rational.	This is a little bit better. But I would remove "full range". Our systems don't need 100% of all human cognitive abilities, and few people in AI are working toward that.	This definition is too narrow, and constrained to some very tiny problems in AI. Also it's very difficult to read - too jargon-heavy.	This is much better. 	Please remove rationality from your definitions. Rationality was a concept that grew in favor in the mid-20th century by theoreticians who sought to diminish human experience into a 2x2 grid. Current science recognizes this is not the case.	This is the best definition so far. This is what researchers want in AI.	This is not a good definition. There is no such thing as an unbiased machine. Furthermore, see comments above on rationality.
Some avenues of Neuroscience also study  "the computations that make it possible to perceive, reason, and act."	This is an important and critical aspect of intelligence as noted by evolutionary theory, cognitive neuroscience, etc.	I only disagree with this portion, "working with insufficient knowledge and resources." It's not clear that humans have "insufficient knowledge" for many or any tasks. Rather, humans use knowledge in a variety of different ways and different types of knowledge. 	This definition works for some types of AI systems. Ones that will work closely with humans on human robot interaction or related tasks. But it ignores a computer's ability to perform at greater than human ability on many tasks.	Limiting and too narrowly focuses on a probabilistic framework. 	Not a precise definition, but a reasonably functional one.	Humans don't act this way.	Common sense is not well defined here.	
This is what humans do for solving problems.	Changing environments is a crucial sign of intelligence	Adapting is not a sign of intelligence. Most beings are adapted but they are not intelligent.	Generating optimal behavior is highly relevant for intelligence.		Mental simulation is crucial for understanding intelligent behavior.	Decision-making is one key component of intelligence.		
This statement contains true facts about AI; it is not a complete definition but could be part of one.	This statement contains true facts about AI; it is not a complete definition but  could be part of one.	Adaptation etc. could be part of the definition, but this statement is too extreme and limiting in its whole.	This set of sentences is a grab bag of possible desiderata and a goal that is too constraining.  It's not a definition.	These statements are specific to someone's individual theory and thus too constraining.	A limp statement.  AI is not "an activity" it's a research field (of investigations) that has generated a set of methods that are now being incorporated (engineering facet of AI) into systems, etc.	True but incomplete statement.	Can have AI in limited areas, don't need "general intelligence"	true but partial
Too long to write. Contact me if you really interested. Thank you.	See above.	See above.	Subtle question involving "optimal behavior"... anyway, I'll be pleased to deepen this stuff if you really interest, as above (sorry, if you are not really interested I do not want to waste my time. otherwise, if you are, I am too).	Not a crucial point at present in my opinion.	I see a logical loop here on who defined since the beginning the meaning of "appropriately"...	This is really a deep point to deepen. I do not agree on this statement (but it is not at all absurd...).	Not at all at present, I believe they will in a (possibly near) future. By the way, humans (on average) are stupid... my hopes are more in silico rather can carbonium:)	Humans are not the center of the world. 
I like how succinct it is but it is far from testable. 	I like this as well and fits with my view. Also untestable.  		Really not a fan of restricting intelligence to the set of abilities held by humans. 	Not sure I understand this one. 	Again, too subjective. 	Best possible action is not intelligence. Maybe rational or rationalizable actions?	I just don't feel an AI will require the full suite of skills that we possess, even if each were reduced in ability, is required to be intelligent. 	This would certainly be an example of an AI. 
								
Although this is a facet of AI, it is not a defininative one. 	It is very possible to have this capability without AI.	Intelligence, in my view, has no link to finite capacity or processing time.	There is no meaning or purpose in defining AI and natural intelligence. They are exclusive of each other, and while similarities and parallels may be drawn, should be judged by entirely different criteria.		Foresight is not a defining factor of intelligence in my view.	this is simple capability, not intelligence 	It would be impressive indeed, but not really a suitable indicator.	Although simplistic, this is quite a good basic definition.
the term "computations" is too narrow, or at least it should be defined						Adaptive behavior doesn’t need to be rational		like question 7, optimal solutions don't need to be rational
computation-centric		fine except for last 2 words	optimal' as a term here is loaded		empty definition	ill-defined terms 'rational' and 'best'		
Must mention learning	Goals are formulated by humans		Anthropomorphism is silly. How about divine intelligence?	Goals are formulated by humans	What is "appropriately"?		Anthropomorphism is silly. 	
Defining in terms of abilities is the right general approach, but the list of abilities is not long enough. Now-a-days, we'd certainly include learning, for instance. What about emotions too?	Too narrow. What about more passive forms of intelligence, e.g., giving advice? Is that a form of goal achievement?	Far too narrow. It's necessary, but not sufficient. What about problem solving with full information done offline: mathematics, puzzles, board games, etc.?	Too narrow. We don't want AI systems to be limited to human-like achievements or to have to emulate them all either. 	Too narrow and badly expressed to the point of incoherence. Same criticisms as for #3.	Not bad. Very vague, of course, but perhaps that's a feature given my criticism of other definitions as being too narrow. This avoids being too narrow by being close to vacuous. 	Too narrow. Why tie to action, unless we define action so broadly as to be vacuous?	Why limit AI to matching humans? Lacking in ambition. 	Why only service to humans? Too narrow. 
								
meaningless	meaningless	meaningless	not constructive	not constructive	meaningless	meaningless	lots of fluff	
This seems more like the definition of cognitive science.	It applies to all sentient life forms equally. 			too esoteric, doesn't make any sense.	appropriate is too vague, and foresight is an advanced skill. 	the real world isn't ideal.  rational action is too vague.	common sense is a fallacy. 	no slavery please.
It is a relatively good approximation.	A good approximation as well. Slightly worse than (1).  Computation is an important part of intelligence.	I think this explains only partially what intelligence is.	A good point, but i don't see a clear definition from this.	There are intricate details that influence the process described. I think this definition leaves a lot out of the picture.	This definition seems less useful in practice than other good definitions. It says too little.	Similar to 6, questions arise about what the 'best possible action' is. In very complex environments, no one can really know what the best action is.	This seems to catch the 'essence' of intelligence more accurately than the rest. The ability to learn , reason and adapt is central to intelligence. The ability to meet complex information-processing challenges is also central to intelligence. Viewing intelligence as complex information processing is very accurate and useful.	I don't think machine intelligence should be defined as directly related to the human species. More scientific and less anthropocentric thinking has always proven to be the effective road in science and technology.
The cognitive component of intelligence is not confined to reasoning. Ultimately intelligence is a property of the physics of intelligent systems. 	Intelligence is not only about achieving goals. Most intelligent systems, including humans, often don't have explicit goals at all. 	This is still not quite right. Intelligence should not be equated with rationality. 	Not just in humans but in all animals with nervous systems. 	This is an external view of intelligence which is fine for studying it. I doubt if it will help implement AI. 		Read the behavioral economics literature!		Intelligence is not optimization or rationality. It is the physics of the brain-body system. 
AI is not just a field of study	Does not mention learning/ adaptability 	Good, but does not mention accumulation/ reuse of knowledge and skills		Good, but a bit too abstract?	AI is not an activity	Ration action is too vague. Not the best possible...	Missing real-time, interactive, limited resources (perhaps implied)	AI needs to be able to act autonomously. Also too vague.
						The best possible may be a show of emotion (faked)..		AI can promote cultural positions that are not rational. 
too imprecise as a definition (what exactly is "reasoning"?) Also, it proposes a kind of architecture (sense-plan-act) that I do not find useful				I don't really understand it (it leaves too many parameters open to count as a definition)	rather unprecise, but captures some part of AI	this relies on the definition of "rationality" and "best action", which is inapplicably in most real-life situations		same as no. 7 + "unbiased" is impossible, even for machines (e.g. data-driven methods rely on biased data, programs are biased by the programmers)
Reason + Experience x Time = Wisdom. So perception and acting on an experience with reason over time [towards some goal] possibly makes it Artificial Intelligence.	"Achieve" would need a broadest reasonable interpretation.							
Does not imply building anything, nor that it has to be very intelligent. Merely perceiving, reasoning and acting after some fashion. 	Succinct, nontrivial.	Essentially a wordy variant of definition 2. 	Too human-centric. A project aimed at achieving what dogs can do would not be AI by this definition, yet aim at largely the same goals. 	Makes the prior assumption clear,  but "real" AI will presumably be able to update it based on experience. Efficiency is a relevant factor: most definitions leave it out, yet something like Hutter's AIXI(tl) would clearly be highly intelligent while being hopelessly inefficient. 	Assumes a given environment. 	A classic view.	Rather loose and human-centric. 	Leaves too much open - rational and unbiased might be badly defined or not what achieves optimal outcomes (and what constitutes optimal is also messy).
								
								
The definition does not address the foundations.	Intelligence should provide a scale from simpler feedback devices like thermostats to more complex organisms.	The definition is not clear enough.	This definition considers only descriptive aspects but not normative ones such as being useful.	Not an awful definition but still does not provide a scale.	Decision making seems more important than just prediction.	"Best" is defined situationally by goals and resources.	Machines do not need to match humans to be considered intelligent.  Humans are only an example of intelligence.	This definition comes with bias since it does not lay out the context for decision making.
Those are just computations. Any definition of artificial intelligence should include system self-awareness.	Necessary but not sufficient. And "wide range of environments" is too vague.	This is actual event / state programming.	We don't quite understand the range of cognitive capabilities of humans, we cannot use it for a definition.	As this is a new concept, "pragmatic general intelligence", the authors can define it as their own likeness, same as we can set an axis anywhere in space.	The foresight part is interesting. But still it is too vague and incomplete.	Efficient functioning, not intelligence. That action of the intelligent agent may be embedded in its structure (for instance an efficient mechanical design) and that does not make it an intelligent agent.	Same problem as before, incomplete definition of human intelligence.	No need to call this behavior intelligence.
								
ambiguous - perceive, reason and act require definition	Good attempt - but would more closely target the reasoning component (excluding perceptual and "actability" differences) e.g. "to successfully plan goal achievement" rather than "to achieve goals"	A traditional answer, expanded upon  Deals with early pragmatic human issues but less and less relevant over time, as intelligence starts bending the environment rather than adapting to it.	A long precursor to a succinct definition that never comes.	Close but a little opaque.	is 'function' another word for 'act'.  If so then this requires embodiment which doesn't quite cross over into the artificial.	Nice, but too strong (e.g. "best possible action" - in some search spaces that could take almost as long as forever).  lose embodiment (e.g. replace "agent takes" with "agent determines"	Initially thought it was average but it grows on you.  Just about covers it all without falling into anthropomorphic issues.	Off track - intelligence is strongly linked to human.  i.e. if the agent can help a human then, as a human is intelligent, it follows that the agent must be as well .... not
Intelligence, inter alia, is the study of computations...  	It requires intelligence to formulate measurements.	Who, or what is to decide what qualifies as "sufficient", "finite", "unexpected"? How is this quantified within a relative rationality?  	Without achieving significant economies of scale, this objective seems fruitless. 		Judging by how human beings generally cope with life and disasters and abuse substances, should we conclude they have no intelligence? 	Again, if an entity took the worst possible action, is it without intelligence? Who or what decides what is "best" or "possible"?		 Your definition could equally apply to military applications as well. All is relative. Should we moralize machine intelligence?
There is more than these 3 pieces.	This ignores the important aspect of intelligence as to the picking of goals.	"Adapting" is a misnomer and leads to misunderstanding. 	This "scope" of AI goal isn't telling us much about the element of intelligence. 	If the intelligence "gets" goal and environment space, it is already intelligent...	I agree in part, but don't agree with the "Artificial intelligence is... " part. 	Like the KISS principle here - the work is in implementation, not definition. 	what is common sense?  what is reason? plan? complex information...? 	This wishful thinking has little or nothing to do with defining intelligence. 
								
								
	This is a component of intelligence, but in my opinion not the most important one.							
AI is a human-made tool, and as such humans' extended phenotype' (Haraway, 1995) to understand and solve human-made problems and issues	...the definition lacks the issue of the costs associated with achieving such goals, and the coevolutionary dynamics of the agent and its environment...	...the definition (it does not say 'artificial') misses the fact that 'agents-individuals-solutions' do not adapt, but coevolve with their environments, and it restricts itself with the term: 'processing', which it is associated to the concept of 'computation'...biology does not 'compute' in the 'traditional' Turing-machine way...also, biological (vs ''artificial') intelligence is not 'rational' in the classical sense...unless you consider coevolutionary dynamics and the need for populations (in the so-called nature's inspired computation swarm intelligence inspired e.g. on ants...) of 'solutions-agents-individuals' exploring phase spaces and multi-objective moving targets and dynamic criteria of optimization (e.g. temporal Pareto-optimality...)...	AI is at best when it is designed to complement, nor substitute (vs. "...exhibit the full range of (human) cognitive capabilities...") , human-intelligence, considering that AI is different from biological intelligence...and as human-extended phenotype, it does need to generate optimality (multi-objective, dynamic, Pareto-optimality...)		AI is the outcome of human-work aiming at develop tools to solve human-made problems and issues...in which foresight is incomplete and optimality is dynamic and multi-objective...	A definition like this must include the definition of 'rational' and 'optimal'... 	in relation to AI, the later should enhance human capabilities for solving complex problems...the above definition must exclude the concept of 'common sense' (is it scientific? is it 'rational'? is there a consensual definition of it?) and stop trying to separate 'natural' from 'abstract' (is abstract different from natural? Is there something (anything) outside nature?)...	even when designing an algorithm to serve as a platform to AI, there is a bias that is sometimes declared explicitly, defining it as 'weight' in the optimization problem to be solved, or implicitly in the design of the algorithm...
There's nothing explicit about language and the central role it plays in social and cultural constructs that enable intelligence.	Too narrow -- a lot of intelligence isn't specifically related to "goals".	Very mechanistic.	Better -- emphasizes a range of cognitive capabilities rather than intelligence as a monolithic thing.  Includes social environments as an important component.	Too focused on achieving goals.  It basically pushes down the definition of intelligence to what counts as a goal.	This would apply to lots of entities that we don't think of as "intelligent".  My cat functions appropriately in his environment and satisfies his primary goals, and even has foresight to a limited extent, but I can assure you he isn't intelligent.	Lots of people disagree with equating intelligence with an assumption of rationality and optimal action.	I like the use of "challenges" rather than "goals".  Still too narrow -- it's not just common sense.	Completely off the mark.
AI attempts to provide algorithms that model on cognitive processes	I prefer a definition that measures the duration/ automaticity of cognitive processes	Yes, I prefer to see cognitive mechanism as a processing that adapt to, and manage a finite set of environmental factors.	To design effective intervention strategies, the measurement of optimal behavior of a certain ability group would be essential.	Would there be any hidden/non-deterministic resources in humans’ general intelligence that are non-external and non-observable?	the ability to predict is an important part of human intelligence	How about emotional quotient?	I suppose there are already invention of machines that can generate speeches by combining new words, new experiences and new concepts.	How about the input itself is bias?
								
		Too Wordy 		Not layperson friendly				
Does not distinguish animal/human intelligence	It is one possible definition, does not include learning.						Unachievable	
It is also the development and creation of those computations.								
								
Encompasses the 3 possible ways to interact with the environment 				Too complicated definition		What is meant by "best"		What is meant by "optimal" - may differs from one person or situation to the other
								
AI is an interdisciplinary science whose goal is to reproduce behavior that is considered intelligent, it not only the study of computations (how to reproduce it), but also the study of intelligence in humans  and animals (what to reproduce)	"Intelligence is the ability to achieve goals...", not just a measure.		 "...full range of the cognitive capabilities we find in humans and animals." Animal intelligence is in some aspects different than human intelligence (not merely a subset of). Take the dog's social intelligence.			Intelligence and rationality are not the same. A greedy algorithm (suppose that this algorithm is "implemented in wetware") is rational, but we do not find greedy people to be intelligent in general, often quite the opposite, we view them as shortsighted. 	"Human and animal" instead of "human"	"guidance and service" have nothing to do with intelligence
Too ambitious		Disagree with essence of intelligence is ... Premise	Goal is some cognitive ability. Might exceed humans in specific tasks.			The best action, considering constraints		
								
Action is essential	One can reach goals without _understanding_	One can adapt with _understanding_	I see no added value in copying humans in this way			Optimality is one useful way to create _understanding_, in that the system will be able to explain _why_ it makes decisions		
Perceiving, reasoning and acting is already achieved by some very simple agents ... but a good start.	Too vague	practical answer	Reference to human intelligence limiting	How to define the goals? Humans are not necessarily goal based or the goals change over time. No accounting for changing goals	An agent can function perfectly well without (much) intelligence, e.g. a thermophilic bacterium in a geyser.	Good and practical.	Why "matching" and not "exceeding"? comparison to human intelligence is limiting.	This infers a use case (i.e. as an advisor to a human)
								
As humans we do it naturally, machine needed to be hard coded at first.	it should be environment measuring the intelligence of agent.					Reinforcement learning will explain it better	When I was working in salient based object detection it was kind of pain to teach machine.	Explore and Exploit mechanism serves here better
	This is just adaptability. Behavioral programming allows doing this, but it's not intelligence.		I said earlier that I differentiate between human and machine intelligence. Machine intelligence could be limited to some capabilities.		This could be achieved by any well-written program!	Intelligent people can make very stupid mistakes, yet they are still intelligent... 		This definition is too broad... my smartphone does more or less the same
This definition is sufficiently general to be true; the burden now lies on the definition of "perceive, reason, and act" on which people in the community would probably disagree...	The definition lacks the concept of "novelty". According to it, a giant look-up table containing the exact solution to a huge number of situations would be considered very intelligent. Yet I think people wouldn't consider such a reflex system intelligent indeed. What is more considered "intelligent" is to come up with a solution in a situation never encountered before (and so not in the look-up table)		I agree that solutions found by the system don't need to be optimal. On the other hand, I disagree that the concept of "intelligence" must necessarily relate to human intelligence. It would already be a big step to develop intelligent systems analogous to simple animals.	The definition lacks the concept of "novelty". According to it, a giant look-up table containing the exact solution to a huge number of situations would be considered very intelligent. Yet I think people wouldn't consider such a reflex system intelligent indeed. What is more considered "intelligent" is to come up with a solution in a situation never encountered before (and so not in the look-up table)	This definition is too vague, and one would need to define what "function appropriately and with foresight" means	Intelligence as we see it in animals (humans included) is largely not rational and rarely produces optimal solutions.	I like the mention of "common sense" and "learn" which are important for intelligence imo.	I understand that this kind of definition is shared by a part of the community but I strongly disagree with it. This sounds like machine intelligence should be different than human intelligence in nature. It has an engineering perspective which is great today to develop useful applications, but doesn't produce much insight about what are the mechanisms behind intelligence as we see it in nature.
								
	Intelligence can be defined for different domains. Some narrow, other more general		That is one goal		Statement hard to interpret		Somewhat hard to compare since the system would have other needs than humans, and would be motivated to learn other things.	
								
			I understand this definition as a definition of AI as a scientific field ("The goal [of AI] is to build..."). As this definition mentions "physical [...] environments", it would be a good definition of AI if AI were to include robotics as a sub-field. It currently does not, although I think it should (as I cannot define intelligence without a body, having no example of it in nature). 					I do not see the relevance of "guid[ing] and servic[ing] humans" in a definition of intelligence.
rather vague and basic	although not wrong, this definition is not precise enough (could relate to more than just to intelligence)	not comprehensive	rather mimicking only parts of the human behavior than creating artificial intelligence	limited to pragmatic intelligence	vague	this relates more to optimization than to intelligence	no execution	unbiased and guidance/service to humans do not fit in my opinion
								
								
			We need machines that help us, not machines that match us.					
								
Doesn't mention learning	Doesn't mention learning`	Emphasizes flexibility and learning.	A bit too human centered. 	Too utilitarian.	Decently vague	"Best possible" is too high a bar, even for people.	I think "information processing" is too limiting. Intelligence is not just about that.	The definition of intelligence in people does not require them to be helpful. Not sure why computers would be different.
perception' and 'reason' are black-boxed concepts themselves. Either we use an assumed [arbitrary] human baseline to measure those concepts, or we focus on more general measures of performance and capability.						Falls afoul to debates over rationality and 'the best possible', but sure.	Human-level is a fairly arbitrary benchmark.	It may be a relevant definition within a social sciences context, but not within a sciences context.
	The term 'goals' is not specific enough, and leaves the statement vague.	I think real-time does not have to be a criterion.	I don't think human cognitive capabilities should be the aim.				humans might be a good comparison, but will for sure not be the upper limit of intelligence.	service to humans should not be the AI's main goal as definition.
This encapsulates well what I think of as 'intelligence' generally.	achieve goals' is too vague and can refer to a thermostat.	The 'real time' restriction seems arbitrary. One could easily imagine an intelligence working on solving less urgent but still important problems in the background, slowed down.	it definitely doesn't need to include optimal behavior any more than an 'engine' needs to run on the Carnot cycle.	Thank you, Bayesian Conspiracy.	This includes thermostats again.	Ideally, yes. But this leaves out deciding on what the 'best' action is.	As you can guess, I think there's a valuable distinction between 'machine learning' and 'artificial intelligence'.	I refuse to accept any definition of AI that I would find insulting if it were applied to me.
Tries to define one concept using concepts.	Intelligence is the goal; intelligence is not the measure.				#4 above "... but it does not need to generate optimal behavior."	#4 above "... but it does not need to generate optimal behavior."	#4 above "... but it does not need to generate optimal behavior."	
			Confining "intelligence" to human intelligence is far too restrictive. My intuitive notion of intelligence includes other biological systems.			This assumes a (non-existent) consensus on what constitutes "rational" and "best possible" action.		
								
								
								
It's a good start. But simply acting or reasoning is certainly a low bar. We want to act and reason *intelligently*.	This one is a bit better, since it has a loss function that can be optimized. However, it seems like having good goals is part of intelligence, and that's left out of this definition.	Finding a way to work transcend limitations on processing capacity would be awfully intelligent!	There's no guarantee that human intelligence is the only kind. We're adapted to the kinds of problems that we faced over evolutionary history. Certainly, understanding human intelligence is *one* goal. But is it the only?	Again, this ignores the difficulty of selecting good goals. 	I think this is exactly right. I feel like it may be too vague. What's "appropriately" mean?	I'd really prefer these things to be graded. "Intelligence is the degree to which an agent takes the best possible action in a situation." Things can be more or less intelligent. That seems important. 	See previous.	
Disagree with the assumed computationalist mindset			Whose goal? Why assume one goal? Different people have different goals.			Under-defined statement, depends whether you equate 'best' with 'rational'	Anthropomorphic definition, sometimes relevant sometimes not	This is one definition relevant to one person's goals, not to another person's
								
Make it possible to create enclosing narratives	ability to navigate the world minimizing inner conflict	Intelligence of any kind is not constrained by logic		goals by themselves are marginally relevant; normalizing computing resources is an implementation tactic only	this places explicit functioning as the first class goal. I believe that internal conflict mitigation is the primary goal and actions a result.	I believe no one living would state this, even in economics.		What happened to independence and collaboration? 
"Study of computations" (to me) implies a restriction to computation, negating any other means of achieving perception, reasoning, and action.	I think measuring achievement of goals is necessary as an end-state, but perhaps not complete to how those goals are achieved	I think there's an essence of "exploring" the environment as well as adapting to it	Perhaps because of marketing, but this broader definition I've come to associate with the term "artificial general intelligence", with AI itself being a more piece-wise label of achievements and techniques along a path that could lead to AGI.		"Function appropriately is way too vague"	Rationality is applying constraints that mean we can reason about, understand (or believe we can) an external intelligence - I don't think rationality is critical to the achievement or definition, however beneficial it would be to have it.		A compass can provide "machine intelligence" by this definition today.
								
There is more to Artificial Intelligence than just the computational aspect including but not limited to finding and achieving a goal or outcome.	This statement forgets 			It considers computational constraints to make decisions, thus including basic utility in determining the outcome. 	Too simple. 	Irrational Intelligence is still intelligence. Actors may act irrationally to achieve a goal if it's a better option. 		
This definition is good as far as it goes but does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc.	This definition is good as far as it goes but does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc. It also does not specify the ability to create new goals, modify goals, reflect about goals, etc.	This definition puts too much emphasis on adapting to an environment. Human-level intelligence often modifies environments or creates new environments. This definition does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc. 	This definition is good as far as it goes but does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc. However, such features are within "the full range of the cognitive capabilities we find in humans".	This definition of "pragmatic general intelligence" falls short of the pragmatic general intelligence demonstrated by humans which should be the goal for human-level AI. This definition does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc. It also does not specify the ability to create new goals, modify goals, reflect about goals, etc.	This definition is too vague in its reference to functioning appropriately in an environment. Human-level intelligence often modifies environments or creates new environments. This definition does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc. 	This definition is too vague in its reference to "best possible action in a situation". This definition does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc. 	This definition is good as far as it goes but does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc.	This definition is too limited in restricting machine intelligence to guidance and service to humans. In the future, machine intelligence may often need to act independently of humans, and only indirectly of service to humans, e.g. in autonomously exploring the solar system, and later the stars. Also, this definition does not specify features of human-level intelligence such as natural language understanding, imagination, consciousness, etc.
								
missing aspects: planning, self-reflect	too simplistic, missing many aspects of intelligence	too many aspects missing					almost complete definition in my opinion	too limited
								
Perception, reason, and action are quite comprehensive when describing what we consider the human psyche is capable of.	The qualifier "wide" is vague. Also, theoretically at least, that ability be achieved via brute force techniques.	It covers a good range of requirements but defines the essence as adaptation. Adaptation is a very abstract activity at best when looked at from a Darwinian perspective, and may not be conductive for defining AI.	Focus on tasks and problems as opposed to awareness and choice limits the broader view of what AI could achieve.	Can apply to brute force methods. Assumes there's a linear scale of ability.	Limited range of functionality to task performance and planning.	Limited to rational action, neglecting cognitive reasoning and perception.	Covers all aspects if the language is not very precise.	Speaks more of the utility of AI to humans.
Circular definition	Too vague	Vague; circular	Knowledge and learning are not defined	Too limiting 	Circular definition	Many animals , including humans, do not make 'best' choices	How to know if machine 'matches' human?	Why limit to service of humans
	Very restrictive definition of intelligence; we can understand intelligence in the absence of goals.	Intelligence is not obviously predicated on scarcity of knowledge or resources.	This may be *a* goal, but it is one very specific goal, and I don't think it is "the" goal.				This is the best answer, only in that it is vague enough to not really say anything.	
				Is this a definition?		Typically hard to quantify 'best', makes it a bit pointless, but sure	Is this even a question or a statement? What do you mean?	Intelligence is as good as its designer. It will never be unbiased
I would generalize this: it is the study of what constitutes intelligent behavior -- where "behavior" includes any activity, physical, mental, or other -- in biological entities. One aspect of this study includes exploration using computations to attempt to replicate, simulate, or emulate this behavior, with the goal of understanding intelligence, and explicitly not producing an artificially intelligent entity.	This is only true if the definition of "goal" is suitably broad: a goal implies planning, and there are mental activities other than planning that also constitute intelligence.	This seems to imply that the goal is the production of an artificial  entity that fully emulates biological intelligence. My goal as an AI researcher over the past 35 years has been to apply human-like reasoning to specific and generally narrowly-defined application areas, not to produce an intelligent artificial entity, and I strongly disagree with the proposition that producing a _generally_ intelligent artificial entity is even possible, even if it were desirable.	My objection here, as in other cases, is to phrases like "the full range of cognitive capabilities." In fact, the author here appears to be back-pedaling later in the same quote, where he refers to "a broad range of domains," "broad competence," "a wide variety of problems," etc. These are much more reasonable characterizations of what I expect from an intelligent system.	There is no way to measure something that can't even be defined. There is not now, has never been, and IMO can never be a definition of "general intelligence,"  because whatever it is, intelligence can manifest itself in more ways than it's possible for us to enumerate. Furthermore, intelligence -- whatever it is -- can't be measured in any meaningful way:  Read The Mismeasure of Man by Stephen Jay Gould for a great discussion of this.	As I've said more than once already, the basic premise is flawed. I don't want to make marchioness intelligent: I want to emulate elements of biological intelligence in situations where I need to do so in order to solve a problem.	Humans will often demonstrate behavior that is demonstrably irrational: look at the research by Tversky and Kanheman (and others) on heuristics and biases.  Furthermore, how does one determine "the best possible action?" There are situations in which there is no clear "best," just different sorts of compromises, and which compromises are chosen are dependent on far more than intelligence as defined in any way with which I'm familiar.	There's that troublesome phrase "general intelligence" again. And once again, comparing intelligence when one can't even define the term is vacuous. Finally, I don't subscribe that "matching humans in general intelligence" should be a goal of AI: it certainly isn't mine.	This isn't an unreasonable definition, but I'd drop the qualifiers "rational," "unbiased," and "optimal." My goal would be to provide the humans in question with the kind of guidance they want, perhaps even emulating their decision style. I'd also restrict the "range of circumstances" to a particular type of task in a particular domain, and not generate expectations for more general applicability of my system. Over time, this could be generalized, but one of the primary reasons for failure of systems -- intelligent or other -- is an attempt to apply them outside the scope for which they were developed.
								
	Though I do not fully disagree with the definition, I find it too "high-level" and not useful to help defining intelligence. Indeed, it is self-defying, as it misses a definition on the criteria to measure intelligence.		No need to set human intelligence as the paradigm, IMO. OK for the rest.		Too vague.	No optimality required - unless we presume we can define a cost function which contemplates all possible criteria that direct the behavior of an intelligent entity.	Though a bit vague as well, I like the inclusion of "common sense", a key factor.	
I believe this definition states necessary but not sufficient information. For example, what is meant by "perceive" and "reason," and in what ways would an AI need to "act?"	Humans and AI's seem able to act, better or worse, in uncertain, ambiguous, conflicting or multidimensional environments. More precise definition of "environments" is needed. An AI's goals must be programmed, explicitly or implicitly, and a human's goals may be explicit or not, as well as have other characteristics that distinguish human from AI goals.    	However, "resources" constitute different things to a human and AI, at least in concrete - if not broad conceptual - terms.	This definition needs to elaborate "the full range of human cognitive capabilities." Is there a comprehensive, widely-accepted list of these? It's also doubtful whether AI's will ever match humans in all respects, although AI's already exceed humans in some.					
Artificial Intelligence should include the study of intelligent systems that operate on the same principles as human intelligence, and it is not uncontroversial that human intelligence reduces entirely to computation.		I do not see how having a finite processing capacity has anything to do with intelligence. In fact, I would consider an infinite, omniscient, omnipotent being to have maximal intelligence. Operating in real time and having the capacity to learn certainly make systems more intelligent, but I would not consider them essential either.			My alarm clock functions appropriately. 		Not very informative. 	
I partially agree because AI can study computations that *make it possible* to perceive, reason, and act under a specific paradigm or conception of implementation. But it is a category mistake to presume that advances in AI using simplified machine intelligence models ("simple" wrt. to the brain) are either indicative of brain processing or, worst of all, necessarily how the brain "works". Especially problematic is the attempt to shoehorn the brain into AI models like deep networks and ANNs, which themselves were originally derived as incredibly simplified models of neurons!	As a general criterion this fits the bill, in my view. The problem is taking a conceptual notion of "intelligence" and then instantiating it in ontological terms. Organisms with brains can exhibit intelligence, and machines can also exhibit intelligence, but just because the same word is used, does NOT mean they are the same "kinds" of intelligence!	This is a more specific and informative definition than #2 but suffers from the problem of creating too rigid of a box to fit "intelligence" in. It is difficult to define creativity as being "intelligence" using this definition, for instance, even though most people would surely agree that creativity in art and culture are signs of "intelligence". I note that the move to define "creativity" as "making large jumps through parameter space" and similar notions are myopic since they sound good but are difficult to operationalize in terms of actual acts of creativity. 	I strongly disagree not because I do not believe it impossible to build such systems, but rather that inherent in the sociocultural beliefs of workers in AI is the belief that this can and even *should* be done independently of any detailed understand of how the brain works, or even that understanding the brain will be facilitated or accelerated by first independently creating AI systems. This is putting the cart before the horse. Second, I believe it will likely be impossible to create fully capable AI systems according to this definition without a mostly complete understanding of the nervous system that has been acquired first. A history lesson is instructive here. Reading through the venerable Parallel Distributed Processing volumes published in the mid-1980s, the excitement of finally having the tools to understand the brain is palpable and infectious. Yet, after more than 30 years of progress since this volume was published, connectionism and AI have provided little to no direct contribution to understanding how the brain works, and have not produced general AI systems, a.k.a. "strong AI". Indeed, progress in AI and machine learning, though tremendous, has been strictly confined to "narrow AI" domains such as pattern recognition tasks, and are still incredibly brittle and even have no scope of application outside the narrow strictures of the domain they've been developed for.	This is a perfectly good definition of intelligence (or pragmatic intelligence, as it were) in a very specific context of Bayesian learning. It is difficult to apply this definition outside of this context, however, not only with respect to living organisms that exhibit intelligence but even other, non-Bayesian forms of AI.	This is somewhat of a tautological definition and is not informative. What is "function appropriately"? What is "foresight in its environment"? 	Underneath this definition seems the assumption that intelligence is best embodied by the character Dr. Spock in Star Trek. We have known for decades now that, first, humans are actually very bad at being "rational' agents and yet exhibit intelligent behavior. At best, one could say "rationality" is a specific type of intelligence or a separate *module* in an AI system, but much, if not most of intelligence is irational. Lower organisms (without a cerebral cortex) exhibit incredibly intelligent behavior that still surpass the abilities of all of our AI systems and robots, yet are certainly not "rational" in terms of taking the "best possible action". The notion of "satisficing" strategies as per Herbert Simon is very relevant here. Intelligence may even be characterized as finding flexible and satisficing solutions to many problems an organism may encounter, since finding the "optimal" or "best *possible*" solution is impossible, for the very least because it is never feasible to acquire enough information to make an "optimal" decision in a specific context.	Although this is the ultimate goal of machine intelligence (and I think, for that purpose, this is a great definition), it is terrible as an intermediate or even short-term goal. Machine intelligence is nowhere near meeting this definition. And yet, we cannot refuse to label AI systems today as being intelligent in some way. Thus, this definition is wholly incomplete.	Again, this is a perfectly good definition of "machine intelligence for assisting humans" but a terrible definition of machine intelligence in general. AI systems can be made to be entirely self-serving. Artificial life/AI robots that seek out sunlight to charge their batteries, for instance, are not serving anyone but themselves. Yes, they have been designed this way, but it is clear that they exhibit intelligence and simultaneously do not provide any service to humans. Thus, this definition is incomplete.
								
I would say the STUDY of AI is the study of the computations that make it possible to perceive, reason, and act.	Many goals can be achieved without great thought, like the goal of eating a meal.					Some forms of intelligence, even genius, might rely on intuition and doing the unexpected (i.e. making a disturbing piece of art) rather than the rational.	This is not a complete sentence. It is not possible to agree or disagree.	Seems like a GOAL for what machine intelligence would be used for, not a definition of machine intelligence.
Isn't reason a synonym for intelligent thought? It's like defining a beach ball as something ball shaped that you take to the beach.	Is the appreciation of Beethoven's 9th a goal? Is intelligence really only a utilitarian tool? One of the hallmarks of intelligence is the ability to go beyond utility, to imagine new things. This definition is much too narrow.	This is just a reworking of the goal oriented one to include the reality that infinite resources are impossible. Not to mention that including "rationality" in it is another way of saying intelligence is that which does intelligent things. Thanks for nothing!	Meaningless. 	This is the same as the goal oriented one only with the word "pragmatic" tacked on to make it sound useful. 	Might as well rephrase this garbage to, "Artificial intelligence is intelligent because I say it is."	Ideally, a definition of intelligence will be correct and useful, as opposed to this gibberish.	There is it again. Intelligence is common sense and the ability to reason. IOW Intelligence = intelligence. Thanks for that. I would never have thought of it myself.	No. This is a definition of an artificial servant. And it defines intelligence itself as that which is rational. By that definition, whoever wrote most of these definitions fails to qualify.
computations' is a poor word choice	Not bad, and applies to many living creatures.  Still, would like more specifics as to abilities, and how goals are achieved.	Adaptation and intelligence are two different things.			Appropriate is good.  Not sure Foresight is needed.	What is Rational?  Best action not required.  	Matching human intelligence and complex processing across a wide range of domains is asking a bit much.	
								
								
AI implies computational power used as the primary source of the intelligence.	This definition seems to apply more to AGI versus AI in general.  The use of "wide range of environments" would exclude experts (such as physicists) in one area from being considered intelligent if one of the areas evaluated was unrelated (say the legal field). 	This definition seems to limit the intelligence to artificial and undefined constraints.  For example, if I were to spend a week (or year) researching a topic to solve, this definition could consider me to not be intelligent as I did not solve it in real time.	This definition ties the AI to an interpretation of human capabilities. By this definition, animals would not be considered intelligent (although some are clever).  Additionally, there is a difference between intelligence (in the general sense) and emotional/social intelligence.	This appears to be a more general form of intelligence which allows the definition to encompass dedicated systems (game playing systems) and more broad systems (expert systems for example).	Although the answer is relatively good, it is too vague to be a clear definition.	Similar to the prior example, this one is also too vague.	This definition ties intelligence to humans and uses an unmeasurable term (common sense) that will just be too vague to be generally usable.	Removing the "unbiased" and "service to humans" would make this more of an actionable definition.  An intelligence would be biased based on its prior knowledge and why would an agent not be considered intelligent if it were just helping other agents?
I don't believe that intelligence -- natural or artificial -- can be  reduced to computation.	A chicken's intelligence can figure out how to  cross the road, but it needs legs or wings to achieve the goal of getting to the other side.  Intelligence is not enough.          		We need to divide in order to conquer			I don't understand "best".	I don't understand the question.	
Requires reason and it shouldn’t. Re. Brooks.		Too many intermingled concepts.	Anthropocentric. 	Replace "computational resources" by "energy over time"	Remove "foresight"; "appropriately" is too vague.	Needs rationality, too big a condition.	Anthropocentric.	Anthropocentric.
We could understand something without necessarily being able to build or simulate it. I think that AI as it is currently understood has to include the building of the system, not just the understanding of the human aspects of intelligence.	This essentially says that it intelligence is general. This is a very important dimensions to consider, especially in the face of the current movement to rebrand ML as AI.		I would note that we need to consider how an AI works with inputs that are not accessible to humans, e.g. learning from reading the entire web.		This misses any statement about goal formation, intention, etc. An ant can 'function appropriately'.	Adjectives like 'best' bring a certain type of judgment to the proceedings. Many agents that we consider intelligent do not make the 'best' possible actions.		
AI comprises the study of the computations that make it possible to perceive, reason, and act. Other elements would involve e.g. group action	Surely it's part of the picture, but doesn't tell us anything what it consists of.	rely on finite processing capacity -- the universe is finite, so this is trivial; "real time" -- you mean what? could swarms of robots act intelligently on cosmic scale? learning -- make sense. but the learning could be on a group level 'relative rationality' -- sound like an empty phrase	It gives concrete goal for AI research; but until proven otherwise there could be other forms/types of intelligence. 	It should be part of definition. Where's collaboration.	Not ambitious enough. E.g. artificial alligator would do. 	Adaptation sometimes requires a random action, not rational from individual agent's point of view. 	I like "natural and abstract domains.”  I don't like the focus on 'information-processing' -- this is too limiting e.g. the challenge might be to cleverly use your body, which I would put into the typical 'information processing' bucket	Too vague... what ranges do you have in mind?
Core to AI are reason, perception and action	Why focus on goal achievement?	Why limit AI artificially?		This is gobbledygook	What is "appropriately"?	What is "rational"?	Why be limited to human intelligence?	Why make AI subservient to humans?
								
		Defining intelligence in terms of rationality.				Defining intelligence in terms of rationality.		
	"Week" (i.e. field/task specific intelligence) is still intelligence		Even for humans there are many different "types" of intelligence					
This defines the AI research enterprise but it avoids trying to quantify "intelligence"	I would agree if the phrase "in a wide range of environments" were deleted. Intelligence need not be general or broad to be intelligence. We have many examples of narrow intelligence that are still "intelligent". DeepBlue or AlphaGo are clearly intelligent in the narrow sense.	The problem with this definition is again it would exclude DeepBlue or AlphaGo. If we changed the rules of Go, AlphaGo would probably fail badly (although with human help it could be retrained to do well). The point is that AlphaGo was behaving intelligently at a particular moment in time. Let's use "life" as an analogy. No one denies that dinosaurs qualified as being alive even though they were not able to adapt to subsequent climate change.	There are two problems with this definition. First, there is AI research on highly non-human forms of intelligence (e.g., theorem proving, solving giant constraint problems) that by definition is not co-extensive with human intelligent behavior. Second, "human level" literally means matching human performance but says nothing about breadth. There are many interesting research questions about how to achieve human-breadth AI, but in fact hardly anyone in AI is working on those questions.	This is pretty good. It could be applied in narrow domains simply by using a narrow prior distribution, so the adjective "general" is not needed and maybe not appropriate. Computational limits must be considered, but it is not obvious how to "normalize" by them. In my experience, we typically have a computational budget we must live within.	I very much like the first clause. The second one is too vague to be useful. It assumes that the environment is changing but in a predictable way. That is not always true.	I also like this definition, but it doesn't deal with computational limitations. It also relies on definitions of "rational" and "best possible", yet these are notoriously difficult to define.	This is a goal of many researchers. It emphasizes breadth and "human level". But  as I've argued above, we can have narrow and super-human intelligences that also count as intelligent and as forms of AI.	This is an application of AI, but it is not the definition of the field.
								
I don't think mere perception or action should qualify as artificial intelligence.	This is on the right track, but the notion of learning is missing. An agent is not intelligent if it is merely exhaustively pre-programmed to achieve its goals.	I like this answer but think it is a little heavy on moralizing about computational requirements and real-time results. An apparatus's accomplishments should be viewed in light of its size, but I would gladly accept a moon-sized, molasses-slow supercomputer as intelligent if could prove new theorems independently.	I don't think merely reproducing the existing capabilities of humans is the goal for AI.	This definition also misses the concept that ability to learn contributed to intelligence. With enough tape we could hard-code anything to satisfy the above requirements.	Intelligence is more than the capacity for survival in an environment. Slime mold is also the culmination of 1.3 billion years of evolutionary selection: is it intelligent?			
1. explicit mention of computation. 2. perception, reasoning and action are at the core of computation for individual agent success and survival. These are amenable to inclusion in definitions of both biological and synthetic intelligence	Vague and incomplete; teleological behavior is necessary but not sufficient for attribution of intelligence	The principle of "Bounded Rationality" is a fundamental building block for AI, but it is a constraint based on pragmatic considerations, not a definition.	Anthropomorphic systems are fine, but that is not inclusive of the range of all possible systems to which we would attribute intelligence. 	Too narrow and specialized.	Close.  I like "making machines intelligent" because the engineering of intelligent systems is a key differentiator of AI from philosophy and cognitive science.	Intelligence is much more than  rationality.  Reasonable intelligences may disagree on what is "best" in any particular situation.	Human intelligence is one kind of intelligence. It provides a benchmark.  The word "matching" is vague. It is task, situational and individually determined.	That is a nice requirement for an intelligent system, but not a definition.
The definition includes necessary aspects but does not sufficiently address the importance of memory and motivation.	I believe "measures" is too strong - predicts would be more to my liking.	The description is too contrived, not worth fixing.	This would seem to me to be a reasonable "initial" goal.  There is no reason that the pursuit of computationally intelligent behavior should be constrained by the limits of human intelligent behavior.	These look like somebody's textbook definitions for which I have little interest.  Performance metrics for intelligent behavior should indeed be developed, but certainly time and time limitations as well as input/output constraints and context are a few additional considerations that I believe should be explicitly included in any attempt to define "efficient" intelligent behavior.	Too narrow and incomplete a definition.	Some would equate "rational action" with "intelligent action", so the first sentence is a bit tautological to me. One could argue the truth of the second sentence as well - "best possible" probably does not mean the same to everyone.	This is a good chunk of the definition, but not complete.	Interesting - more of an operational definition than a descriptive one, still it only captures part of the concept of machine intelligence.
with the concept of human cognition,  machine is able to perceive, act the things effectively								
AI includes the mentioned but also includes more	The explicit goals are product of the intelligence, not its definition	Many non-intelligent machines may do these things. Also many non-intelligent animals.	The humans are intelligent, so I agree with the first sentence - if a machine exhibit the full range of...[...], it would be intelligent!	No, no, again goals! The intelligence is not about goals!	Reasonable, but not full definition.	Strongly disagree!	I agree with all mentioned, but there is more; this is not a full definition.	This is a definition of a robot, not of an AI.
Like: avoids harmful restriction (at the cost of allowing non-AI?). Dislike: interaction machines (as being expressive beyond a single Turing machine) are only covered when computations are perceiving and acting among each other as well as with reality. 	Goal orientation is too restrictive. I consider this to be a harmful view because (important types of) intelligence that is not goal-driven scales and integrates better (by multiple orders of magnitude). Smart cities, power grid, transport and traffic systems, ... require solutions at scale that are unattainable by goal seeking  intelligence on its own. Among others, such goal-focused intelligence keeps too much information private, preventing single-source-of-truth designs.  I agree on the "a wide range of environments" part. Coping with such a wide range is hindered by goal seeking.	Avoids excessive restriction by the wording "the essence" which leaves room for other aspects, views. Lists the shortcomings of early AI looking at real-world concerns. 	Like and dislike: it is an ultimate objective that provides direction but will never be achieved in full. Dislike: does not incite to search for a wide variety of intelligence that does not aim to achieve something specific. Does not elaborate on human cognitive capabilities along the axis "common sense and wisdom" >> "savant (autistic with specific skills)".   	Goal achievement is too restrictive in a harmful manner. 		Focus on making choices is too restrictive. This restriction is really harmful, only serves certain academic agendas. Causes scale-ability and integrate-ability issues. 	The objective that provides an aiming point that never will be reached in full. Missing: the present state of affairs only achieves savant human intelligence (autistic). Common sense is not even a key/main topic of the AI community?	Optimal is too restrictive to the extent that it is harmful. 
I have published a benchmark for intelligent agents that minimally includes these and other cognitive capabilities (Fox et al, AI Communications 2003)	Because the goal of cognitive science generally is to understand how agents can cope with this problem and to provide a unified theory to support design of such capable agents.	A general theory of intelligence should explain why and how intelligent agents fail, or produce sub-optimal performance, and guide the development of solutions to mitigate these issues. 	This could be a general goal of the field but not necessarily the goal of every practitioner or researcher.		See earlier comments.	No, this is overstated. Success criteria do not need to demand perfection but only acceptable/adequate results in the circumstances facing the agent.	I agree but its setting the bar very high, and failure risks bringing the field once again into disrepute unfairly. 	Machine intelligence INCLUDES this ability but may not involve interaction with humans in any direct way at all.
		What I like from this definition is the lack of comparisons with human beings. Artificial Intelligence should be whatever it is on its own behalf. That's good. However, I did not find the tasks (mostly problem-solving, knowledge management and learning procedures) complete enough as long as we do plainly ignore what is required to achieve true AI (in this regard, I'm more than open to the idea that other skills are required)	agreed! no optimal behavior is required but I do entirely dismiss any comparison with human intelligence. It is obsessive and unnatural to relate "artificial intelligence" to "human intelligence" and I think the sentence: planes fly but they do not do it as a birds actually captures this difference. There is no point in imitating birds as there is no point in imitating human intelligence. The objective should be serving the same goals fulfilled by human intelligence (say perceiving, reasoning and acting) but by no means in the same way we human beings do.					This is one among various services to be provided by AI
This answer is too restrictive.	It's general enough to apply to most forms of potentially intelligent systems (including human).	It's too specific and is biased towards a certain approach to AI.	Let's not constrain intelligence with human intelligence. That's much more interesting in CogSci but not AI. There is more to intelligence than human intelligence.	Again, too specific and biased.	Ditto. 	Ditto.	Again, there is more to intelligence than human intelligence. At least, I hope so.	
		Why should real time processing be necessary? It can take humans a long time to process, contemplate on, and learn from with certain situations. Similarly, for learning from experience. Humans can learn much from formal, non-experiential, channels.	I believe domain specific intelligent systems can, and should, be classed as AI.				I'm not sure AI needs to be general.	
I suggest to add perceive, reason, act and react	 modern AI focus on development and evolution within dynamic environments, 	finite capacity draws limitations to the growth of intelligence	should not only focus on human intelligence, there are other different types of intelligence like animal intelligence, nature intelligence and all other evolved forms of creatures' intelligence 	if an agent cannot achieve its goal, its success rate is very low has to be redesigned completely	it would be fantastic property even for human intelligence	it should not be limited to its own best action, we need agents that are able to improve the learning process from other agents and situations   	not enough to match human intelligence	it would be nice, but we need more than guidance
								
			Humans are also intelligent in principle. It's in fact weird to see a human acting optimally. Every human is biased by its relatively short knowledge.			As said above, humans don't normally make optimal decisions and they have been defined previously as intelligence.		
						This begs a definition of "best."		
Better if "study of the computations" were replaced with something more along the lines of "an agent being capable of".	too simplistic (yes, aware of Occam's Razor)	Good points but needs to be more succinctly phrased.	Better if we begin with "An artificially intelligent system exhibits the full range...".  	Too constraining.	Too simplistic.	First, who defines "rational action"?  Often an "intelligent act" will seem irrational.  Second, this "definition" is limited in scope to actions and situations.  ...and lastly, who defines "best possible"?	No reason, it just sits well with me.	This is perhaps more a goal than a definition.
independent of substrate	some people might interpret goals restrictively	this is only one form of intelligence	relative to humans rather than implementation-independent					
								
Many technologies that enable machine perception, reasoning, and action are no longer considered Artificial Intelligence; they are their own topics (for example, Computer Vision).	The term Intelligence, like the term Race, is a fiction based on past biases rather than objective evidence. I'd prefer to talk about behavioral complexity rather than some as-yet-unidentified internal process.	This definition is biased by current research challenges. The definition should encompass technologies that have been considered AI in the past (such as simple feedback systems or neural networks) but are no longer considered AI.	The goal of automation has always been to exhibit behaviors that humans exhibit (such as assembling the parts of a car), but exceeded human capacity in some way (for example, stamina, strength, number of errors, energy consumption).	I don't see the pragmatic value in a general definition of intelligence, either to direct academic research or to define requirements for products.	Attempts to define intelligence rather than focusing on specific complex behaviors.	Reduces the term Intelligent to simply Optimal.	Limited by today's research topics and approaches. I don't see why a machine should approach behavior in anything like the ways humans do (reasoning, planning, plan revision).	Interesting approach, but the terms Rational and Unbiased seem needlessly limiting. The definition is fine without them: I don't see why a machine should not give a human an irrational, biased guidance that enabled the human to succeed. Irrationality and bias become prominent as we move into creative endeavors - imagine a machine designed to assist human artists to explore new ideas.
At some point in operation, a system's intelligent behavior may require "higher" level "reasoning", e.g. based on moral values and principles, that sheer logic-based implementations completely seem to bypass. Therefore the definition should instead say "...computations that make it possible to perceive, reason, judge (based on higher criteria) about outcomes of multiple such reasoning outcomes and act accordingly or choose not to act".		Again, same comment as on the previous item. Intelligence must be provided with a higher layer of "emoting" approaching what we humans call "values" or "overriding moral & ethical principles".	Depending on the application this statement may be or may not be true. It may not even be desirable. On some occasions we do expect an optimal solution. On other occasions we expect the system to be capable to deal with unknown metaconditions. One cannot generalize this way.	Yes, provided the application domain is very limited and well defined. If I expect a face recognition system to classify an input file properly is one thing. But if an autonomous car must decide whether to kill a pedestrian to save my passenger's life instead of saving the pedestrian but kill me in the car by causing a horrible accident, is a completely different discussion.	Foresight here is interpreted (at least by me) as an approach to what I called earlier ability to deal with "metaconditions".	Ideally means what? Not always possible. And "best possible" means what? It depends from which angle one looks at a specific situation. Confer with the example of an autonomous car I mentioned on item 5, where the choice a human would make is morally induced and not necessarily always a rational one.	Yes, but I would skip the word "matching". Humans don't act based on reasoning only! There are higher layers of personality, ambitions, fears, aspirations, imagination, desires, beliefs, intentions, etc. that may interfere in a complex way on multiple decisions. It again requires limiting the AI definition to an applicability domain.	
To describe AI as the "study of the computations.." is too narrow, since it does not include the philosophical issues involving intelligence	I believe that intelligence is not restricted to achieving goals	Basically agree, but knowledge is not always insufficient	"Full range"  is asking too much	Definition is much too complicated.				Humans seldom use "rational" guidance to achieve optimal or suboptimal outcomes
								
self-learning and knowledge acquisition 	adaption	adaption is fine, but finite capacity and real time is too restrict	general purpose system is too big. Genetic algorithm for vertical problem is fine. 	vertical goal, don't like to include the computational resources.			adaption and reasoning, but general intelligence is too free, not specific.	MI is unbiased? Come on. Every supervised ml is biased by the labeled data.
								
"computation" might be too narrow here. I rather stick to "processes".	"goal" oriented behavior is a very narrow view of intelligence		very disembodied	narrows the definition to a particular set of tools to study the general problem, i.e. probabilities, and probably Bayesian.	vague! "function appropriately"? very subjective.	assumes "best" not circumstantial...	vague. kicks the ball further: "common sense"? what's the definition of that?	
			it is not necessary to think only about humans; many animal behaviors are of interest too					
								
I believe that understanding what computations are required to perceive, reason and act is an important part of the study of intelligence in general, though not necessarily the whole part, as there may be more (or less) to intelligence than perceiving, thinking and acting, and those processes in humans may or may not ultimately be fully describable as computation.	Achieving goals in a variety of environments is a behavior one might expect of an intelligent being, and conversely seeing a being achieve goals in a variety of environments might cause one to attribute to it intelligence. They may not be completely overlapping, however, as some attributes of intelligence (such as introspection) do not always seem related to goal-seeking, and occasionally "less intelligent" people can out-perform "more intelligent" people at tasks requiring practical application of knowledge.	Adapting to an environment with limited knowledge and resources is not necessarily the "essence of intelligence", but it is a good outcome one might expect of an intelligent system, and it this is a concrete and operationalized breakdown that would promote further study along these lines.	This is an evidence-based definition remarking on a number of properties found in humans to which we attribute intelligence, and as such has the most potential meat so far in providing avenues for research and investigation.	This is a reasonable though restricted definition of intelligence which might be useful for helping motivate problem solving or decision theoretic research; however it is likely to be too restrictive to encompass all the definitions lying behind the use of the word intelligence when applied to humans.	This hedges the definition of intelligence slightly, but remains clear, so it is even more useful as a definition of a field, as it makes the goal of the AI field clear while being open to further discoveries that would refine the specific definition of intelligence (hopefully, as an outcome of the work in the field).	This is a good description of the t	This describes the goal of AI well: creating machines that do a variety of specific things that if we observed them in humans, we would ascribe to them intelligence.	This is a myopic, application-based "definition" focused around one specific subarea of AI: mixed-initiative systems. Yes, we might say that an agent that can guide humans in a rational way is intelligent, but no-one should say that machine intelligence "is" this. A cognitive architecture that simulates twenty million neurons of the human brain to perform a pick and place task might be described as machine intelligence, but there's no notion of guidance or service in this system, as there are not in many other reasonably described systems.
I don't see the relevance of 'perception', which I'd say is inherently subjective	I can think of agents that are highly 'fit' for a wide range of environments but that would not be considered highly intelligent (insects, viruses etc.)	This seems more like I would mean by intelligent, but the definition is so broad  that it may not be useful	same as above	same as above. I'm also not sure why intelligence requires normalization 	Again, I would say that this is broad to the point of not being useful. However this definition is at least short and easy to convey	There are countless examples of irrational behaviors in humans and animals, so I don't think rationality is at the crux of it.		
this is a good start, but too vague, the problem is merely being pushed off on to the definition of perception and reasoning, even action has a huge range of possibilities.	I would add that it needs to do this fairly independently and repeatable					This bar is too high. 	This bar is too high, are dogs not intelligent? Or birds? Machine intelligence does not need to be equivalent to or similar to human intelligence. It needs to be independent and have the ability to learn and reason and plan in complex situations. 	that is one possible purpose or use of machine intelligence, it need not be the only goal
At the machine level, perception, reasoning, and acting are the major tasks that needs to be done autonomously			I think it is foolish to make machine intelligence equal to human intelligence. What we need are machines that would make life easier for humans, and it could be achieved by granting them some form of intelligence. Broad competency, I believe, is not necessary. Machines are generally meant for specific tasks. 			Rationality based on what? If by rationality you mean logic, then this kind of definition could create trouble for actions that require moral decision. 	Machine intelligence does not have to be the same as human intelligence	
								
A chess computer perceives, reasons, and acts, but calling it "intelligent" makes the term "intelligence" almost meaningless.	"Measures" is critical. If the goals are ambitious and have not been foreseen by the designer of the agent, and if the range of environments is wide, even unbounded - then, I think, I'd be ready to call the agent "intelligent". All systems in existence today would rate very low on this intelligence scale.	This definition is vague in many ways, but as I understand it, I quite like it because it emphasizes the open-endedness I brought up in my answer to the preceding question.	I disagree with the first sentence. I don't think an AI system needs to exhibit the full range of human cognitive capabilities, and I don't think we can even define this range. The rest emphasizes broadness, but appears to apply to closed, fixed domains (unless the "learning" is intended to break this limitation).	As I understand this definition, I can define a specific distribution over goal and environment space, design an agent that solves all goals in all environments that occur with >0 probability (which may be a quite narrow range), and this agent would qualify as intelligent according to this definition.	Far too vague. Many existing systems would qualify as intelligent according to this definition.	Far too vague. Many existing systems would qualify as intelligent according to this definition.	This definition, and my reaction to it, are much like Question 4.	This is more wishful thinking than a statement about what AI should be capable of. I do not think it is possible to build a machine that can provide "rational, unbiased guidance" because this would require perfect knowledge of the situation, of the available data, of the possible consequences of the advice given, etc., as well as a perfect algorithm. This is totally elusive. The best we can hope for is data-driven methods, and data (directly or indirectly) generated by humans always reflects human biases, plus more.
		too long / unclear	too long / unclear			rationality is neither necessary nor sufficient for intelligence	any definition that relies on terms like "common sense", "effective ability", "complex ..." only shifts the problem to defining those terms	
The definition incorporates concepts of embodied cognition and goes beyond brain-centered definitions of cognition		Adaptation to the environment is key concept in intelligence.		The concept of computational resources limits this definition.	Limited to prediction without the adaptive  and the behavioral components	Too mechanistic		Autonomy concept is missing
I think it is a reasonable definition	It does not capture the whole essence of the concept							
								
One can define a term. It is good to have it written. But AI term is an economic institute to create malformed technical project definitions - see its initial coinage. (In 1958 Marvin Minsky and John McCarthy founded the Artificial Intelligence Group at MIT.)	You give a definition. AI is a buzzword.	You give a definition.  AI is a buzzword.	There is non-human intelligence.	Pragmatic means " relating to matters of fact or practical affairs often to the exclusion of intellectual or artistic matters :  practical as opposed to idealistic ", see e.g. Merriam-Webster dictionary	You give definition.	You give definition.	To "match" all is not possible due to quantum physical effects.	You give definition.
This doesn't over-reach and focuses on the intended outcome.	Achieving goals is an important aspect, but "intelligence" simpliciter would also include goal-setting.	This omits goal-setting.	Pretty good, but omits task- or domain- specific systems.	Pretty good. But where do goals come from?	Nearly circular	"Best" defined how?	Again, this omits domain- or task-specific systems.	I like the inclusion of humans, but now there is little autonomy in the definition.
	Intelligence is not restricted to task completion. Btw what goals are relevant?	The reasoning/learning aspect is missing.	Intelligence is not restricted to human. AI is not restricted to computers.	Intelligence is not restricted to task completion. Btw what goals are relevant?	AI is not an activity but a field. The reasoning and acting aspects are not really exposed.	Why only one maximum? Why the maximum should be chosen? On what criteria the maximum is chosen?	Intelligence is not restricted to human beings!	Intelligence is not rationality. Human service is not at all a goal of intelligence! This is the worst definition of this list.
It misses the raison d'etre for intelligence, focusing instead on some artificially delineated anthropomorphic concepts.	The "wide range" should be restricted to physically possible environments. This definition has been (ab)used to argue that an AIXI agent is intelligent, even though it can only work in a wide range of nonsensical environments.	This goes to the crux of intelligence: handling limited resources. As soon as we imagine any resource of an agent to approach infinity, this agent can get away with performing exhaustive search, which makes redundant everything we associate with intelligence. QED	All of this is implied by Def.3 and thus redundant.	No need to bring in distributions and normalization	This makes us none the wiser	This is a definition of optimization, not intelligence.	Misses the crux: handling limited resources.	This is a definition of AI-driven tool usage, not of intelligence per se.
This definition is poor because it doesn't provide a quantitative definition of intelligence that can be measured objectively.	This definition is incomplete, because intelligence must also decide on its own goals, not just execute toward them.	This definition is poor because it lists several symptoms of intelligence (adaptation, tolerant of partial observability, learning, etc.), but does not identify a single unifying "root cause" of those symptoms.	This definition is very poor, because it is proudly human-centric and precludes -- without motivation -- the possibility of "alien" intelligences that do not resemble our own.	This definition is incomplete, because intelligence must also decide on its own goals, not just execute toward them.	This definition is exceptionally poor, because it is circular: "artificial intelligence" = "intelligence" = "appropriate functioning" + "foresight".	This definition is exceptionally poor, because it is circular: "intelligence" = "rational action" = "best possible action".	This definition is exceptionally poor, because it is circular: "intelligence" = "common sense".	This definition is exceptionally poor because it proudly defines machine intelligence as subservient to humans, precluding the possibility of machine intelligence resembling humans that are not subservient to other humans.
								
Seems clinical and a step or two removed from what AI actually is.	Reactions to changing and unpredictable situations seems to be a core element of intelligence.	Why "finite processing capacity"?			I think AI is the thing, not the activity/process involved in making the thing.	What is "best"?  It is going to depend on your point of view, goals, etc. etc.  And sometimes there is no "best". How would an AI respond to the choice made in Sophie's Choice? A choice had to be made, and both choices were equally bad.		This feels a bit friendlier to humanity than turning over all decisions to machines.
								
The 3 attributes given (perceive, reason, act) are reasonable choices. The definition is weak, however, in that it is describing attributes rather than the essential nature of intelligence.	I still agree that this is an attribute of intelligence, but this is an even weaker definition than #1 above.	The given attributes get much closer to the heart of the problem. Limited resources and partial observability are an important part of the problem description.	I disagree that humans should be the benchmark of intelligence. However, it is a worthy goal to try to match human intelligence.	The vague allusion to a probabilistic formulation aside, this definition touches on the job intelligence must do (achieve goals in an environment), which I believe approaches the more fundamental issue. The notion of working with limited resources is important.	This is a somewhat circular and weaker form of #5.	I agree that agents should act rationally. The definition would be better if it mentioned limited resources and partial observability, as #3 and #5 above do.	While I agree with the given attributes, this is again merely a list rather than an essential definition.	Whether it helps humans or not is irrelevant to the nature of intelligence.
								
								
				Too generic and vague	Distinguishes artificial intelligence from intelligence - but they are not distinct: AI is a form of intelligence.	Probably too subjective of what is optimal in such a generic description of AI		This of course entails some danger: who defines "unbiasedness"? 
								
A bit narrow and a bit unclear	Interesting but seems focused on general-purpose	The first sentence was great, not sure after that	It's not really a definition and I disagree that is the goal of AI 	Again seems focused on general-purpose AI 	Not bad but vague	Kind of saying it's all about optimization, not sure that's true	Most AI is special-purpose not general-purpose	Not sure about "optimal" but it makes sense to me
								
Missing any notion of quality of perception, reasoning, or action					Appropriately is too vague		Missing notion of data efficiency, but otherwise good	
								
Incomplete; there is more than just those three elements	Too restrictive	Too restrictive	The goal(s) may include exceeding that range	Confusing verbiage	Intelligence sometimes inhibits appropriate function	Some aspects of intelligence explain irrational action as meaningful and reasonable		Misses many aspects of "intelligence" that aren't unbiased or rational
The concepts "perceive", "reason", and "act" are not clearly defined.	This is better but you could have "intelligence" in a narrow range of environments as well. So, for example, you can have a "smart pencil sharpener" which knows how to sharpen different types of pencils. It is intelligent within its narrow ecological niche.	This definition of intelligence focuses on a particular type of intelligence.	This is an acceptable definition of "artificial intelligence" from a Strong AI perspective but I don't think it is a good definition of "artificial intelligence" as currently practiced and defined by the AI community.	Assumes a particular type of artificial intelligence.	First part of definition is circular. Second part of definition is ok if you delete "foresight".	This seems reasonable to me personally but it’s not technically the definition of artificial intelligence.	This is an acceptable definition of "artificial intelligence" from a Strong AI perspective but I don't think it is a good definition of "artificial intelligence" as currently practiced and defined by the AI community. 	Too specific.
There is no sense of scope or articulation of what qualities the reasoning and selection of action must fall within to be considered artificial intelligence.	Vague - what types of goals? What range of goals? Must this include unanticipated goals and environments?		Risks ultimately just reducing back to definition by mimicking humans...		So simple as to note really be useful - this seems to just shift the problem to defining "appropriately".		I am not sure the inclusion of the phrase "common sense" is helpful	
The word "act" is what sells it for me. I work in machine learning and describe what I do as "making machines behave appropriately in human-like situations" because I want to explicitly avoid questions of consciousness, subjectivity, and agency. Those are interesting, but orthogonal to engineering goals.	I actually don't like the word "intelligence". It's highly subjective and culturally mediated. You might as well ask whether machines can be "funny". But again, the emphasis on achieving goals makes me willing to accept the word here.	The same. I have a feeling I'm going to give all these things a "4" unless you stick in some kind of obviously specious distractor.	This one had me with everything but the phrase "we find in humans". I see explicit emulation of human beings as an AI non-goal. A bird is an animal that flies and a plane is a machine that flies, but a plane is not a giant mechanical bird. 	Sure why not?	These statements are all pretty much interchangeable to me.	Let's nitpick: is "rational" the same as "best"? There are plenty of distinctly human activities--riding a bike, playing music, arguably having a conversation--that you will mess up if you're thinking too much about what you're doing while you're doing it.		The purview probably extends beyond assisting humans, but at the moment that is a good practical area in which to focus our efforts.
								
It's good because it leaves out words like "think" that raise distracting philosophical questions.  It may be a little too broad, though; arguably every computer program fits this definition.	I think defining AI is much easier than defining intelligence.  (I like McCarthy's definition.)  Defining intelligence is full of pitfalls.  In this case, for example, a quadriplegic can be really smart but still unable to achieve even simple goals.	There was a time when computers could do really good weather predictions, but they were too slow to get the answer before the time they were predicting about.  Then computers got faster, and now they can do timely predictions.  Do you want to say that the prediction software became intelligent when computers got faster?  This definition is just way too complicated and self-defeating.	In a multiple-choice survey like this, you really can't give whole paragraphs that say several independent things as examples.  I agree that an intelligent system doesn't have to generate optimal behavior; people are a clear example of that.  I also agree that human-level intelligence is a great aspirational goal, but I'm skeptical that it can be achieved in my lifetime, and meanwhile AI software is doing all sorts of useful work.	I have no idea what this is trying to say.	It's a pretty modest definition in that it doesn't give too much detail about what counts as intelligence.  By this definition a dog is intelligent, which I think is right.  If we had an AI as good as a dog, I'd be really happy.	By this definition people aren't intelligent.  Whoever wrote it has clearly never read Freud.	This is not a complete sentence.  If the implied beginning of the sentence is "The ultimate aspirational goal of AI research is..." then I agree with it.  If the beginning is "Practical AI systems are..." then I disagree.	That "unbiased" is a red flag, and "service to humans" is another.  Service to /which/ humans?  Given the expense of building modern AI systems, they are clearly designed to provide service to /rich/ humans, e.g., owners of advertising media, banks, etc.  I /wish/ I lived in the world envisioned by whoever wrote this!
			Humans should not be the ultimate benchmark of intelligence. In my view, a definition of intelligence, if one is possible, should be void of humans.	Too involved and specific.	Seems very broad. What is "foresight"? What about learning from experience?	Rationality is part of intelligence. However, the reference to "situation" makes this definition sound quite myopic.	Yes, but remove "human" (see my earlier comment).	What is "unbiased" guidance? Is that possible? Why only service to humans? What about full autonomy?
Ok in some particular contexts (this general remark is developed in section 10). In general, no: intelligence is not (should not be defined as) a study. It should primarily viewed as a possible property of a cognitive agent.	Ok in some particular contexts (this general remark is developed in section 10). Ok also with the idea that it should be quantitatively estimated and in this sense it could indirectly "measure" an ability. In general though, no: intelligence is not (should not be defined as) the general ability to achieve (?) goals, but more specifically the ability to "learn", i.e. to improve expertise (synonyms: know-how, skills, competence, etc.), i.e. the product of knowledge (ability to deliver the right information) and speed (inverse of operational time), as experience grows . (Notice that the constraint of "wide range of environments" in above definition perhaps aims somehow at implying learning?).	I strongly agree with "The essence of intelligence is the principle of ... learn[ing] from experience". I would stop at this point (and define learning and experience elsewhere; possibly along with a more general theory of cognition). The other elements also have interesting aspects but rather weaken the first, crisp statement: -"adapting to the environment" is also quite correct but vaguer; -"insufficient knowledge" is the typical initial situation of learning systems; but hopefully not in the final state! -"resources" is vague and cannot be taken for insufficient if the system finally learns and succeeds ; and if success is not the result, intelligence is irrelevant. -"an intelligent system should rely on finite processing capacity" : yes! Cognition requires an engine, a physical infrastructure, and therefore is bound to limited processing capacity. But remaining in cognitive realm, this limit translates in "focusing", modeling reality only in goal-related aspects, of infinitesimal quantity. -"work in real time" - for ultimate results: yes! All reality, including critical threats and opportunities for the cognitive infrastructure "is",  lies in "current time" - Time is just an idea, a dimension in a model. So cognition must primarily ensure agent's survival. -"open to unexpected tasks" - I would advocate "yet unknown" rather than unexpected. In this sense, it is redundant with the idea of learning. - “... rationality” - this refers to cognition, but cognition requires a goal, and the latter depends on values, i.e. to instant threats and opportunities in real world, and therefore goals may occasionally change drastically (re. emotions), which may well look irrational,  yet is necessary for agent's survival.	Ok in some particular contexts (this general remark is developed in section 10). In general though, no: intelligence is not (should not be defined as) a “goal”; it is not either “the full range of cognitive capabilities” nor necessarily limited to “capabilities we find in humans”. “The goal is to build computer systems that exhibit the full range of the cognitive capabilities we find in humans.”  This definition is also calling for a certain “quantity” of ability/intelligence, which should be treated independently from its essence (e.g. a little water is not less water in terms of nature, than a lot of it).	Ok in some particular contexts (this general remark is developed in section 10). Ok also with the idea that it should be quantitatively estimated and in this sense it could indirectly "measure" a capability. Ok also for the idea of learning, that is implicit in “capability … to achieve … relative to prior distributions…”. In general though, no: intelligence is (should be defined as) a concept per se. This is already a very critical challenge for the current study. Sub categories (re. “pragmatic, general, efficient”) should be defined elsewhere, in a subsequent phase. (By the way what is said about efficiency looks very much ok for me; notice though that it extends beyond the imaginary world inherent in cognition, to include the physical infrastructure that makes it possible, operational; in this regard, the notion of efficiency is far from central in the concept of intelligence.)	Ok in some particular contexts (this general remark is developed in section 10). Ok for the idea that “artificial” may generally be taken as equivalent to “machine-based”; this is particularly relevant for denoting a technical infrastructure, by contrast to a human body, in matters relating to intelligence. In general, no: - intelligence is not an activity. - intelligence is then defined as a quality, but here the definition is lacking the most essential element of intelligence, which is the capability of learning. - the given definition might somehow fit for expertise (or synonyms: skill, know-how, or competence for example), in cognitive realm; but “to function appropriately … in its environment” an agent needs a lot more than intelligence, and even generally cognition:  it requires a connection to real-world (re. embedding, embodiment, infrastructure): in particular energy, perception, action, and appraisal of values, threats and opportunities.	Ok in some particular contexts (this general remark is developed in section 10). In general, no: - the given definition might somehow fit for expertise (or synonyms: skill, know-how, or competence for example), in cognitive realm. - But for “the best possible action in a situation” an agent needs a lot more than intelligence, and even generally cognition:  it requires a connection to real-world (re. embedding, embodiment, infrastructure): in particular energy, perception, action, and appraisal of values, threats and opportunities. - Here the definition is elliptic, which is not optimal but can be ok; it implicitly “takes [a decision triggering] the best action”, which is well in cognitive realm. - Now in cognitive realm, rationality, though often considered prestigious, is but one possible processing type, very much in line with logic and predicate calculus notably. There are however other possible processing types, including statistical, probability-based, or distributed approaches, like in deep learning or in intuitive decision-making.	Ok in some particular contexts (this general remark is developed in section 10). Very muck ok for “an effective ability to learn”. In general, no: - Before talking of “general” sub category, let’s address the concept of “intelligence” per se. - Humans should not be taken as the reference for quantity (“matching humans”); reciprocally how intelligent would be humans if they were judged by comparison to machines (matching machines)? - Definition 8 includes many other concepts relating to cognition, i.e. a field larger than intelligence, where we focus our attention in this study. Moreover those concepts should also be properly defined, notably: “common sense … reason … complex … abstract”  .	Ok in some particular contexts (this general remark is developed in section 10). In general, no: - Implicitly “agent” is here a machine; I would define agent as a more general entity, that may be a human as well as, say, a machine. - The definition misses the essential property of intelligence, which is the ability to learn, i.e. the ability to increase expertise by experience. - the given definition might somehow fit for expertise (or synonyms: skill, know-how, or competence for example), in cognitive realm. - But “to help [humans] achieve optimal outcomes in a range of circumstances” an agent needs a lot more than intelligence, and even generally cognition:  it requires a connection to real-world (re. embedding, embodiment, infrastructure): in particular energy, perception, and appraisal of values, threats and opportunities. - Definition 9 includes several other concepts relating to cognition, i.e. a field larger than intelligence (where we focus our attention in this study), and even more broadly, to  operation in real world . Moreover those concepts should also be properly defined, notably: “rational … guidance … service … outcomes” .
		That's a property of intelligent systems, but I don't think it's the _essence_ of intelligence.		Too much conceptual baggage that may not always be useful.	Doesn't say much.	Emphasis on optimality is a distraction.  Satisfice!		Covers only one kind of way AI can be used.
								
								
Too narrow -- leaves out the role of embodiment in intelligence, in general limits to one level of analysis (computational), which is necessary but not sufficient	On this account it is difficult to argue that plants are not intelligent -- definition is too inclusive because too abstract and general	Concept of adaptation is important here, but rest of the definition is too vague -- how finite? Whose real time? Unexpected assumes expectations, which begs the question.  Learning specifies one kind of adaptation only.	Why limit definitions of intelligence to the human case?  It's not a terrible start if that is the domain, but this is a bit like trying to define furniture by defining tables.	Vague and yet narrow at the same time.	This equates intelligence with foresight and thus shifts the burden to a definition of foresight (prediction of some kind?).  It's not a bad move, but it does rule out reactive systems that are capable of adaptation without foresight.  Thus too limiting?	Vague and shifts burden to definition of rationality -- an equally disputed term.	Better but still too narrow if one wants a general definition of intelligence not limited to humans.  As noted above, the goal should be a more inclusive definition of intelligence.	Limits both the nature and the purposes of machine intelligence too greatly.
								
The statement is true, and I like the bit about "the computations that make it possible". However, it feels somewhat incomplete; the computations in artificial intelligence should make it possible to do more than just what is listed.	This sounds more like productivity to me than intelligence. It's the way that the agent goes about achieving those goals that indicate the presence (or absence) of intelligence. However, it's true that intelligence entails some ability to achieve goals.	It's too easy to interpret these criteria as constraints. While it's true that there are intelligences (e.g. humans) that can adapt to their environments while working with insufficient knowledge and resources, it does not seem reasonable to define intelligent systems to be only those entities that are similarly restricted. This would suggest that if the limitations on human intelligence were removed, that humans would somehow no longer be intelligent, which doesn't make sense.	This definition is at best incomplete and at worst incorrect. It does not define the "full range" of cognitive capabilities found in humans, it merely lists some of those (admittedly important) capabilities. However, if a system had all but one of those capabilities, would it not be intelligent? This definition does not seem to allow for varying degrees of intelligence. Additionally, in some cases, it may in fact be necessary to have a machine generate optimal behavior, and in those situations, the intelligence DOES need to do that.	I don't disagree with this definition, but it doesn't really seem to capture the core of AI. It's too specific.	This is a good start! It would be nice to expand on this. Perhaps combine this with [1] in some way.	This definition seems predicated on us being able to define what the "best possible action" is in a situation, which is not always feasible. Moreover, rational action is not required in all situations for an agent to be deemed intelligent – humans do not always act rationally, and yet they are intelligent. Intelligence is much more than rational action.	This is pretty good, but it seems incomplete. What is "common sense"? What does it mean for these abilities to be "effective"?	This sounds like an excellent goal for AI, although it doesn't quite address the "how" aspect as much as I would like. I really like that it frames the goal of machine intelligence as "guidance and service to humans". However, AI is more than just the ends, it is also the means. AI is the study of *how* to improve the abilities of machines so that they can provide us with this guidance and service.
It mentions what abilities one should have. However, it does not describe how the abilities were acquired.	It covers ability to achieve something, but not of being connected to one another.		Intelligence can be about optimality, although it doesn't have to be.	Too low level and process centric.		Doesn't cover long term goals.		Machine intelligence can be for any agent (like an animal)
						It is simple and covers pretty much everything.  Of course, it might just push all the complexity into defining "rational", but I think the definitions of that are far more agreed upon. 	It seems quite vague.  	Too tied to helping humans, not just making decisions independently. 
Perception, reasoning and being able to act are aspect of AI, but not the only ones.	In my opinion, it's one of the few definitions of intelligence that work for humans. 			too technical	too general, other qualities than intelligence match this criterion			machine intelligence shouldn't be defined as a tool to serving humans
It covers a wide range of areas considered AI.	Goal-driven reasoning is only one facet.	This is a very narrow subset of what constitutes AI.	AI is broader than just mirroring human cognition.	Again, the goal-driven nature of this statement is misleading and narrow.		This is just one aspect of AI.	They can succeed, and do so in entirely different ways than humans.	This is a use of AI, but doesn't define it.
Except, this is the definition of the field of AI research, rather than of intelligence of a machine.		I don't see why intelligence should have a strict requirement to adapt.		These 2 specific definitions are fine for the specific terms they define, but not so good for AI as a whole.	The first part is a definition of the field of AI research, rather than AI. Second part: I don't see the need for an environment necessarily in the definition of intelligence.	Not wrong, but seems like an incomplete definition.	"natural and abstract domains" is a much better wording than "environments". And, "complex information-processing challenges" it's better than just saying "goals".	This is only one goal of a much larger set.
								
								
			this goal appears too human-centric					
Hits the mark -- sensory input; automated reasoning such as with logical inference; motor output. 	Too abstracted and too remote from the mechanisms of thought and reasoning.	Describes problems we encounter in building AI Minds similar to human minds.	A Seed AI only needs to demonstrate basic conceptual thinking, not  "exhibit the full range of the cognitive capabilities we find in humans."	The definition is sheer gobbledygook.	Too vague.	Begs the question.	Too general; does not address specific features of machine intelligence. 	Our goal in building AI Minds is not "guidance and service to humans".
								
too vague and open to trivial interpretations	Better variant: agent's ability to determine optimal behaviors in a wide range of environmental situations. Achieving the goal depends also on other factors 	Better formulation: principle of adapting to the environment by identifying optimal behavior given limited and partial information...	Generally correct, however, equating specific human intelligence in all aspects is not essential. Example in aeronautics: planes don't exhibit the full range of flying abilities of birds.	Better formulation: is objectively measured by the observed capability of an agent... for machines as well as humans, Intelligence is the ability to find a solution but other factors may be involved to put it in application. Externally, however, it is the application of the solution that can be observed.	too vague	As before, better formulation: ...to identify the best...	Too many vague concepts. Good enough for philosophers but not practical for AI.	Note: Intelligence is a system capability that is independent of human needs.
			Modeling intelligence after humans may be just one of many approaches.	What happens when the pragmatic solution excludes humans?				Any definition that used humans as part of the definition would be unfit to identify intelligence in humans.
AI is the study of computations that is possible to perceive, attend, decide, evaluate, learn, memorize, feel, reason, create, act and being aware								not service to humans, like "human intelligence" is NOT for a service to someone else. Also, not necessarily "rational". Human intelligence is not always rational, why should machine intelligence be one?
								
"Reasoning" is too vague.	Would definitely add "Machine" before "Intelligence". 		Is our goal to build machines that behave like humans? We have enough humans already. In my view, the point would be to understand how humans work, but actually, who knows whether replicating human behavior means we've replicated the mechanisms by which humans behave in such a way? 		"Appropriately" is very vague.	What does "best" mean?	Again, why is our goal to *match* human intelligence?	Guidance and service to humans are pretty limitative. Doesn't even cover the ability to play Go.
	This is a good starting point, but too vague. A non-intelligent system can meet this definition.				Uselessly circular		Closest.	Intelligence is a trait of a system; it does not need to be defined in terms of how it interacts with other systems, although such interactions may be a useful way of measuring.
	I favor a behavioral definition of intelligence.  This includes a built-in flexibility, capable of dealing with or adapting to different environments.	All processing capacity is finite :-/   Again, we're implicitly modeling after humans in some of this.	This framing implicitly assumes that we are trying to replicate human intelligence.  I think that the goal should not be defined in terms of humans, since machine intelligences will have different constraints, and might, in some areas, show super-human abilities.				Modeling after humans, again.	
This just pushes the hard part of the definition up a level.  What does if man to reason?				Not specific enough. Todd could define almost any set of algorithms.		This excludes most humans in most domains.  Best possible is too hard.		
Yes, this generalizes across humans and machines and does not invoke arbitrary limits.	It is unclear whether the concept of goals requires consciousness or not. Also, systems can be functional, acting to achieve states such as homeostasis, which may not involve explicit goals at all.	If the agent is able to adapt to the environment, in what was is its knowledge insufficient?	I can see no reason why machine intelligence should be constrained by human intelligence.	Yes, this captures learning through action and experience. 	The answer is too dependent on an inexplicable concept of appropriateness.	Depends what action is supposed to include. Is solving a theorem an action?	Why machines? What is meant by machines? Are steam engines included? Does intelligence have to be limited to one physical encapsulation? Can't it be widely distributed and ontologically malleable?	Human intelligence is not defined by social service, so why should machine intelligence be?
You don't need to be intelligence to achieve those things.	From Sternberg RJ; Salter W (1982). Handbook of human intelligence. Cambridge, UK: Cambridge University Press.	From Sternberg RJ; Salter W (1982). Handbook of human intelligence. Cambridge, UK: Cambridge University Press.	Humans should not be the poster child of the definition intelligence.	Nice, but very complicated		Best is an optimization problem, and it typically impossible. Decisions are typically time bound.	No definition should mention humans.	Unbiased isn't necessary. Achieving optimality is typically NP-Hard, so nearly always impossible, so by this definition won't be intelligent.
This is completely accurate but a little boring and leaves out that there's a large systems component (so not just computations, also architecture.)	I think that's pretty good but a little fluffy.  Some agents will specialize on wider range, some on achieving goals better. I prefer a more computational definition that makes those tradeoffs clear.	This is also pretty good, what I don't like is that it talks about "should" / is normative, and implies there's any alternative to working with finite processing. So again while it's fairly accurate it's phrased more like hype than like a technical definition.	I don't think it's a very good definition, it's just a good description of AI.	Again, this is pretty good, it would get a 4 or even a 5 if it left out "general".  The term "general intelligence" is utter BS.  The people working on it are NOT better than or discontinuous from the people working on AI for the last 60 years.	That's very close to mine.  I'm dithering over a 5, but I really prefer instead of "actually devoted" to say "constructed deliberately".  That is, the only difference between NI & AI is the latter is an artefact, which is only--but critically! --important because of HUGE ethical implications.	boring but right as far as it goes, for most but not all definitions of "rational"	Again mentions general intelligence, see above, but also overlooks how much of human intelligence depends on embodiment so will not be "matched" (identical to) or a subset of AI even where AI outstrips NI in sheer computation.	Not computationally tractable, also differentiates between machine & animal intelligence.
Wrong!	I can easily find counter examples.	Doesn't have the problems from the above definitions, but still it is too inclusive. I think that this definition would formally accept entities that one does not consider intelligent.	Again, it is limited to human type of intelligence only.	Complicated.	This one does a good job; "appropriately" is what makes it good. I would even delete "and with foresight" because this does not bring anything new that "appropriately" does not already cover.	With "best possible action" it goes in the same direction as "appropriate action" from another definition above. The problem is with "rational", which is a term that potentially excludes a lot of appropriate actions or, if it doesn't, than it is superfluous.  	Complicated.	It is limited to human type of intelligence. No other forms of intelligence are covered.
What about evaluating, choosing goals, speculating about grand questions?	It assumes intelligence is something measurable. There are huge varieties of intelligence (even in humans) that are not commensurable and cannot be combined into a single scale. There are many partial orderings. Not all intelligence is concerned with achieving goals. Much mathematical intelligence is concerned with achieving insights that were not goals.	Much intelligence, e.g. some kinds of philosophical intelligence, artistic intelligence, mathematical intelligence has nothing to do with adapting to the environment.	It's impossible to combine the full range in one individual, whether human or machine. that's partly because some kinds involve incompatible goals. 	I don't think those spaces and distributions exist. They are inventions of a subset of recent decision theorists, etc.	as before	There often isn't a best possible action.	Which humans are you referring to. There are enormous differences across humans.	This might be a feature of machine slavery, not machine intelligence.
								
Computations are probably conditional probabilities.		Again, what is learned does not necessarily satisfy "rational" laws.	There are probably 100 kinds of intelligence.	It should model its own use of computational resources.	It uses its model to guess the result of its actions.	Then people are mostly not intelligent.	There are many kinds of intelligence.	AI is not necessarily rational.  It may be more associative, like deep learning.
								
								
I miss learn, adapt, plan and using incomplete knowledge.	Good, but only a part, just restricted to 'achieve goals'	Adaption with insufficient knowledge is an important point. Real time is optional, I myself am just filling this a second time because it timed out the first time as I didn't reach the real time goal.	Except for the reference to humans it includes the various dimensions someone should take into account.	It just defines how to measure how efficient some intelligence is.	Describes artificial intelligence but lacks a definition of intelligence, 'function appropriately' and 'foresight'... :-)	Best possible and rational are personal and whomever tries to judge will need to have some insight in the black box.	Short, precise, but somewhat to simplistic.	Unbiased is a target to aim for, to help is another, optimal another, so these might be targets for someone developing a special system, but it shouldn't be used to define machine intelligence.
								
						This definition leaves out any examination of what constitutes the "best possible action"		Machine intelligence should be defined in terms of human objectives.
	Intelligence can be in a very specific environment if it needs to.	The ability to learn given experiences is part of intelligence.	Because we don't need computer systems to do everything, we just need them to do some tasks better or on par with humans.					Machine learning is there as an aide to human intelligence, to give guidance. It is also there to help in cases where humans cannot or will not be available.
Possibly true on some level but so vague as to be practically useless.	too vague		This is a reasonable working definition of "artificial intelligence" but I don't see the need for practical AI to exhibit the full range of human cognitive capabilities.	I don't think tying the definition of AI to the resources used makes sense	too vague.  what does "function appropriately" mean?  in which contexts?	To be in anyway useful, an AI will need to be able to deal with situations where there is no well-defined "best possible action"	Better but still rather vague	"Rational unbiased guidance" and "optimal outcomes" are problematic phrases.  Training data biases the models.
Oh boy.. these are all terms that must be unpacked and barely constitute a necessary set for intelligence. Perception is not necessary, sensation is. Reasoning is not necessary, learning is. Acting is not necessary, modelling (learning, prediction) is.	Intelligence can be measured based on three incremental abilities: the ability to learn, the ability to predict, and the ability to control (or act). Why? To achieve goals in environments assumes that the ability to control is already well advanced, yet a system that learns and/or predicts can already be very helpful to others and should be considered intelligent. The optional definition I propose focusses on the fundamental necessary (but not sufficient) conditions for intelligence and acknowledges that these three elements may be present at different degrees of sophistication.	I broadly agree with this attempt define intelligence but it lacks rigor. Adaptation can take many shapes and not always necessarily require intelligence (a car suspension adapts to the change in road conditions).	This should not be the only goal but can certainly be part of the goal.	This implies statistical learning. This is not necessary.	Broadly agree but lacks rigor.	Rationality is not necessary for intelligence - it often implies deduction not induction. Taking best possible action is ok but lacks rigor.	Not necessarily need to match humans (could be subset or superset), not rigorous enough.	
								
	Intelligence has evolved through natural selection							Machine Intelligence can include bias, but it is explicit
Intelligence should not be limited to reasoning. Animals also have a (degree of) intelligence, and not necessarily the same capacity of reasoning as humans.	Limiting intelligence to "achieving goals" seems unreasonably restrictive to the range of intelligence found in nature. What explicit goal does a cat have when it is licking its litter?	I agree with the first sentence. I do not agree with the rest of the paragraph. I don't think the rest of the paragraph follows from the first sentence.	Intelligence is not equal to human intelligence.	Intelligence is not limited by goals.	This definition seems too vague.	Intelligence is not limited by rationality.	Intelligence is not limited by humans.	Intelligence is not equal to rationality.
								
A fair, the three questions - perception, reasoning, action - are still strongly debated in philosophical circles. Do they therefore make good goals for AI research?			I've a bit of a problem with this since "full range of the cognitive capabilities" found in humans is restrictive - a machine having capabilities that humans do NOT yet lacking some humans DO would be an exciting outcome for me.	A bit engineering-like but why not?	"Appropriately" and "with foresight" are a bit vague - to what degree do we consider animals acting in this manner, and would we consider them "intelligent". Also, AI *research" is the activity of making machines intelligent, I'd argue.	I disagree: one can be highly intelligent yet still arrive at irrational outcomes, due to bias, phobias etc. In fact, given the general disagreement about a lot of topics (political, economic, social etc.) between people that most of us would consider "intelligent", marrying rationality and intelligence seem wrong to me.	"Common sense" is a capacity lacking in a lot of humans! :)	I don't see why service to humans should be a defining criterion of machine intelligence - it would preclude truly autonomous (non-slave, if you will) machines from being considered intelligent.
This focuses an actions again, but with respect to acting machines, it is fine.		If the start is replaced with "The essence of artificial intelligence is...", then I am more in accordance. However, intelligence is surely not limited to actions only.	"the full range of cognitive capabilities" is probably not needed in all cases and an overshoot of the definition. 					The statement involves purely positive AI. AI does not be definition include the use for the good.
	Restricted by the definition of the measurement and the environment. 			Again intelligence should be a factional property of a system not necessarily a measurable property. 		Not applicable under incomplete knowledge	To anthropocentric definition. Having a cockroach level intelligent system is already a huge leap forward. 	Definition of machine intelligence should be more general then servitude 
	Yes, this distinguishes intelligence from "mindlessly" following directions to achieve a goal in similar environments.				Too vague.	Too vague, yet too specific about "best possible action". Intelligence does not always make the best possible action.		No, this may be the goal of some machine intelligence applications, but it does not define machine intelligence.
	Intelligence is a multidimensional construct. The question assumes a single dimension. Yes, that dimension is important, but it is not the only thing that "intelligence measures."		I disagree that this it "the goal" but I agree with the items described.					Machine intelligence can be biased (often for the same reasons human intelligence is biased), and as machine technologies become more advanced, they need not be rational. Rationality is not the only way that people function, nor is it always the preferred way.
AI needs to Decide not necessarily Act - AI can simply decide and/or instruct another system to Act.	Focus on, or Limitation to, particular goals and environments does not mean there is no intelligence. Execution does not mean intelligence has been used.	2nd sentence does not follow from 1st. One goal of developing AI systems is a system that uses resources appropriately and recognize when to call for and use more (e.g. spinning extra clusters). It should certainly learn from experience - noting variations in more dimensions than can be recognized by a human 	"To build a CS  that exhibits the full range ..."  is not and should not be the goal of AI. Each AI application should have greater capabilities (not necessarily cognitive) than a human.	Not sure	Yes but it should also learn.	This "ideal" seems to be unattainable -- how would AI decide which is best and best for whom?	This is a milestone on the path to dramatic surpassing of human GI	agreed "MI is ......outcomes" but it need not be in a range of circumstances.
								
	Environments is ill defined.		A drone that can fly people more safely than a pair of pilots is discreet, but displays a mastery that can only be called artificial intelligence.	Word salad.	I like that intelligence can be intra-domain versus always having to be inter-domain.		That is the last thing we need. Domain intelligence first.	
no indication of how it should act	learning is missing as is efficiency	missing sensory input 	much better but too wordy and too anthropocentric	feels too much like statistics and lacks ability to communicate its knowledge	this reeks of "planning" and lacks learning	optimality is not intelligence	too anthropocentric	people don't want optimality and we aren't describing "genius" here
								
Lots of systems that that are not considered "intelligent" pass this task, such as cruise controls, automatic door openers, ...	Broad enough to capture intelligent systems, but insufficient to filter intelligent from non-intelligent systems. It leaves out the agent's repertoire of actions and where they came from (are they fixed or programmed, composed by the agent from elemental, generally useless behavioral components...). It also leaves out where the goals come from. Are they provided? Programmed? Self-determined? What is a suitably wide range of environments? 10, 20, 1000 environments? Unlimited environments? What determines their width of the range?	By definition, nothing can adapt effectively with insufficient knowledge. This actually implies that the system can succeed using data that is insufficient for solving problems using established techniques. The definition in the first sentence is not measurable in any way. It boils down to adapting while working. What is success? Kicking out a different wrong answer each trial would satisfy this definition. The remainder is mostly unrelated to intelligence (finite processing capacity and real time performance). The only related elements are being "open" to unexpected tasks" and learning from experience. Again, these are too vague.	Getting closer, since the whole notion of intelligence is grounded in human performance. However, the definition uses phrases  that are no more concrete than "intelligence." Phrases like "cognitive capabilities," broad domains," and "broad competence." It also establish vague requirements, like "wide variety of problems," "different types of knowledge and learning in different situations." 	The definition presupposes a particular implementation. The definition should be implementation independent. Reference to goal and environment space suggests a highly constrained problem. Real world environments are effectively unlimited in scope, as are the goals that might be pursued in them.	This is too simple. An electric drill with a thermal shutdown switch passes this test.	This would rule out humans and systems that function as humans do. This definition is likely derived from a particular implementation strategy (utility functions). 	This definition tries to break the circular definition (paraphrasing: machine intelligence is matching human general intelligence), with poorly defined components: "common sense," "ability to learn, reason, and plan," "complex information processing challenges," and a "wide range of domains." None of this has a commonly shared definition. I give it a three because it does use human intelligence as the primary reference.	Too narrow. Humans are the quintessential example of an intelligent entity, yet a machine that replicates human function perfectly would fail this test. Rationality and bias-free responses are a nice to have, but do not capture the essence or even the core challenge of intelligence.
	Depending on how we measure. Quantifying is probably very difficult (at best).	If the last sentence was gone it would be close to optimal. However, the whole "rationality" thing implies human rationality, which probably is debatable. 	As long as systems are plural, and we are not saying one system to do it all. It is important to not wish for optimal behavior.		Foresight, perhaps. 	Rationality is an extremely fuzzy concept and quite likely an illusion in humans (unless you are an economist of course).	This in particular seems to ignore the whole hardware/embodiment issue. There is a core issue as to whether "human-like" intelligence can be achieved on a digital platform. 	Bias might (do) arise from the data any system uses. That unbiased guidance might be a goal if perhaps a good idea. However, probably impossible.  
It is simple and clear, but doesn't seem to convey the complexity of the subject, as if accomplishing these 3 tasks is sufficient.	It seems something is missing. Figuring out what the agent's goal is, should it be part of intelligence too?	It seems incomplete, more than just adapting, maybe there should be something about achieving. Also, I am not sure that working in real-time is an absolute requirement for intelligence.	This is not quite a definition, but rather a short list of requirements		It is clear, simple, while implying the complexity of the subject.	Is intelligence all about acting? I believe there is more, for example learning from the outcome of past actions (which were not necessarily the best actions, and not necessarily taken with the full knowledge of a situation)	The use of "common sense" seems debatable to me	
	It's too vague. What is wide?			This is way too specific				Why should humans be in the definition of machine intelligence? 
								In its purpose, yes, it should be. In practice, I'd say that it's not the major and the only use case.
Still vague, but at first glance this one seems a little more practical	Pretty vague and hackable	So, mitochondria are intelligent? BTW, brute force also deals with finite processing capacity (and real time is pretty flexible if we look at it at different time scales). Also, rationality is a different topic, which is about rational decision making cycle, not adaptation (so this grade is worse than "disagree", but double the "disagree") 	What are these cognitive capabilities? Also, optimal means we have a measurable evaluation criteria and metrics, what are those? How do multi-criterial decisions comply with optimal behavior? 	Trying to add probabilistic views into this definition is nice, but we still try to measure the ill-defined intelligence in terms of goals and environments, which fits RL agents, but has little to do with humans and subhuman intelligent creatures 	Substitute "intelligence" with "magic" and you get a new startup trend (all aboard the artificial magic hype train choo choo). 	I see what you did there. It is a definition about rational agents, not intelligence. Hopefully, intelligent agents should be rational, but the main difference between intelligent agents and rational decision making algorithms is that intelligent agents will be the ones to set their goals and their evaluation criteria on their way to making decisions (including hypothesis generation, choosing the optimal one and executing it).	Something Kurzweil could say in one of his singularity-related prophecies	Somewhat something prescriptive analytics redefined? And what about agent-free analytics? Can a search index and even array sorting (median/IQR estimate) be considered MI?
								
This definition is not general enough	This definition is not general enough	This definition is not general enough	This definition is not general enough	This definition is not general enough	This definition is not general enough	This definition is not general enough	This definition is not general enough	This definition is not general enough
				A bit technical				Sounds like machine intelligence would be something completely different from human intelligence
								
This is interesting! I agree, however I am not sure how we can measure the computational power, especially when we consider the embodiment part of the intelligence where a part of the world knowledge is embedded in the shape and material of the parts composing the agent... still interesting!	Defining intelligent machine using the word intelligent is not that helpful.	Yes, but not only that	To base the definition of "machine intelligence" on human service or any other human centered reference would be a limitation if not a mistake in the field. If needed, there must be 2 definitions, one for "Machine intelligence" and one for "Intelligence for human services".	Too broad of a statement 	Yes! now the problem is what is reasoning?	The goal is to comprehend our own intelligence by creating intelligences and failing to do so. The notion of optimal behavior is completely subjective, is it optimal from a task point of view, an energetic point of view or an evolutionary point of view ? all are correct when neither is at the same time (for what we know this far).	Why rational? Based on what? why the best action ? based on what ? Are human intelligent beings ? do they always take the best actions? do they take the best actions even often ? 	The relative rationality part is a reductive part, implying rationality means that the definition is switched to this term now.
								
Vague, but agreeable.		I don't think this part is necessary "while working with insufficient knowledge and resources" But the idea of relative rationality is appealing.	This would be a definition of "artificial human" not AI	Disagree - I think that AI should be able to define its own goals for an environment.	So a prediction algorithm is sufficiently intelligent?	Similar to #1	Why humans? Intelligence need not be measured in human-units.	Why service to humans?
Too broad as it says nothing about how well it perceives, reasons and acts.  It also includes all of vision, signal processing and robotics which are distinct fields.  	Pro: It not too limiting, con: by this definition every agent is intelligent, just a question of how much. 	It says nothing about how well the system works in that environment. It says nothing about creativity of the solutions. 	It says nothing about how well the system works in that environment. It says nothing about creativity of the solutions.	Pro: It not too limiting, con: by this definition every agent is intelligent, just a question of how much.	Any computer program can function appropriately and with the foresight provided by its programmers. 	Intelligence need not be rational, and rarely takes the best possible action.  And a definition with "ideally" in it does not say what happens most of the time. 	It’s not a definition, it’s a statement. 	Intelligence need not be rational or unbiased and has nothing to do with service to humans. 
Seems reasonable, though it shouldn't be a field (study of); it should be more of a thing, e.g. AI is the ability to perceive, reason, and act. 	It's not just about doing; it's sometimes about thinking. Though Alva Noë's book Action in Perception argues the opposite, you probably get my point. 	too constrained / contextualized	Sound basically like, "Act like humans," which seems reasonable. 	Seems true, but not well-written. Too academic.	I agree with the statement, but it's not sufficient to define the richness of intelligence. 	Don't like the word choices b/c "rational" is not necessarily the best possible action. This definition could lead to favoring utilitarian ethics. 	Sounds reasonable but not too academic. 	There's no such thing as unbiased when information is filtered. Thus, this leans towards too much emphasis on rational and hiding the bias. 
	Intelligence is not necessarily about achieving,	Adaptation and learning are hallmarks of intelligence.	The criteria seems a bit human-centric to me.  			"best possible action" is not well defined. 		Intelligence is not about being unbiased, it's about having the right bias. Intelligence is not about serving humans. 
								
								
Perception, reasoning and action are all components of exhibiting intelligent behavior.	A robust automatic control system is capable of meeting this definition, but does not necessarily demonstrate intelligence beyond that of its designer.	The ability to work on unexpected tasks is an important part of intelligence. Simply following programming to accomplish pre-computed activities does not constitute intelligence.	Intelligent behavior does not necessarily have to be exhibited across a wide range of domains.	It is not clear that efficiency of computation should be considered a proxy for intelligence.	Self-referential. Foresight could be part of a suitable definition, but it needs to be separated from model predictive control.	Arguably, human intelligence is focused on finding solutions that are good enough rather than best in many cases.	This has an aspiration that is generally true to traditional AI research rather than incorporating existing technologies that are increasingly being branded as AI as marketing exercises.	This definition could encapsulate a printed flow chart that captures knowledge but does not demonstrate innate intelligence.
								
Problem solving and automatic inference should be algorithmic	Intelligence is proportional to  optimality of found solutions	I agree: Intelligence is the possibility to interact with the environment to get a gain	I agree. Computer solving capabilities should mimic those of humans	I repeat, intelligence is proportional to  optimality of found solutions	Interaction with the environment is the clue!	Intelligence is concerned with finding an effective solution although not optimal	Common sense is far from machines, they are not social entities,	Computers should serve for life level improvements
						"best possible" is ambiguous; the two previous definitions were clearer.		This mixes goals in with ability - an intelligent agent may have no desire or need to guide humans
								
		We still use "AI" to refer to particular, fairly limited tasks.  	AI does good work by solving particular subtasks.		I think we can do a lot with particular tasks, and call it AI, though we have done diddlysquat for "foresight".	There can be many different notions of "best".	This is not a well-formed sentence.  Sentence fragment.	This is best honored in the failures --- if we program it, we tend to program in our own biases.  If it evolves, it will evolve its own biases.
Too much anthropocentric 	A very weak but acceptable definition.	I agree with the working definition as it gives an operational measure to recognize the presence or not of an artificial intelligent system in front of us. On the other hand, I don't support the "relative rationality" interpretation. Intelligence isn't (just) rationality, which is one of the "modus operandi" that an intelligent system can switch on, according to the context and its needs.	The very concept of optimal behavior clashes with the concept of creativity, which entails failure, which is not optimal, which is the source of intelligent behavior. So yes, an intelligent system must not generate any optimal behavior.	If this very same definition would be applied to a human would you feel comfortable in being defined in this way? Why shouldn't an intelligent machine perceive and feel the same then?	Cumbersome	Again, rationality is a very limited and restricted attitude of any intelligent creature. Over stating the importance of rationality is a typical Western bias.		This would fit better the definition of a slave system rather the definition of an intelligent system.
This is one of possible frameworks for the study of AI.	Achieving goals or ability to predict/anticipate is only a part of general human intelligence.	This is too simplistic view. I do not believe in one single definition for all sorts of intelligences human possess - emotional, social etc.  "Intelligence " is very complex phenomenon and it is not always about adaptation - it might be about ability to construct and change, to anticipate what is invisible immediately to lead and not only to obey.	I believe we will need HUMAN level intelligences for situations where humans are involved in collaboration with machines. We will also need different kinds of intelligences that are quicker, parallel and different in varieties of ways from human intelligence - as intelligent systems capable of communication with thousands of other intelligent systems. For human intelligence level we have humans. That is why we will develop more powerful intelligences for specific usages.	This is nice definition, but again it is within the framework that may not be of major interest if we are studying social intelligence. It will help us better to understand underlying mechanisms but not what social intelligence is about.	Again this definition captures only a part of it. Not only the ability to adapt and use immediately available resources from the environment to fulfil immediately foreseeable goals but even the intelligence of a genius capable to redefine goals and re-conceptualize the environment. In short, it is impossible to compress all the aspect of the use of the term "intelligence" into one single definition. More effective would be to draw a taxonomy from which it could be clearly visible how different uses differ, why they are common, and how they relate to each other.	In 1983 an American developmental psychologist Howard Gardener described 9 types of intelligence: Naturalist (nature smart) Musical (sound smart) Logical-mathematical (number/reasoning smart) Existential (life smart) Interpersonal (people smart) Bodily-kinesthetic (body smart) Linguistic (word smart) Intra-personal (self smart) Spatial (picture smart) Only one of those is Logical-mathematical - kind of intelligence AI wants to mimic. Gardner, Howard (1983), Frames of Mind: The Theory of Multiple Intelligences, Basic Books.	This again takes only a subset of multiple intelligences.	Human level machine intelligence is an ability of a machine which, if possessed by human, would be considered intelligence. And human intelligence should be seen as all of them, with rational intelligence as a subset. Machine super intelligence would be generalization of the above.
			The definition of artificial intelligence should not depend on human-level intelligence because human intelligence has (also) no clear definition.			The terms "rational" and "best possible action" need further explanation.		I prefer an abstract definition without specifying a purpose, here: service to humans.
						Not a criterion that characterizes human intelligence.		Subjugated intelligence... we may want to maintain it in MI, but it shouldn't define MI.
act?  choose actions, maybe, but not control effectors	why pick out wide range? you can have an intelligent chess machine	doesn't get at function, attributes are not essential	not my goal for AI	why normalized in...		defining "best possible" action is also subject to thought	why general	intelligence is not optimality
								
Same as above. Perceive/Reason are human verbs. Machine obtain input and produce output (Both can be multi-modal, multi-dimensional and as complex as needs be) . If the output is  useful and correct   - that's good enough.	Depending what the goal is. Not all require a broad-range-of-environments.	Human intelligence has many contexts and many meanings in the different contexts. Machine intelligence seems much simpler to define (e.g. changing behavior over time, learning/adaptation, and maximizing some kind of reward based utilizing this change.)  Hence, I disagree with a single definition of intelligence (which seems to be driven by the definer's own goals).	I do not think that an intelligent machine need have the Full Range of Cognitive Capabilities human  have. Do we even KNOW what this full-range is in humans? I do not think so.	Agree with the last part. I would not call it pragmatic General intelligence though, but rather pragmatic way of defining intelligence for the purpose of defining Machine Intelligence.	Again - the Foresight is a human property. All one needs is a machine that delivers. Can we please stop attributing human properties to machines? Describing Machines using machine-related nouns and verbs (as in not saying that submarines/boats can Swim) would make the definition of machine intelligence so much simpler...	I assume this refers to Machine Intelligence (Intelligent Agent) and it seems like a sturdy enough definition for this purpose.	See all of the above. 	Again, specific-purpose driven, argumentative/philosophy-centered in nature, and not pragmatic enough. I would like to have machine that do useful things. Washing machine that washes effectively is intelligent enough and a machine that would predict earth-quakes or disease correctly is intelligent enough. Unbiased or rational are not needed if the machine simply does its job well.
this describes AI as the "study"	this seems more like procedural effectiveness not intelligence		leaves out mention of the real-world contexts, complexities and emotions that make us human and that machines could and should not be expected to learn	I don't understand it		the term "rational" needs clarification	I do not think it is possible for machines to have common sense--that is a human quality (and a dubious one at that)	I do not believe that any machine (since it is made by humans) is truly unbiased
								
						This criterion is not true of realistic systems even for "simple" games like Go.		
Too narrow a definition	Lack something about self-consciousness 	Intelligence is not only about rationality, even relative				Emotions are a huge part of decision-making processes	What is common sense? Impossible to define. The abstract part is interesting 	
	focus too much on flexibility	too machine learning centric	restricts work to focus on human-like cognition.	too computational, and too specific to a particular computational paradigm	again, relatively bland, but seems to encompass most of the work in AI	seems so bland as to be almost meaningless, but on the other hand it does apply	human-centric, but otherwise okay	unbiased? based on training/programming done by humans? that seems unrealistic, to say the least.
This definition lacks emphasis on knowledge representation.		It includes the notion of bounded computational resources	I think breadth of knowledge allows to tackle more realistic problems. Incidentally, this is one of the things current AI methods are particularly bad at. The issue is also underrepresented in AI literature, so a definition of AI which includes this notion will benefit research in a much needed direction.	I find this definition too vague.	This definition is not constructive (it does not qualify or define intelligence).	There should be Intelligence in the selection of what is best as well, and this seems to be an even harder problem.	I don't think artificial intelligence should be defined in comparison to humans.	
"Study of the computations"?  Strange phrasing	This isn't a definition of artificial intelligence, it is a (poorly specified and vague) definition of one metric of intelligence.	This is a reasonable statement about one aspect of intelligence, but not a unique definition.	This is a reasonable statement about one aspect of intelligence, but not a unique definition.	Slightly better, but entirely inaccessible to the public.  Lots of ivory tower academic language	This is a circular statement. Also "appropriately" and "with foresight" are not well defined.	This is a somewhat vague statement.  Both "rational" and "best possible" have unclear definitions.	"Common sense" is not well defined.  "Matching humans" is not a goal.  One would hope machines could be much more intelligent than humans.	This statement focuses on service to humans, which is restrictive, and leaves several terms undefined including "agent", "rational", "guidance", and "optimal"
								
I rather like this definition but would prefer to replace "computations" with "algorithms" (since the former imply to me a certain kind of platform, and future potential platforms that are (for example) partly cell-based or implemented by chemicals in test tubes present an unnecessary stretch).  Also, by specifying that AI is "the study that", it does not apply to the actual quality of acting 'intelligently'; it is merely the name of a discipline.  	This is not wrong per se but is too restrictive a definition.  In the narrow sense "measures" is concerned only with an assay procedure and metrics, while intelligence, to me, is a characteristic of the actual processing/'thinking', not the measurement process.  (That is, you can replace the first word in this definition with "IQ tests and similar" and still get this restrictive reading.)  Further, this definition does not require that there be a wide range of different kinds of goals, so it applies (in its intended wider reading) to too many special-purpose algorithms.  	This provides some of the main characteristics of intelligence (regardless of the platform it 'executes' on), and I agree with them.  But it has flaws: it lacks the core characteristic of being (generally) purposive/goal-based, and misses a "be" before "open".  And then one can add a sentence to specify the platform as "artificial" or "machine" if one wants.  	This is a fine answer too, but less general than the first one in that it starts off with the "computer systems" clause.  I like the addition of non-optimal behavior at the end.  	This definition is not bad at all in a technical sense, except that it is rather wordy and requires specialized technical knowledge ("prior distributions", "goal space", "environment space") that the general public and journalists do not possess.  So it's not really helpful as a core definition.   	This definition is not bad but it also punts its core parts, in this case the word "appropriately" and "function".  So it needs more.  	This is not wrong per se but punts defining the essential characteristics to the words "rational" and "best possible".  So it's not much of a definition without additional material.  	I rather like this, again with the proviso that it includes the first part "Machines matching humans in general intelligence".  Dropping that, and defining [general] intelligence by the part after the dash, is quite good.  Again the word "effective" is troublesome.  	This introduces two problems: MI being in the service of humans and MI being unbiased and 'rational'.  Both these requirements are socially normative (i.e., require the ethical/evaluative judgment of an external observer) and I think that has no place being core aspect of the definition of intelligence.  
								
Not clear why it has to be computation only. What if we can have a hybrid system - that has some (analog) control system embodied in the solution?	Too ambiguous.	This definition emphasizes key performance criteria for a machine to demonstrate intelligence - adaptation, incomplete information, finite resources, timely response, execute (and perhaps synthesize) tasks that are not pre-defined and self-improvement.	Such a system is a worthy goal. If designed well and implemented, it would provide new insights into the nature of human intelligence. Eliciting and understanding the nature of human intelligence is a laudable goal - perhaps, not the best focus for machine intelligence.	Has a strong modeling-assumptions flavor to it.	Ambiguous.	We now know that even humans are not rational! For example, humans know that they need to control nutrition intake to manage their wellness - and we all know that we give in to temptations.	Not clear why machines must match human intelligence. 	The environment we interact with is complex and cannot be adequately modeled. Thus, optimal outcome is a far reaching objective that is difficult to achieve. Again, understanding what can be an optimal solution helps us understand the nature of the environment and the nature of the solutions being considered. 
"perceive" and "reason" are just as ill-defined as "intelligence"	This is a vaguer version of the first definition, which I like.	The definition is functional it describes qualities we want any agent human or otherwise to possess in order to operate autonomously in real situations.	Intelligence and intelligent behavior should not be equated with human intelligence and behavior.	This is the same as first definition, formulated in scientific jargon.	This "definition" uses ill-defined concepts to define intelligence and vaguely points to aspects of the first clear functional definition.	Rational and "best" are just as vague as intelligence. This is not a definition.	We should make AI for pragmatic goals, whose achievement requires a degree of autonomy and many qualities of human intelligence, but we should not try to create entities that are like us wondering why they are intelligent and what their purpose, if any, is.	Our machines already do all that. Yet, we do not think of them as AI
								
								
Too vague.	This is part of intelligence, but not all.	This definition leaves out any notion of tasks that an intelligent system is expected to do.	We know how to make human-level intelligences:  make more humans.  A better goal is to augment human-level intelligence.	Too abstract.	Too vague.	Rationality is too narrowly defined to be useful.		
								
								
		Work in real time seems too vague and also too restrictive.	Machine intelligence should open the way for new forms of intelligence beyond human capacity. It should excel in tasks that humans do poorly to augment human capabilities. Merely mimicking human behavior is very restrictive. Machine intelligence should generate locally optimal behavior since that's what machines are good at.					
This is a good pragmatic platform for AI research		Nice but misses the requirement of being capable of generating goals over long time horizons	Too vague			Useless. What is "rational" how is "best possible action" defined?		
		I like the focus on adaptiveness, but finite processing capacity is superfluous (not needed in theory and in practice unavoidable).	dislike the coupling to humans - many animals are intelligent under my definition of intelligence.					machine intelligence exists without the presence of humans.
		The theory of niche construction indicates that we do not just adapt to our environment, but we also adapt to those adaptations. I believe this needs to be added to the above definition 	Optimization is a matter of context, so the conditions for optimal behavior are in continuous flux and the computation that is required to attain optimal behavior would need to change faster than it would take to achieve it.			Yes, but there are tradeoffs in complex systems, so the notion best possible action is not really achievable.		
								
A thermostat perceives, reasons, and acts.  Necessary, but not sufficient.	Flexible goal directedness is a good indication of cognitive ability	It's plausible, but there's no good rationale for precisely these criteria 	Generality is important, but humanity is not. There are many things about humans that are necessarily true of a general intelligence.  E.g. "Tell me a story about losing your virginity" most humans can do, but is not a requirement for intelligence	Eh. Again, necessary, but not sufficient. Achieving goals is important, but so is setting them. Using priors is important, but doesn't explain how novel explanations arise.  Efficiency in computational resources is not the only kind of efficiency that matters.	Seems vacuous.  "Appropriate," while not wrong, obscures important aspects of our ignorance	Humans evolved to be what they are due to reproductive fitness, like all other organisms.  Rationality is not always the same as fitness.	Seems reasonable, although it's not even a well formed sentence, let alone a hypothesis.  Common Sense is just a string of letters :-) 	There are many forms of intelligence, not all (by which I mean no observed instances) of which are rational.  I'd be happy with a machine intelligence that was interesting to converse with on an ongoing basis.
Not clear or precise enough, though true.	I think this properly frames the problem. Perhaps it could be improved by adding a clause necessitating the use of limited resources (time, computational power etc.).	I don't agree with the word "adapting"; perhaps "understanding patterns and natural laws, rules etc that hold in the environment and using them in one's benefit or towards solving a problem". I do agree with the inclusion of the clause on limited resources and knowledge.	Partly true, but we should not employ a human-centric view of intelligence.	Really intelligent agents should be able to function (to some extent) also in unknown, previously unseen environments.	Same as above, except that "appropriately" is perhaps not the right word. An intelligent agent should not just act "appropriately"; it should exhibit goal-oriented behavior.	True, but it needs to be extended with the clause about limited resources. Also it is not clear what constitutes a "best possible action" (best for whom?). Perhaps grounding it into "solving any arbitrarily presented problem".	I think this fits the bill more-or-less, except that we should not assume that human intelligence is the only type of intelligence.	A good one, but this rules out all research related to autonomous agents, agents with self-consciousness and other high-level cognitive abilities.
								
True but simplistic. What is "perceive", "reason" ...?	Evaluation capability is just one of the features of intelligence.	"Adapting" implies some kind of action. Intelligence may also be of an "interpretative" nature. Therefore I believe that the definition, although partially true, is biased.	Yes there is a need for a broad competence. However we do not know if AI will be embodied in a "computer system".	General intelligence may not be pragmatic at all. It also depends on how do you define "pragmatics".	The definition (intelligence) should not contain what is being defined (intelligent)...	Yes but it depends on how we define rational. Are we also taking into account the mutual influence between emotional states and reason? This is mandatory!	It seems perfectly acceptable, at least for the moment.	Beneficial AI should be our goal. However "intelligent" may be spread over a network of entities and not just inside an agent.
too generic		I disagree with the principle of "adapting to the environment", intelligence is "being capable to create solutions".						
			“exhibit the full range of the cognitive capabilities we find in humans” seems too restrictive (“full”) and may not recognize incremental achievements		I sufficiently descriptive. “Function” is existence, not intelligence. Adaptability is necessary, but not sufficient for AI.	We already have this, but I don’t think anyone would claim we have a “true” AI. Therefore this is too limited a definition.		AI is not necessarily bound to serve humans. Unbiased is also... difficult to agree upon.
								
								
Perceive what, reason about what, act how? 	Vague.	Seems like good principles, but is not entirely defining what is meant by intelligence.	Seems restrictive to only focus on human intelligence.	"Relative to" is not a rigorous statement of comparison for distributions.	I think intelligent humans are often said to act "inappropriately." I like the part of defining AI as the act of making machines intelligent, independent of the intelligence definition.	This is a common working definition for AI; in fact it is the one I use when teaching; however, it does not account for the fact that humans act irrationally or define any notion of where "best" comes from (e.g. long-term expected reward).	"Common sense" is undefined and thus the definition is vague. Also unclear is what "complex information-processing challenges" means. However, planning, reasoning, and learning effectively all seem paramount to intelligence.	This focus only on guiding and aiding humans and not on autonomous operation for its own sake.
The study of computations does not make "AI". While such studies might be necessary to develop AI, it is a means, not the end.			AI does not need to possess the "full range" of human cognition. 			Don't think AI requires rationality to have "intelligence"		
								
								
								
							There is no reason to require human-level intelligence to the definition and it contradicts common uses of the term by professionals and academics.	
								
								
								
Vacuous without crisp definitions of perception/reason/action. A mechanical light switch could be argued to fall into this category, it perceives/receives an input, applies a simple reasoning (if switch is up -> turn light on), and performs an action.	Too vague.		True, but pie in the sky. 	Barely comprehensible.	Now we're dumbing down "intelligence" so that our algorithms can qualify for it.	I like this approach, it is a more task-focused definition that does not invoke bad metaphors or misguided analogies to human cognitive capabilities.	This is the same overhyped promise that has failed miserably for 70 years and continues to do so, despite claims to the contrary mostly from uninformed popular science.	
								AI does not have to be in service to humans 
		The ability to adapt is one aspect of intelligence, but IMO not the essence.			This description is too vague to be helpful: What is meant by "function appropriately"?	The question seems to ignore the problem that sometimes a situation might be defined too narrowly, which then might label a decision as bad that from a wider perspective could be regarded as intelligent.	Since intelligence has no mathematical definition, taking human intelligence as a  measure is the best and most direct approach to take.	
								
It all depends on WHAT AI you mean AGI AS`?			I think AI will go over neurobiology of the current thinking brain					
AI is more than just study but otherwise OK.	Quite vague and not all encompassing.	Not all AI need to be real-time but otherwise OK		Why measure against computational resources?		Need definition of "best". Not a clear definition.	Not sure about the term "common sense" which sounds vague, subjective and abstract but the rest seems agreeable	AI is more than a service.
Not clear what "guidance and service" (to anything) has to do with intelligence.	This definition is limited to "pragmatic general intelligence". How does it generalize when the qualifiers are removed?	Again, I am not in favor of defining AI as "the study of" something because that is at odds with common usage. The "perceive, reason, and act" construct derives from cognitive science, but it doesn't say anything about doing it well ("intelligently"), and further, it is easy to imagine an intelligence that leaves one of those pieces out. Imagine a machine that is very good at doing mathematics (not just theorem-proving). Arguably, it doesn't need to perceive anything.	While this is vague enough to pass general muster, it does not mention things we commonly assume to be part of intelligence, such as learning, inference, and adaptation.	This doesn't account for computational constraints. Bounded rationality arose as a response to this position a long time ago.	Is the effort only to define general intelligence? What about intelligence that is limited to one (or a few) specific domains? 	Adapting to the environment is half the answer. Adapting the environment to you (niching, e.g.) is equally important.	This presumes we have a clear understanding of "the full range of cognitive capabilities we find in humans." That is unlikely to be settled any time soon. Further, human cognitive capabilities are limited to the human domain. For example, is a machine that does an excellent job of trading on the stock market not to be considered intelligent? It clearly does not exhibit the full range of cognitive capabilities found in humans. In addition, it can be argued that human cognitive capabilities are not suited to trading on the stock market.	I don't think it makes sense to refer to Artificial Intelligence as the process of making machines intelligent. That is at odds with common usage. In the second part, "appropriately" is too ambiguous. It's not clear foresight is needed either. An entirely reactive machine could also be intelligent.
this sounds more like what an academic does, not a definition of  intelligence	too focused on ticking boxes		flexibility seems important	I didn’t really understand this one		people often make mistakes and it is not possible to know what the best possible action is		I think this is limiting and like slavery
							Machine intelligence can be a kind of capability to learn, to respond, to reason, and to act in a well-controlled condition without common sense.	Machine is designed and created by human to serve human.
Too vague	vague. What are these goals? What is a wide range of environment?			The definition says how it is measured, but not what is the threshold: what kind of distribution is required to be qualified as intelligent?				Machine may be intelligent without any goal related to assistance to humans
Too narrow, but good. Leaves out emotionality, empathy, and other facets of experience that can be important in some contexts. 	There's a lot more to life than goal seeking. 	Good generally, but not comprehensive or especially useful	A very good answer, but not to the exclusion of others. O	Sensible, useful, too narrow for THE definition of AI	Don't simple control systems do that? Too narrow. 	Rationality is way too narrow. Effective behavior, even without logic, should be included. 	AI is too broad for any single specific definition to meet all objectives. This definition is generally worthy, although not exclusively so. 	Seems especially narrow
						I think that Intelligence goes beyond "rational actions" and that it is what makes machine intelligence and human intelligence different. The above definition fits "machine" intelligence but this clearly lacks the concept of "empathy" which is, in my opinion, a part of the human intelligence.		
What about imagination?								
								
								
								
								
A decent attempt to pull out the different factors that go into intelligence.	Some exercising of intelligence is exploratory or reactive, not just goal-directed.	Good to see emphasis on bounded rationality, but perhaps too much emphasis on this?	Don't like the focus on human-level intelligence as a gold standard. "Levels" of intelligence seems naive.	Some exercising of intelligence is exploratory or reactive, not just goal-directed.	A bit circular. I like "with foresight", though.	"best possible" is complicated - I see what is meant by it but there is no explicit consideration of bounded rationality etc. Also - "best for whom?".	I like the attempt to pull out the different aspects of "intelligence" in this definition	Very limited - confined just to AI as an aid to human intelligence/action. Perhaps if regarded on a broad enough canvas, yes - but, likely to be read at a micro-level.
This definition assume AI can be achieved within a computational framework.	Ok, but not as good as previous definition.	Yes good definition. Clearly animals manage with limited knowledge and in real time.	Same reason above - definition should encompass animals.	Don't really understand this def.	Not a bad definition.	Action selection is a good starting point.	Any definition must encompass all animals not only humans.	I like the beneficence implied.
						There are no normative statements that can be derived from positive findings. 		
								
								
I would add "be engineered/artificial means". But a compelling triplet.	The emphasis on flexibility/adaptability is nice, but one should define how a wide range of environments should be required.	Sounds like a compelling criterion to include in a definition, although one should also demand some level of performance for something to be called intelligent, imo.	If by Artificial intelligence one means an emulator of human cognitive abilities, which I think is restrictive. Specialized AI is still AI in my opinion.	Seems like a useful definition of EPGI, but I have not encountered it before.	Plus for mentioning making "machines intelligent", although machine might also have to be defined. What counts as "appropriate" though? And how much foresight is required?	Taking a rational decision (utility-maximizing given available knowledge) should be a part of a definition.	Why not also surpass human intelligence? What is common sense?	Defining machine intelligence in terms of what it can do for humans seems like an unnecessarily restrictive definition. And then one would have to define unbiased in this context imo.
								
it is not the study itself. it is the result of the study								
This statement characterizes classical AI rather well, although emphasis has been placed on 'reason', whereas subtleties of perception and action (and their interaction) have received lesser attention.	This is what intelligence tests try to do -- and we know all the biases related to task selection etc. Frankly, I do not think forcing the multitude of abilities that result in intelligent action onto a one-dimensional scale is very useful.	Most of the points characterize the situation well, although I would replace "insufficient knowledge and resources" by "incomplete knowledge and limited resources", as this needs to be sufficient.		I am not convinced that it is a good strategy to investigate general intelligence over a wide distribution of tasks, domains, and types of agents before we do not have a good grasp at understanding more specific cognitive abilities. Pragmatic general intelligence measures cannot provide more than a superficial indication about how well typical tasks can be performed.	I can agree as the statement is vague enough not to be controversial. We now have to ask what is appropriate, what exactly is meant by foresight, and what constitutes the environment.	I believe that we have overrated the role of rational decision making because this is what we could treat best as rationally trained scientists. But does it not appear almost self-evident that commonsense action seen in formally untrained people, in children, and in some animals would not involve the type of reasoning we perform in machines? We have underrated the role of a multitude of influences from the environment, other agents, and sensitivities -- too complex to force into a formal model. I see rational action as one of several important components of cognitive performance.	Definition would be OK if everybody was 'intelligent' through the same set of strengths and their interactions; however, there are many different ways in which we can appear to act 'intelligently'. Thus, we tend to call agents (including humans) 'intelligent' even if their cognitive abilities are restricted to specialized domains.  	
And maybe, for most of its applications, the capability of being empathetic.  	I think very few have success in a wide range of environments, but many can be extremely efficient in a very particular set of contexts. So maybe intelligence is not that much about adaptability. 	I think the main difference of the AI is that it can augment its average capacity in a scalable basis, while humans remain limited by our biological state.  	If its goal is to perform like an average human, "optimal behavior" is not an expectation, since we can't expect it from our kind.  			Maybe "wisdom" can foresee much further than pure intelligence. A "wise" system –human or artificial– could be better on the long term. 	Is just intelligence being efficient with much lesser error margin, given to more processed information. Humans get it –mostly– from experience and machines from data. 	Maybe that is just one of its many applications. 
	Who defines the goals?					Learning imply experimenting, which imply making wrong assumptions sometimes		
	Too simple		Again AI is not a copy of HI				I wouldn't relate AI with HI. AI should include learning, reasoning and planning but perhaps in a different way we do.	I like the ethic part of the definition but find it incomplete
								
								
							Machines never will be exactly like humans.	
computations are prerequisites for an intelligent behavior	the adjective "wide" is fuzzy					if are actions are based on emotional aspects or common sense, does that mean we are not intelligent in those situations?		From the human-centered perspective, I believe so. However, if humans stop existing, would intelligent systems stop existing too? (very philosophical though)
Ok, but this seems too abstract/general to be useful.	Ok, perhaps a bit unsatisfying but I mostly agree.	Ok, but it misses the guidance for such adaptations. How to define the goal?	This is a good definition for human-level AI, but what about super-human AI?	What does "pragmatic" add here? How do we account for the diversity of environments and goals, so that we can claim an AI to be "general"?	This is just circular.	Ill-defined stuff: "best possible" according to which goals and criteria?	"Common sense" is ill-defined. It seems to me that relying on it to define intelligence is just kicking the can down the road. The rest of the definition does not address the goal of learning, reasoning and planning. I think that understanding where goals come from is essential for a useful definition. I agree with "wide range" of domains, but again this idea of range is ill-defined.	This is not a definition, it's a wish. It is easy to imagine machines with more autonomy and self-interest that would fall under the categories of AGI, human-level AI, etc... Maybe they are not desirable to humans, but that doesn't make them less intelligent.
				One issue is that this definition does not care about new (unexpected) problems.				
			Should not refer to human intelligence only.		Quite empty definition as "appropriately" is not defined.	AI does not only deal with rational problems, but also for example with sensory-motor problems, etc.	Should not refer to human intelligence only.	AI does not only deal with rational problems, but also for example with sensory-motor problems, etc.
								
						emotions play an important role 		
						Too narrow. Emotion and intuition also play a role in intelligence. Furthermore, by reducing intelligence to one action in one situation we reduce the problem to toy examples -complexity must be part of the definition.		
For simple enough problems, it is possible to perceive, reason, and act without needing intelligence.		Including adaptation or learning in the definition is good. Takes into account limited resources.	Focuses on goal-oriented problem solving while taking into account limited resources. Using humans as a yardstick may be difficult, however.	Focuses on goal-oriented problem solving while taking into account limited resources. Does however not say anything about the type or range of environments.	Leaves open the possibility of functioning appropriately and with foresight in a very simple environment, which is possible without intelligence.	Except for very limited toy problems, optimal actions are not feasible to compute for an intelligence with limited resources (including humans).	Using humans as a yardstick may be difficult, if the compared system has intelligence of a very different type.	If the human component of the symbiosis provides the intelligence, it may not be necessary for the machine to be intelligent in order to be useful as a guide.
Some of the simplest organisms perceive and act. Maybe they also "reason" a little bit. Interesting thing for AI is, to what degree is the AI capable of doing these things.	Intelligence measures an agent's ability to achieve goals in complex environments. Does not need to be "wide range". Difficult to define what is and what is not "wide range".	There does not need to be "insufficient knowledge". Sometimes the knowledge is insufficient, but not necessarily.	There are and will be interesting and important AIs that have a narrower intelligence than humans do. Human-like robots is not the definition of AI either.	This definition leaves many things open. It is a bit vague. Not wrong, but not right either.	I like this. However, I would also think it to be a great success of AI to produce non-intelligent robots that would match e.g. the capabilities of a rabbit, a not-that-intelligent an animal.	Human intelligence does not guarantee "best possible action". Neither must AI.	In my opinion: AI does not have to be *human* intelligence, and there are lots of other forms of intelligence. Current AI already far exceeds humans in some respects, and does not match them in many others.	This definition is not strong enough.
			The human might be an adequate example of intelligence, but should not be part of its definition.	Way to convoluted and unprecise.		The hypothesis of the "rational agent" has been disproven over and over again. It should rest in peace.	The definition relies heavily on the term "common sense", which itself is not well defined.	This is way too weak. In that sense every GPS navigation system would be "intelligent".
Too wide again, and contains the assumption that what makes it possible to perceive, reason, and act is computation!  I would have added a modality like e.g. "...the computations that can model what makes it possible to...".	Very, very open!	Very pragmatic!  However it says nothing about performance (what is performed, and at what cost -conception, and for what cost -application).	Better.  It starts as open as previous proposals, but it adds cautionary conditions like variety, and it does not ask for best solutions, not even rationality.  Also, reference to human is a slippery one ; intellectually disabled persons are still human beings. 	Mildly disagree again.  Pragmatic is always efficient in some way, like e.g. greedy algorithms (in some way is important).  Pragmatic is also often very common sense.	The notion of foresight is interesting, but many AI systems kill this idea by obstinately enumerating possible futures (e.g. game systems).	Mildly disagree again. E.g. best is really too open.	I could have chosen "mildly disagree" if it had existed.  The reason is that I think this definition is too vague and largely subject to interpretation.  E.g. nest building is an instinct, but building a nest requires to adapt to available materials, and their shape.  Too much is hidden in words like abstract, wide, information-processing (certainly an ant does that), reason, etc.	Really the opposite of the previous one.  Does a lawyer apply to the definition? "Working as a lawyer involves the practical application of abstract legal theories and knowledge to solve specific individualized problems, or to advance the interests of those who hire lawyers to perform legal services." [Wikipedia]
A bit weak and no performance optimization, I.e., no evaluation criterion 	Too weak. Is path planning intelligence?	Performance weakly defined	It's great to reach human level performance, but it's greater to be better.	It's OK if you normalize the performance by the computation required.	What means appropriately? Performance ill-defined. 	Because we want to reach optimal performance levels	Matching humans should not be the defined goal. 	
	See my comment on some people being more able to tackle different environments than others and being more generalists. Also, intelligent systems can't be specialized? 	A decent pragmatic definition.	This seems overly human-centric. Even within the human population, there are many savants and other people who are good at far fewer things than others. These are arguably no less 'humanly intelligent'			While it's not a bad start of a definition, clarification needed on 'best possible action' and 'situation'. 'Rational action' too..	General intelligence' is defined vaguely. Many of the other terms by which it’s defined require their own clarification.	Extremely human-centric. Ideologically disagree with this.
not sure the mechanism that makes the system possible to perceive, reason and act needs to be narrowed down to computations.			I like this one, because it doesn't limit intelligence to optimal behavior	same as above, too short-term view.	Somewhat relying on other terms that need to be defined too.	intelligence can be related to more long-term goals or a complex development in the system - not every action needs to be best possible for a (short-term) situation.	as a goal - I don't think we necessarily need to match humans in GI	This is an interesting normative one, because it doesn't see the AI research as neutral (for the sake of making intelligent systems), but relates it to helping humans. 
"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.	"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.	"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.	"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.	"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.	"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.	"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.	"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.	"Artificial intelligence" is a pair of words that the author is free to define how he or she pleases in order to convey an idea. There is no need for the community to "agree" on some official definition. I strongly agree with every reasonable definition, and disagree with any claims that some definitions are "wrong". If AI, to you, is only breadth-first search, and you define so clearly, I have no problem with that. If one says that someone else isn't really doing AI research when studying breadth-first search (or genetic algorithms), I do have a problem with that.
							Common sense is evidence that a machine possesses true "meaning" of the world around it and that it is not just manipulating symbols (which then must be interpreted by a user). The importance of this is that Meaning allows a machine to adapt appropriately to the 	There is no such thing as "unbiased" guidance.
The definition is lacking the purpose of action (goal)			How about beyond human level capabilities? Are capabilities the same across different domains?	I agree with the definition except that the statement about the prior is ambiguous. Is it a "true" prior over the respective spaces or the agent's belief, or something else?	"Appropriate function" is not well defined, otherwise sounds good.		Intelligent behavior may range outside information processing challenges	
AI isn't the study of ... . If anything it is whatever this claims to study.	Not necessarily about achieving goals.	Too many words that talk around the subject.	Too much like a "kitchen sink" definition. 	Too complex.	AI isn't an activity to achieve AI. The second part suffers from inclusion of the words "appropriately" and "foresight."	The words "rational" and "best" make it much less useful.		AI is not necessarily about machines giving advice to people.
Insufficient, but interesting.		Why "real time" ?		I don't agree with the definition of intelligence as to achieve goals. Is asking question a sign 	Doesn't say much though…	Depends on the criteria to be chosen, and the definition of "best".	AI should not be thought as to be compared to human intelligence. 	
Sure. If you are defining the field, as it currently stands, of artificial intelligence.	No need for goals to be intelligent.	Adapting, insufficient knowledge, finite processing, and real time are all parts of it, but don't hit everything. 			Don't need to act appropriately. Can be really intelligent and act inappropriately. Foresight is probably required to achieve human level intelligence however.	Best possible requires a cost function, which is impossible to define well. You can be intelligent and make decisions suboptimally, or based on emotions, or flip a coin. 		Anything that is intelligent will be biased and irrational to some extent. If it's not, I would argue that it's not very intelligent...
								
The separation between these three spheres is maybe what AI and cognitive science can help us overcome. 	Good but very broad. Also, the notion of a good it too much clear-cut. 	This definition seems much more grounded. Yet, it does not specifically apply to  AI. 	This seems to be too narrow of definition for AI in general. AI may display capabilities which are alien to human intelligence. 	Computational resources (and resources in general) should be taken more and more as a good indicator to define intelligence in a finite world. 	Same reason as above. 	"Best" in this context is a very dubious notion.  		Way too narrow. 
	Some agents are near-optimally adapted to achieve a fixed set of goals (behave intelligently) in a relatively static environment.		Stating the goal of other entities (some researchers) does not belong in the definition of AI itself.	This is essentially correct, but could be clearer.	Incomplete, over-specialized definition.	The "best possible action" assertion is empirically false, but this definition at least comes close to implying a correct/viable one. 	This answer is missing essential information, and does not directly specify or imply the meaning/basis of intelligence;  It is generically correct, but obviously insufficient.	
								
See my answers above. This is a narrow and partial definition with a built-in architectural assumption (and a rather old-fashioned one too).	It depends what you mean by goals.  	I am not at all sure what 'relative rationality' is supposed to mean so I do not agree with the last sentence.  I certainly agree with the adaptation bit and the ability to function acceptably with limited knowledge and other resources. Also the ability to interrupt for new tasks and learn from experience. 	I do agree with some of this - certainly that one does not need 'optimal' behavior, assuming we know what this is. I am not at all keen on its task focus. I do not accept that the single goal relates only to humans. 	It depends what you mean by goals. Homeostasis gives one a different view of this space.	Better. I'd add 'responsively',  'over the long-term' and 'an appropriate degree of foresight'	This ignores procedural and process-oriented views of intelligent action and supposes a narrow logic-based definition of rationality not to mention  optimality.  It separates agent and environment and neglects interaction between them (including other agents aka social interaction). In addition it fails to consider the nexus of affect and cognition.	Because it neglects embodiment, situatedness and interactivity, implicitly buys into Cartesian dualism and the very disputable computational metaphor. It is also heavily individualist and human-centric  - matching a dog would be a sensible goal in some contexts and collective swarm intelligence is also a consideration.	Again, heavily task focused. A world of batch processing. Maybe the agent is able to help a social group function more smoothly for example. Or cheer up an elderly person with dementia. And the idea that humans should expect to achieve optimal outcomes in most areas of activity seems very odd - 'optimal' cannot even be defined in some cases. Consider driving a car from Edinburgh to London - you would never know if your choice of route had been optimal in terms of time since you cannot know how long an alternative would have taken. And you might in any case have a set of preferences, and you are unlikely to know for sure if they were met optimally either, assuming that they can all be met optimally in one go. Why is satisficing not mentioned at all on this page?
			Some of the human cognitive capabilities are not perfect, we didn't have to copy birds' wings flapping to fly faster and higher.					
								
								
It seems to me that we do not yet fully understand what the involved sub-processes might be so I think the definition should stay away from such "internal" concepts as perceive, reason, and act.	To me this is a "lay persons" version of the one I like most above. This one can act as a simpler, summary version of the more detailed one above. But this also means this one can more easily be misunderstood.	The good thing this definition adds is the adaptivity and insufficient knowledge (and partly but less so the "insufficient resources"). In my definition I would merge these aspects into my preferred definition above.	I don't think a definition of Intelligence or AI should focus on specific capabilities of humans; this is an unnecessary bias/complication.	This is the best one to me. Quite general and requires more and broad knowledge of the person "reading" it and taking it in but it is more precise and explicit to me while also focusing on the right aspects without adding unnecessary elements or human biases. I still don't give it top marks since it somewhat lacks the elements below	"Appropriately" sounds a bit too vague to me, it kind of begs the existence of something, external to the definition, which need to then clarify what appropriate means in each specific circumstance, but without the definition making this clear in itself. I think the more explicit definition below with "distribution over spaces" is clearer, if more demanding to understand for a general public.	Less clear than the ones with distributions and goals above since it does not say where the definitions of "best" should come from.	Again, the mentioning of "common sense" makes this definition unnecessarily concrete to me. It imbues the definition with a certain bias to be about capabilities and goals that humans have. This is unnecessary to me.	I don't think the definition should be about servicing humans. That is more about how this technology is used; an orthogonal, albeit important, question imho.
"Computation" is too loaded of a word	Vague, but open enough to include most of what we tend to mean by AI.	This is clear- I like the addition of "relative" to rationality, but most of our best psychology and neuroscience tell us humans aren't "rational" the way philosophers have always defined the term; until this is clear, no definition that relies on rationality is going to work right.	I quite like this one. It's specific and clear, and shows what most of us in AI mean when we're working on this problem: not a tiny problem in a specified domain, but human-level adaptability and response.	This is convoluted and not what most people mean by these terms.	Not a bad definition, but, of course, a bit vague.	Humans aren't rational, even in intelligent action; why should we make machines be rational if we want them to be like us?	Not a bad definition, but "information-processing" is a loaded phrase. How often is "information" defined for us in these definitions? Almost never.	Service to humans has never been the goal of AI that I know of.
First, I believe that AI is a field of study, and it should not be defined in terms of the words “artificial” and “intelligence”. Rather, AI is a label proposed by our colleagues at the Dartmouth workshop, which should not be confused with human intelligence. The niche that discusses human versus artificial intelligence belongs to philosophy, and not to computer or information sciences, or even organizational sciences. Second, the goal of the field of AI, as proposed by Russell and Norvig (2009) should be that of “rational behavior”. In this sense, we are looking for computer programs that can reason, perceive, and act. To do this, as other colleagues have studied, we need something called “knowledge”. Not necessarily expert knowledge, but that construct that allows us to make rational decisions to take rational actions. In this sense, I bring up Simon’s colleague, Huber, who has defined decision making as “the process through which a course of action is chosen” (Huber, 1980, p. 9), which he proposed to model through the three steps of intelligence, design, and choice. There is a direct relationship between the elements from Winston’s definition of AI and Huber’s definition of decision making. Perceive is associated with problem identification; reason with generation of alternatives, and act with choice. Based on these observations, one could define AI as the field that studies automation of decision making, which is another way to interpret Winston’s definition. Finally, I’d like to point out the definition of knowledge from Alavi and Leidner as a justified belief that increases an entity’s capacity for effective action (2001, p 109), which is also an important component of producing rational behavior.	"Wide range of environments" is vague and does not take context into account. Besides, this is attempting to define intelligence, not machine intelligence or AI.	This excerpt is valid. It may not be ideal as a definition, but it is valid to explain one. Peter Jackson's (1998) book proposes 3 Necessary grounds for computer understanding: Ability to represent knowledge and reason with it; Perceive equivalences and analogies between two different representations of the same entity/situation; Learning and reorganizing new knowledge. These are important aspects to expand the goal of rational behavior while perceiving, reasoning, and taking action.	The risk of using humans as references in quality is too high. Using humans as references of quality would immediately eliminate the possibility of ever validating any learning from big data, for example.	This is defining pragmatic general intelligence, not AI. And it does not seem to be doing a good job.	The concepts of intelligent, intelligence, appropriately, and foresight are too vague to be used in a definition.	The positive in this one is that it brings up rational behavior. It still requires a clarification of "best possible". Winston's is still more complete as it indicates the tasks perceive, reason, and act.	The term “general intelligence” is (very) vague. Besides, the risk of using humans as references in quality is too high. Using humans as references of quality would immediately eliminate the possibility of ever validating any learning from big data, for example.	The evaluation of a behavior as rational can be defined as doing the “right thing”, given what a system knows (Russell and Norvig, 2009, p 1). Therefore, providing service to humans depends on the mission those humans have. I again bring up the concept of knowledge, and argue that knowledge is context- and mission-dependent, and accordingly, so is the act that may lead to optimal outcomes. For example, knowledge about a culinary recipe that is applicable for baking may seem useful to any agents that share the baking process, such as restaurant chefs and hospital nutritionists. However, the same belief may lead to effective action for one and not for the other because their missions differ. While the chef strives for flavor, the nutritionist has very clear constraints to meet for each patient depending on their condition. This suggests that knowledge is mission-specific, because the same belief in an organization may lead to effectiveness in one department and not in another (e.g., marketing versus finance) due to different departments possibly having different missions. In sum, an optimal outcome is relative, and for this reason would not be ideal in a definition of a field.
The definition is about a research field, not broad enough. There are many existing systems which can perceive, reason, and act in a limited "quality". How can we call them "artificial intelligence"?	"Goals" is not clear.	I don't like word "relative". It is relative or absolute.	Too long.		Artificial intelligence should not be a process, not an operation, not an activity. It is an ability.	It is not clear in word "best". Moreover, not the best is still ok in intelligence.	Seems rather good.	My boys cannot do such things in this definition, but are they intelligent? Does the target of machine intelligent go beyond my boys' ability?
Not quite broad enough	It would be nice to achieve goals in just one environment	This excludes maths as intelligence	General purpose AI is a rather silly idea	It is possible to achieve goals without intelligence	Most of what we call AI is algorithms, not intelligence	Intelligent people are not particularly rational.	Matching humans isn't interesting	Machine learning is spectacularly biased
								
								
	Achieving 'goals' is not a central part of much AI work. Vision, NL comprehension, learning: none of these have to do with achieving goals. 		AI work has little to do with the fantasy of building a "human-level" intelligence. The term is meaningless in any case: intelligence is not measured in "levels". 	This is meaningless jargon. What "prior distributions" are being talked about here?		Again, this is meaningless. What counts as 'best'? Best for who?	This is an important goal, but many active areas of AI research would not fit under this definition. Moreover, the term 'general intelligence' is meaningless and should not be used in a definition. 	
too narrow	succinct	relevant and pithy	we don't need artificial humans we need smart machines (preferably smart in the ways humans are not)	overly wordy and not necessarily relevant	"appropriately" is vague, but "foresight" is relevant	define "best"?	we don't need- and would have a difficult time making- artificial humans	this is exactly the direction I’d lean towards in a definition- a complement to human intelligence
This is in line with definitions of AI in biology and, more generally, evolutionary epistemology.			We may also want to build specific systems that do not span the entire range of cognitive tasks.	Computational resources do indeed play a role in efficiency.		Rationality is very hard to define, except in a very limited setting with well-identified boundaries.  All intelligent agents, whether human or artificial, have computational capacity constraints and therefore cannot be expected to always take fully optimal decisions.	This sentence is not a question.	From an economic perspective this is a very naïve statement. Machine intelligence is owned by firms.  They have their own profit-maximizing motives that do not necessarily match with optimal outcomes for humans. (PS: I am an economist)
								
The definition depends on a previous definition of what constitutes "perception", "reason" and "action". All three of these terms can be narrowly defined as human domains, or wider as things that can be carried out by a machine as well. (For instance, "sensing" could mean machine perception, while in a different understanding, perception needs to be embodied). Defining Artificial Intelligence as "the study of the computations.." seems also problematic, because the computations themselves are part of what most people refer to with the term AI.	This definition is very broad, but if used in this way it could hold for a whole range of existing applications of "AI".	this definition seems to describe machine learning, rather than intelligence. However it seems to be part of how machine intelligence could be defined, even though it doesn't cover all aspects.		This definition seem precise as general intelligence is called "pragmatic" and the range of "actions" is confined to a specific area (environments, relative to prior distributions over goal and environment space). However, the definition is quite technical and I would have to learn more to correctly understand it. 	This definition of intelligence does not encompass many of the processes that are currently described as "AI"-technologies (for instance, pattern recognition)	how is "best possible" defined? the parameters for "optimal" would have to be defined first.	It's very questionable that this kind of general intelligence could ever be achieved by a machine. Machine intelligence would never be "matching" humans in general intelligence but rather succeed in simulating certain traits of human general intelligence.	this definition of intelligence also does not encompass many of the processes that are currently described as "AI"-technologies. It seems to purport machine intelligence as merely a tool for efficiency.
Computations as structures that generate functional (mathematical) patterns.	Terribly incomplete. What if the goal "shouldn't" be the goal?	Best characterization, so far, but most "definitions" still miss the mark as definitions. Definition? By what authority?	AI shouldn't be limited to an ersatz form of human intelligence.		Creates a self-referential paradox.	The problem, here, lies in the assumption of obviousness to rationality.	Agree with the statement, but it provides rather a poor definition.	Competitive co-evolution between humans and artificial agents.
						The definition is quite simple, focused on a single aspect that seems to embrace the essence of intelligence (what matters is what an agent does).		
								
1. Learning and relearning capabilities 2. Generative vs collaborative models.	Learning can happen from surrounding as well as past experience.				It can learn both on supervised and unsupervised methods which makes learning capability and that makes it intelligent as they built new knowledge from the existing one which are called derived rules and knowledge 		It's true that machines can compete humans in some specific aspects because they are mostly case-based as of now. But not on a holistic background as it has to evolve further.	Agent based programs in Machine Learning makes it interact with case based expert systems which makes it possible to come up to a higher level of intelligence. 
"Reason" could be more fleshed out. A thermostat could arguably pass this definition.	This is a bit better in that it does not include "natural" environments. Does not include complexity, though.		It does talk about complexity, but as I said before I don't think complex physical environments are required.	Almost perfect, but should have said environments are complex, otherwise the thermostat is in. Although we can also think of a continuum going starting at 0 complexity/intelligence.	A thermostat could arguably pass this definition. It is not enough to function appropriately and with foresight; the environment needs to involve a certain capacity and the mechanism requires a certain generality.	Very good, but again, leaves complexity out and the thermostat in.	I don't think the domains need to be that varied. An AI in a simulated, non-natural world without appropriate sensors that allow it to function in natural domains could still be considered intelligence. Also, "common sense" is a bit of a cop-out.	No humans required. A robot could in principle operate for itself (WALL-E, for example). Also, AI may provide not completely rational and unbiased guidance and still be useful.
Too many AI definitions include words like "perceive", which bundles the phenomenon of sensation, erroneously, in to systems which can generate new Knowledge quanta from an existing network of knowledge quanta and or through discovery of new information.  Sensation and the much higher order phenomenon of consciousness is most likely orthogonal to what is being discussed as intelligence. They are not dependencies.	This again seems to be a kind of surreptitious way at returning the measure of intelligence towards a set of human constraint domains. Similar to the "common sense" language from.  It seems overly anthropomorphic to require the magnitude of intelligence to correspond to human concerns. Isn't it just as likely that we will invent synthetic intelligences that far exceed our own capabilities but are by Design, domain specific savants.	Seems too much to bias toward the premise that intelligence requires an embodied physical agent	See above.  	It seems entirely reasonable that intelligence as a function of an agent in an environment is something that can exist independent of biology and other norms associated with intelligence.	This definition is flawed because it presumes that an intelligence must be operating within an environment. If one accepts the definition of environment here to mean a corpus of knowledge or a corpus of predicates which can be operated on computationally, then sure you can include the requirement of an environment.  Again, all of these definitions neatly skip around the fundamental failing of the field to address the distinction between sensate experience as a phenomenon and knowledge processing/knowledge discovery.	Seems too limited in constraining the sort of "predicate Corpus" to preconditions for Behavioral choice. (And I'm acknowledging that non predicate, non-Bayesian systems have ascended but I still have the mental model of chunks of knowledge quanta being some form of subject predicate object amalgam) 	Again, phrases like "common sense" seem to be a function of software adaptation towards increasingly day today and human outcomes.  These are intelligences, and they are adaptive and can learn. But there's a danger in consistently leaving unsaid the likely reality that there is no self or sensate phenomenon involved at all.	Hair the reduction of artificial intelligence to an agent, which can simply be inferred to mean a knowledge processing system interacting with one or more day two sets and or making inferences seems appropriate. The highly anthropomorphic nature of what we're calling machine intelligence seems perhaps an inevitable function of the design pressure on software to mimic intelligence as it’s expressed in humans in a servile roll.
							The sentence is incomplete and makes no sense.	
allows the computer (or machine for that matter) to "behave" like people. I didn't choose "strongly agree" because I don't think that AI is "the study of", but the result of that study which makes it "act" like human	I don't think that intelligence is only measured by achieving goals (even when a goal is somewhat general term)			Very technical definition that imposes many constraints on the definition	This definition makes AI be somewhat neutral and "emotionless"			Only because it refers to "Machine Intelligence". If it had said "AI" than I would change for disagree, as this definition is quite mechanical...
This definition is meaningless. You need to define first what you mean by perceive reason and act	Still needs definitions but is so generic, applies to animal and human agents even biologic agents	See previous	Applies to any agent as the previous.	You cannot just take these "definitions" of AI out of their context a still hope they make sense. This is probably correct in whatever paper you took it from. But makes no sense without that background	This one is better. Firstly AI is indeed an activity, a field of study, not a "thing" . In fact, talking about "an AI" just identifies the people that have no idea what they are talking about. For the rest this definition suffers from first problem of the previous: define foresight first	Applies to any agent. Furthermore intelligence is NOT only rational action	Again. Definitions!!!	Has all the problems of the previous ones multiplied . Every word here needs definition. And agents are not necessarily machines.
It is not verifiable								
								
This is a definition of a scientific, not a technological goal, hence I agree. But the method may not involve "computation" in the usual form, thus I only agree, but not strongly.	Well, I would agree, but (1) that's the opposite of what intelligence tests do (they use a specific task/environment setting), and (2) most intelligent acts cannot be "measured"			As above (certainly, intelligence knows nothing about distributions)	I agree, but apparent foresight may not be such a crucial component.	OK, but the best *possible* is not necessarily rational, from an external point of view.	Correct as a description of what happens (sometimes), but most probably not how things actually work.	Not rational, not biased, not optimal. Otherwise it's OK !
Nilsson meant "[The subfield of computer science called] Artificial Intelligence...."  He is defining an intellectual endeavor, not "AI" in a context-free sense.	The word "measures" is obscure here -- how is "intelligence" a measure of anything?  Intelligence is an attributed capability.  What might be needed is a measure of intelligence.	This is a better definition but there's no reason to say "the essence." It's just a perspective. Again context is missing, the essence in what situations or domains?  Notice again the mention of tasks. Of course the list of requirements is generally useful (e.g. real time). But it is not necessary.  A weather forecasting program that has high accuracy five days in advance predicting hurricane paths need not operate in real time. What does "real time" mean?  Is an hour to produce a five-day weather forecast real time?	"The full range" is absurd -- in what context is this desirable? Why restrict intelligence to pursuing "tasks."  Is writing a poem a task?  Is dancing a task?  Not every human activity requiring cognition is a task, i.e., having a functional purpose.  Reducing human activity to defined goals and tasks is putting blinders on the nature of intelligence.	The idea that intelligence is about goals is too restrictive.  Many human activities do not have defined or definable goals, e.g., watching TV to pass the time and be entertained, dancing, listening to the radio while driving. Human motives requiring intelligence to pursue are broader than the intellectual goals used in AI research. 	Again, this definition describes a human activity, an inquiry. It is not about "intelligence" or AI as an object, an independent thing like planetary bodies or trees.	See above remarks -- Reducing human activity to rational action is putting blinders on the nature of intelligence -- consider art, poetry, singing for pleasure. Ignoring the aspects of intelligence brought to bear in non-logical, non-functional activity is to ignore aspects of intelligence brought to bear in rational, goal-driven activity.	Matching human intelligence is one perspective. Complementing human capability is very different and arguably more valuable, economically and practically.	This is too limited. Perhaps I want guidance that fits my biases. "Optimal" is not defined -- does it pertain to cost? time? resources consumed?
								
						Totally disagree, my 5y-old daughter is very intelligent, but totally irrational.	I disagree with the starting statement:  "Machines matching humans in general intelligence", as I believe machines will never match human-level intelligence and will never have what we understand as "general intelligence". However, machines already have a particular kind of intelligence that is (and will always be) distinct from ours in the same way that the intelligence of ants in an ant hive will always be totally distinct from human intelligence".	
Not the study but the thinking entity has AI. Acting is not absolutely necessary, e.g. an autistic or paralyzed person is not stripped of his intelligence	Thats the definition of a gifted person. Intelligence in a single field should be possible too		Is it necessary to create an entire person with a wide range of abilities? It is certainly useful for many problems, however compensation through the combination of different AI entities depending in the problem at hand is possible			See above action and lack of AI definition	Minus for choice of words. AIs do not necessarily need to think like humans do.. definition of intelligence ok but no differentiation between natural and artificial	That is one aspect of AI
			Goals may be to go beyond human cognitive capabilities e.g. swarm intelligence			Don't agree intelligence mainly concerned with "rational" action, "best possible action" may be open to interpretation		Consideration - unknown biases may exist in data
I think "general intelligence" or "artificial minds" is a better way to describe this.								
It is one piece of the puzzle, AI like HI is dependent on the environment, but there needs to be a way to adapt through learning and action via computation and memory of a domain.	Intelligence is so much more than a measurement. It is an emergent property of a optimally functioning neural system.  Intelligence is a part of us and any AGI will have a similar intimacy with the machinery or algorithms  (just like us with our heart beats) within that environment.		The higher the intelligence, the more optimal the behavior is within a particular domain. When we transfer domains to solve problems, then you will see inefficiencies which define human behavior; however with experience we adapt quickly.	This divorces the necessity of life from general intelligence. 		Only in a single domain, but this is not the case when we switch environments.		I tend to think of MI as weak AI  this includes ML and DL. HI cannot be saddled to a select few systems, the human body is so much more complex.
								
Similar to a definition of human intelligence	simple and succinct		confusing	Too long winded	As above	Humans are intelligent but not always rationale		Too vague
								
What you have is a definition of a field of study, not a definition of the goal or object of study.  	This captures aspects of the flexibility to function across some range of contexts, while leaving open how wide we need to be and which goals we might have.		This is a bad goal-- see comments given above about matching human intelligence.	With such constraints, anything or anyone can be viewed as intelligent, given their computational resources.	This is more on the mark, especially as it is a definition of AI, and leaves open the complex questions of what functioning appropriately entails.  	This is too simplistic and ill-defined to be helpful.  Philosophers still debate what is rational, much less what is best.  It is unhelpful because unlike leaving open what the goals of the particular AI are, this makes it seem like those things are settled.	Human intelligence is tied up with emotion and embodiment.  Reducing human intelligence to information processing is reductive.  This is why machine intelligence will likely never "match" human intelligence.  Unless we give machines bodies that can feel (emotionally and physically), machine intelligence will be different in kind, particularly in its basic strengths and weaknesses.  Attempting to match humans is an inappropriate goal.	Any AI will have biases that will originate from its constructing parameters and original data set.  This is thus a daft definition that fails to see those fundamental constraints.
It is broader than the "intelligent classification" that DNNs perform, and says something about AI in the world (because perception implies an external environment, and acting implies interacting with it.	This is rather vague: and is rather a movable  feast, because the goals and the environments can be moved as required. Yet this is essentially what has been happening in AI over 30 years: the goal of playing good chess, the goal of playing good backgammon , etc. Still, not terribly useful	I do like the real-time issue, and the idea that the information being used is incomplete. 	It implies that the intelligence should do the same things as the human intelligence. But I do like broad competence. 	I do like the pragmatic side of this. It isn't necessary a useful definition, however, because it measures achievement of goals - without any specification of the nature of the goals or the environments, nor of how the AI gets to have goals in the first place. 	Implication is interaction with environment. But I'd like something more about perception. I find this very similar to the definition above. 	I think this is more like the definition of some kind of optimizer, rather than of intelligence. 	AI does not need to match humans’ intelligence: it's likely to be different in some/many respects. Common sense is not easily defined. Yes, the domain issue is good, but altogether, I prefer the earlier definitions.	While unbiased guidance is certainly useful, and while helping to achieve optimal outcomes is also useful, I do not think this *defines* machine intelligence.
								
It is the study of the simulation that makes it possible to perceive, reason, and act	Most agents are useless in general domains	True. An ant qualifies.	Agree but unlikely due to nuance and adaption required	Pragmatism being the key word	More accurately framed	Not the case with humans	Since consciousness creation highly unlikely a wasted pursuit - see Penrose and Hameroff	Generally agent based and nearly always task specific
								
Any system that has sensors can perceive.  Any system that can calculate can -- by some definition -- reason.  Any system that has output can act.  Most such systems have no appreciable intelligence.  	Too vague.  A rat living below a city can achieve goals in a wide variety of environments.	This seems like a good start towards a goal for AI.  Whether it describes human intelligence is another question.	AI research does not have one goal.  Different AI researchers have different goals, and that's good.  Some are trying to design software that can beat people at specific games.  Some are trying to make cars that can avoid collisions.  Some are trying to find data hidden in huge data stores.  And, yes, some may be trying to match human general intelligence.  But since the latter goal has not been defined in any objective and agreed-upon way, it would not be possible to determine if one has achieved that goal.	This definition sounds like a computer scientist trying to define intelligence without reference to hundreds of years of psychological research in psychometrics and intelligence.  Psychologists have been studying, trying to measure, and trying to define human (and animal) intelligence for over two centuries.  Let's not ignore that.	Much AI research isn't about making machines intelligent.  E.g., some AI researchers are working to create robots that, like cockroaches, rats, or ants, can find their way through difficult terrain (perhaps to search out routes or to find people trapped in rubble).  Ants, rats, and cockroaches are not normally considered intelligent.  AI research often developed techniques, such as alpha-beta tree pruning, that are useful for a variety of problems, even though those techniques are not examples of -- or even components of -- intelligence.	Intelligence is not concerned mainly with rational action.  Most of human behavior is governed by fast, automatic processing (Kahneman's System One) based on pattern recognition.  The combined effect of all these automatic processing keeps us alive and constitutes the bulk of our human intelligence.  We and a few other animals also have the ability to reason rationally in certain situations.  On the other hand, one could adopt rational action as a goal for artificial intelligence, rather than stating that it is the main concern of all intelligence.	I disagree because the definition assumes human intelligence is a specific goal. Which humans?  Under what circumstances?  People vary greatly on these criteria.  The bell curve for IQ tests has long tails.  Also, people also do things that raise or lower their ability to do these things, such as drink wine or exhaust themselves.  People in a burning building tend to be very low in their ability to reason and meet complex info-processing challenges.	This might be a good goal for AI to aim for, since people are usually not rational in their approaches to problems (Kahneman, Thinking Fast and Slow).  With this as a goal, AI could aim not to replace people but to help correct for human irrationality, i.e., to supplement and assist people.  It's more-or-less the argument seen repeatedly on the old TV program Star Trek, where both the irrationality of Captain Kirk and the rationality of Mr. Spock were required to solve whatever problems faced them.
								
it is not clear to what extent perception, reasoning and action imply intelligence and neglects the important relationships among the three abilities.	Ignores the potential side effects of achieving the goal, e.g. to survive the actions.	can hardly be used as a definition.	Too complicated, is copying a human's intelligence (which one's?) satisfactory?	Too complicated!	leaves open what it means to "function appropriately".	lacks context!	Isn't a key ability of human intelligence that they can give reasons for their behavior/actions?	this definition is strongly oriented towards a service to humans which does not completely cover all types of intelligence.
This recurs to a rather old notion of AI in the sense that the focus is put on 'computations' that back (rational) action. In language there are 'moves' which might be computed and constitute actions their own right. The divide between a 'backing computation' and an overt (speech) action is misleading for these kinds of intelligent actions.	This definition basically moves the 'burden of definition' to the notion of 'the environment'. Without saying what one conceives of as environment, whether there  are different types of environments, etc., and without drawing limits to potential environments, the definition is not of much use.				This definition basically relies on the notion of 'functioning appropriately'. Without saying what precisely that means - and this might differ in different contexts - this definition is just transferring the "burden of definition" to this other notion of 'functioning appropriately with foresight' which doesn't really help matters.			This definition, already from the outset, restricts machine intelligence to be at service to humans. While this might be appropriate for most practical applications, it is too narrow of a definition for machine intelligence in general.
I don't think merely studying computations is enough. It should evolve efforts from all sides.				AGI is not about computational resources, but rather the capability to generalize to unseen tasks. It should be normalized by the amount of observations.		Emotional intelligence is another aspect of intelligence as well.		Intelligence implies independent thinking capability. Machines should not be human slaves, instead they should be allowed to develop, grow and create.
Not restricted to computations	Algorithmic enhancements do a lot more than just measure	Processing capacity has never and will never be finite.  As processing capacity at a quantum level has the potential to be in-finite!	Human intelligence is far more than just the accumulation of knowledge and learning. It is more akin to the objective analysis of circumstances and incidents.	There is nothing normal about teaching machines to be superior to humans	convoluted and confusing	Humans do not necessarily act rationally yet they are intelligent	Not matching, the intent is to 'teach' a machine to have superior intelligence to the human brain utilizing 100% of his capability	An agent implies that artificial intelligence works for humanity. Equally an agent of 'double agent' could work against humanity
	This definition doesn’t define whether those goals are desirable or even intended. 				“Appropriately” requires definition and is culturally sensitive. 		What is common sense?	This seems like a wishful goal that is hardly universally applicable 
								
AI should also include emotion and motivation, and creative imagination.	Intelligence doesn't measure the ability, it is the ability.	Then term "relative rationality" is even less clear than "intelligence".		unintelligible!		But motivation and emotion need to be included too.		That is the aim. Whether it's achieved is another matter. (E.g. unbiased...)
"perceive, reason, and act" needs to be defined, but if they are, this might be a reasonable definition.			Description rather than a prescription.	Close to a formalization, which is good. However, there should be deeper reasons motivating and confirming this as the correct definition.			Not much of a definition as much as an advertisement.	This would be necessarily possible, but not sufficient for something to be intelligent.
This definition is too broad. It encompasses very simple agents that are not associated with AI and it does not define what is meant by 'reason' and how AI relates to the context it is placed in.	This definition does not care for the way goals are achieved, how the reasoning works. It would consider the actions of someone in the Chinese room to be intelligent.		This definition rejects AI research that is not based on human-intelligence, such as evolutionary algorithms.		Many areas of AI are unconcerned with 'foresight' or an environment. This definition is suitable for areas such as planning, but unsuitable for areas such as deontic logic.		This definition associates intelligence to the context it may exist in.	Under this definition, a self-interested Artificial Agent that exhibited human-intelligence would not be considered to have Machine Intelligence.
						Ideally we should act rationally but that will not always the case, and there is research on irrational behavior and how it may be beneficial		
			I like it, except for "the full range of. .. in humans". I think an AI will have capabilities beyond humans, but will also miss some human capabilities.	too technical	I think this is not specific enough. Unless an automated elevator is intelligent...		Common sense and ability to learn are essential. I also like the term "wide range of ...".	I can imagine non-agent AIs. Notions of "rationality" or "optimality" are somewhat orthogonal to intelligence, in my view.
Again, nice, short and clean. However, my homo-sapiens sense is tingling: is perception, reasoning and action all there is to it?	Goals in environments, more of the same. But, who sets the goals?	More of the same, only more verbose.	This one is a bit of a hodgepodge, based on mimicking humans, but also without a very clear focus.	Specific to pragmatic intelligence, therefore cannot be used as a general definition. For pragmatic intelligence, I can't really disagree with it.	Functioning in an environment, that's more-less OK for a definition. Very similar to the first one. But there is more to intelligence in my opinion.	Short and clean, but lacks detail: what is an action, what is a situation, who sets up the setup, etc. Also, sidesteps the notion of "goal", but it seems to be in there implicitly (who judges what is "best"?).	This definition implies mimicking human intelligence, which I don't think is vital and ubiquitous.	This definition is more relevant to hybrid human-machine systems than to defining general machine intelligence.
Intelligence is nothing like computation. This fascination with comparing humans to devices is tiresome and antiscientific.	An invasive plant can achieve goals in a range of environments. 	A few problems: "the environment" (as if _the_ environment exists as such, a very strong theoretical to include in a definition, is _the_ environment of a fly the same as _the_ environment of a person? Is _the_ environment of a blind person the same as the environment of a deaf person? Sometimes yes sometimes no, but there is no "singular" environment, which is not to say that I deny the physical world, I most certainly do not, but only that no one has direct access to it). Anyway, other issues include "knowledge" (whatever that is),  I also find this definition unnecessarily heavily focused on work/tasks. 	This is the best one so far, which doesn't say much. Just too bad it's about the ability to "pursue tasks". and "work" and "generate (!) optimal behavior". These are important, sometimes, but don't necessarily relate to intelligence. 	This definition is not of "intelligence" but of "(efficient) pragmatic general intelligence". This is apparently one possible kind of intelligence, but it doesn't even purport to give a definition of intelligence. Since I don't know what "pragmatic general intelligence" is, I better not comment much more on it here. In the event that the author means to say that "pragmatic general intelligence" is the same thing as "intelligence", I still find the definition problematic.	The word "appropriate" just kills it. Appropriate for what, according to whom, in what context? This seems very normative, even as if the entity (here "entity" and previously "agent", this is another issue) had delicate Victorian sensibilities which may be upset if one doesn't act "appropriately"! 	This definition is weak (note the words, "is concerned gained with" ... "Ideally..." ... "best possible"). And "best possible" according to who and for what purpose? This is ambiguous and highly normative. The definition also conflates intelligence with rational action. I don't think that's a helpful approach. 	Very problematic because it reduces intelligence to a kind of "information processing". This is the worst kind of definition of intelligence. Human intelligence is nothing like information processing (to say nothing of the strange distinction between so-called "natural" and "abstract" domains...)	Well, this is obviously nothing to do with what we mean by intelligence in normal language. This is maybe a definition of intelligence as a product.
		"Rely on finite processing capacity" is contradictory to my definition of AI.	An ambiguous statement in my reading...		This is not the significant core characteristic. "Learning", or adapting to unknown, or new, capacities or elements in the environment, is the core.	The uniqueness of AI is that the bar for ""best possible" is raised every time the AI-system is being used. 		The brunt of the challenge  is buried in the meaning of "rational" and "unbiased" in this question. 
			too vague and cognition is not enough	This is a definition of pragmatic general intelligence, not AI, weird choice to add in the survey		how much context is included in 'best possible'? rational is not always best possible...		
Acceptable, but avoids central issue of whether these computations are "the same as" what happens in natural intelligence; or., indeed, whether natural intelligence is computational at all.	Useless because it just kicks the can down the road - how do we measure that ability?	Fairly good, but again, as ever, adaptation is only one aspect of natural intelligence.  What about playful exploration?	It avoids the Turing issue of imitation.  Is 'exhibit' equivalent to 'possess'?	This definition is so overwrought as to be useless, however exact is may be in formal terms.	Too thin.  'Foresight' is only one aspect of intelligence.	It assumes intelligence is only concerned with rationality.  This is wrong and restrictive.  There are many types of intelligence e.g. emotional intelligence, moral intelligence   .....  etc..	Exposes the real problem  -  "common sense" is crucial and un-definable	A more limited assistive role for AI is less problematic, though dull.
								
Even when we cannot deny this assertion, it refers to the research field which is studying what is artificial intelligence among other questions.	This statement in which many ends are left loose offer little space for the debate.	It is more a discussion than a definition. We can debate the ideas herein showed, but likely it will not provide a general definition of intelligence.	Even when I agree some assertions and I would like to discuss some others, this is a set of expected capabilities in intelligent machines rather than the definition of intelligence under the machine's viewpoint.	Since Intelligence is featured as "Pragmatic General" it seems to refer part rather than a whole (even in case the assertion might be true).	I think Artificial Intelligence is specifically a project rather than an activity, but I agree. I am not sure about to refer Intelligence as a "quality" since it will relate the definition of intelligence to our understanding and perception. The subjective experience of perception does not allow a concrete definition (it becomes in discussions even to clarify the grade of intelligence in human beings). I'd be quite interested in to discuss the feature of "appropriateness" in the definition, as well as the function of "foresightedness".	Understanding Intelligence as a capability, the human-being is a counterexample of this statement. Under the viewpoint of Intelligence as a feature this statement might be discussed.	Even when we cannot deny this assertion, it doesn't seem to provide a definition of intelligence. It just provides the explanation of an occurrence.	It seems a broad undefined goal for a punctual project. 
too simplistic	Too simplistic and a repetition of previous questions				Chinese room argument again.	humans are not rational being and are still considered intelligent	matching machine performance to humans' performance does not implies that the machine is intelligence. It may simply overfitting. This lies complete within the well know Chinese room argument by Searle.	This seems more the definition for big data that (machine) intelligence
		It is not insufficient knowledge but rather insufficient information.  Knowledge plus intelligence drives the adaptation.	I struggle with the concept of 'optimal behavior'.  How is this recognized?	The assumption is that machine intelligence will be proportional to the quantity of computational resources.  This is not necessarily true. 	"...its environment." is too restrictive.	Intelligence is mainly concerned with solving problems.  The best possible solution may not necessarily be a rational action.		Struggling with 'unbiased' and 'service' on this one.
								
								
								
								
						It is an almost entirely materialistic view on intelligence which does not cover several important aspects of intelligence		This definition does not attribute the possibility of machines to be intelligent in their own right, only in relation to humans. 
								
								
Too much reason specific, where creativity is not necessarily reason based		restricted to conscious states		Not very clear a definition		Rationality is context specific and observer dependent. 	Comprehensive	
								
AI is not a study of computations					very general, not too specific, basic idea is coming across			implies that machines cannot take intelligent decisions
			I strongly disagree because heading to the human intelligence is so many orders of magnitude beyond our technical capabilities, that it makes the rest of the arguing just not credible.	This is not a good aspect of  intelligence, but rather a measurement of capabilities, w.r.t. a special task.	A cockroach is well functioning within a wide variety of environments (including some foresight). I personally would not consider cockroaches to be very intelligent.	Intelligence does not imply optimality. Nor can an action be rated to be the "best" without having defined a uniform objective function. I have my doubts if a reasonable rating would be one-dimensional, thus there is no "best action".	With a strong disagree to the term "common sense". 	No need to include "service to humans".
It is not clear what constitutes reasoning.				For a general definition the amount of computational or other resources needs to be included in the goals.		The best action may not be rational or be deducted rationally.	There are many ill-defined terms: common sense, effective ability, wide range.	Very narrow
Simplicity.	Sufficiently general.		The range of human capabilities presumably does not exhaust all that is captured in intelligence. 	Cost effectiveness of computation does seem an important criterion. The pragmatic focus may gloss over more internal aspects which may also seem important. 		"Rational action" is too ambiguous. Instrumental rationality, coherence, etc.; all of the above.	Learning, reasoning, and info-processing all seem necessary, but maybe not sufficient? Something like "effecting" or "doing" seems important too.	This could be a strategically useful definition for Safety ends.
The bar is too low. Perceiving, reasoning, and acting are easy.				I like the "normalized by the amount of computational resources" and the pragmatic view	Too vague	There's many cases where the "best possible action" is intractable so this is too high of a bar. Also, this definition is limited to tasks where an action must be made.	"Matching humans" is ill-defined since there is not one axis.	"optimal is too strong"
						Human agent may be joked by his emotions	Not yet, but in a near future 	
								
There are many other aspects of AI	There are many aspects of intelligence. This is only one. 	Over reliance upon the rational.  	I think leaving out some aspects when designing AI is a bad idea, will lead to lack of balance and perverse outcomes. 	not familiar with the terms here. 	I think foresight is important but so is the ability to understand present conditions. 	Two reasons: Awareness that there are things that one does not know one does not know always casts doubt upon rational action. Rational thinking is just one aspect of intelligence. Intuition, somatic intelligence, etc. are others that are equally important.  Even probability research has shown that intuition can be as good a ground for a decision as mathematical or other reason.  See Ken Wilber for more types of intelligence. 	This is not a question or a grammatically correct statement. If the first "in" is supposed to be "is" then I disagree. See answer above re: kinds of intelligence. 	It is not possible to be unbiased. It is possible to  understand and work with one's biases, and to understand that one has unknown biases, being open to learning of them. 
						In many contexts it is practically impossible to determine what is the best possible action. We must bear in mind the context: time available, perceptions, the autonomy that the agent has, ... etc.		
These are all metaphors, and "perceive" is the worst, because it projects human subjective experiences onto the machine the most.	Intelligence obviously includes also the selection of such goals. This definition is a bit limited.	This definition is better than the previous ones in terms of assumptions and gross simplifications. However, I wouldn't call that the "essence" of intelligence.	Is there even an optimal behavior in all situations? "To do what humans do" is a less demanding way to define AI that I like and usually use in my lectures, too.	I'm fine with the characterization that the term "efficient" provides. However, again, there are a lot of assumptions in "prior distributions over goal and environment space" that affect the definition of intelligence.	What does "appropriately" mean? For a person to behave appropriately among others is not the same as doing arithmetic correctly.	Thinking that in a situation there is a "best possible action" is a big assumption that undermines the validity of the statement.	First of all, the sentence is missing a verb. Secondly, but not less importantly, "common sense" is too vague a term to be used to define anything related to a machine.	"Unbiased"? Please refer to all the machine-based discrimination cases popping up everywhere where algorithms are used to classify/evaluate/assess. Of course all the biases of the human programmers and trainers will be included in the system.
	Intelligence is not just about achieving goals. Acquiring knowledge without a specific goal in mind is important.			I don't understand the definition.		The best possible action in a situation may not be the best action in the long run.		Machine Intelligence should function without the need of a human.
		It’s too technically detailed. A definition should be more abstract.				The above definition is correct but does not cover the whole idea of AI		
Yours is too narrow.   For example, GENISAMA is more general but necessary.	Goals are inconsistent.	Your this form is better, but not sufficient.   Needs GENISAMA and more.	The brain itself can be optimal conditioned on its experience and GENISAMA. See Weng's optimality proof in International Journal of Intelligence Science 	AI is the mental capability required by the species in its life environments, that is appropriate at each age.	Must involve species, environment, age, and computational resource available.	Your definition is too narrow.  Intelligence is for competition of the species.	Your answer is static.   As I stated above, AI is measured by mental age.	Human is a part of the environment.  "Service to humans" can be inconsistent, such as serving for two sides during a war.
						Sometimes, humans take irrational choices that are good for them.		
								
								The requirement for "guidance" and providing "help" is not necessary.
I believe there is a part missing on automated improvement and that there is too much focus on the contemporary framework to think on these problems.		I feel that this is formulating AI as: "That which we currently cannot do yet." It is bound not to stand the test of time. I feel that once we will have solved the above task, we will notice how it still is lacking and was not the essence of intelligence after all.	The thing lacking with this definition is the skipping of an embodiment in the word computer system. I would replace `computer system` with `system` or `machine`.	Achieving goals in environments is the reinforcement learning approach. Human intelligence is not necessarily interested in achieving goals. In fact, it usually spends most of its time not trying to achieve a goal.	While a watchmaker is building entities appropriate to their task and with foresight to its environment, I would not say he is working on artificial intelligence. This definition seems too vague or too broad.	Ratio is the currency of science, like power is the currency of politics or aesthetics the currency of art. It is obvious why scientists would think rationality and acting optimally would institute intelligence, but it is missing out a chunk of the human spectrum.	This definition does not define intelligence, but what it would mean for machines to match human intelligence. 	
This is also a partial goal of AI.	But only in cases of situations when there are no effective algorithms, otherwise a direct program may achieve such goals. 	Real time in human terms is not needed for some forms of intelligence. 	Human intelligence is unnecessary restriction and is quite limited.	Efficient pragmatic general intelligence may serve as a specific index to measure machine intelligence. 	Roughly OK but not all situations in which intelligence is evident are related to foresight.	Action is not always required, unless action=change of internal state. 	Machine intelligence is different than human, why to mix it here?	This is one of its aspects and goals. 
								
				I'm not sure that the amount of computational resources should be taken into account 		it's also hard to define what's the "best possible action" 		
	Flexibility -wide range of domains- shall not be part of the definition but a consequence.	Constraints -time, resources- are not relevant for a definition of intelligence. 	Achieving human-level intelligence in machines is not the goal. The goal is general intelligence realized in machines (not necessarily human--like).	As said before, achieving goals is central.		Intelligence is related with processing information for a purpose. Action is a central issue. 		Agree in general but humans shall be removed from the definition.
			This is only a subset of intelligence		this captures aspects of intelligence, but does not pin it down sufficiently.	Under this definition, humans would not be classed as intelligent; this definition captures a utility maximizing agent only.		
Just redefines intelligence in terms of other vague terms. And even if we could define them, this just seems too binary: a system can either reason or it can't. A good definition should at least hint at a measure for degrees of intelligence. 	This is pretty good. As mentioned in the first two answers, I prefer "solving problems" to "achieving goals", but otherwise this is pretty good and concise. 	Most of this definition (AIKR and relative rationality) are great. However, I don't like "adapting to the environment". It implies a way of dealing with goals that is either irrational (i.e. failing to protect the integrity of its goals) or prone to wire heading (which would quickly lead to something very unintelligent).	Very verbose, comparisons to human intelligence that render it useless for defining (human) intelligence, and emphasizing sub-optimality. It's not false that an intelligent system does not need to generate optimal behavior. It's just that the more optimal it is, the more intelligent and vice-versa. 	Too complicated and dependent on the body (e.g. a stronger body increases your ability to achieve some goals). It's better to focus on solving problems than achieving goals for this reason. I also don't think we should (always) be normalizing by computational resources used: if I thought someone was more intelligent than me, knowing it's because they have more grey/white matter would make me go "oh, that's why" rather than "oh, I guess they aren't more intelligent after all". The only thing that matters for determining the intelligence of a system/agent according to a goal/problem is how much that goal/problem penalizes resource usage.  We could perhaps evaluate the "intelligence" of *algorithms* or (cognitive) *architectures*, in which case needing less resources to achieve the same thing would make an algorithm more "intelligent". I do like that this definition hints at a distribution over goals and environments. 	It sounds alright, but what do "appropriately" and "with foresight" really mean?	The notion of a rational agent is pretty close to intelligence, but I would focus on thought rather than action: an agent that can come up with amazing problem solving plans for *me* (i.e. a human body) would be very intelligent, even if its own capacity to act is severely limited. 	I'm not a huge fan of the comparison to human intelligence, or that it contains other vague terms like common sense, learning, reasoning and planning. I do like it defines intelligence as information processing. 	(Machine) intelligence should not be defined in term of how well it can serve another entity. This is frankly ridiculous. 
Make it possible for whom? Humans or machines?		The term „rationality“ is quite rich and often used in a normative setting; cognitive psychology provides many examples where rationality seems to be on the sidelines when it comes to guiding behavior, see e.g. the work on heuristics by Kahneman et al.	Human cognition at large is very messy. Human behavior relies on affective coding of stimuli salience, which is fast and flexible and at the core of most other cognitive functions (deliberation, decision making). Trying to replicate the whole „set“ might get entangled too deeply into the biological constraints of being human. 		This is quite vague.	The focus on action might not capture the need for appropriate information processing prior to taking action, i.e. differentiating between relevant and irrelevant  information, smart and efficient information retrieval, etc.		See above: rationality/ heuristics/ affective coding
This defines the FIELD of study, not intelligence per se. However: Is a machine intelligent if it can perceive, reason, and act? Depends on how well it does that and the evaluation criteria are not mentioned. 	Basically fine but less detailed than the two I fully agreed to.	So, a machine/person is by definition not intelligent if it has sufficient knowledge and resources? That would make the intelligence of a system dependent of the task... the question of whether I have enough money to buy me a book would make me 'unintelligent' (enough knowledge and resources to answer perfectly)? 	Yeah, similar to the two above. Maybe a bit less straightforward. Reads less like a definition, more like a description. 	Very academic, not suitable to human intelligence (in my view - does not incorporate bodily limits of, e.g., comp resources, signal processing, etc.). Might or might not be suitable for AI. Hard to say. What exactly are "prior distributions over goal and environment space"? Can someone be intelligent without any prior? What about analogies which are used whenever there is not much of a prior?	Same as above. It is not a very good definition in the sense of sharply delineating intelligent from non-intelligent systems but it gives the main dimensions along which systems can be evaluated with respect to their intelligence.	This shifts the problem of defining AI to the definition of 'best possible'. If that were possible (I am not sure here), I would agree. However, what is 'best possible', e.g., for a human with limited sensing, time, energy might not be 'best possible' for a machine. Plus, we will always have the balance between 'best possible for some individual' and 'best possible for groups of humans' such as societies. So: Definition of 'best possible' is impossible, but if it were possible, the definition would make sense. So, I am neutral about this one.	I think that this is as good as it gets. Of course, the definition of "common sense" is missing, but at least it indicates that 'sense' is dependent on a society evaluating it and that evaluation criteria should be somehow shared by many to call it 'common sense'. So, given my above statement that I believe that there is no single definition containing all 'intelligent' systems and excluding non-intelligent systems, I think that this is as good as it can be.	There is no unbiased guidance. AI will be first programmed by humans and rely on data which might or might not be biased towards different things. Furthermore, a machine is not only intelligent if it serves humans. The definition of intelligence cannot only be based on the interaction between the system and humans.
not strongly agree, because it is not clear that all intelligence is computation 	this is extremely broad, so it's hard to disagree. But is intelligence *only* the ability to achieve goals?	not all intelligence is about making generalizations, nor is about rationality 				not all intelligent behavior can be classified as "rational" (i.e. following logical, syntactic inference rules)		intelligence should not (only) be measured relative to what is useful for humans. A mosquito is intelligent, but it (presumably) is not beneficial to humans and also not rational (= symbolic)
Much too narrow.	Not bad but pretty vague - we need to say a lot more about what a goal is. 	I like this definition - it’s got broad scope and these are sensible scientific and technical challenges. I would have given it 5 except there are some key things missing, like ability to plan, communicate, perceive etc.	I am going to give strong support for this because I think the current focus on "AI = machine learning" is too limiting and damaging to the field. However, I don’t think that we should set human intelligence as the gold standard - there are probably lots of alien forms of intelligence that are brilliant at tasks that humans aren’t evolved for.	Well there are lots of potential measurements of general intelligence, not just statistical ones, and perhaps we should be measuring a range of different capabilities rather than being focused on "goals".	This is really one of the standard definitions (AI is the study of how to make machines behave in ways that we say are "intelligent" if people show that behavior) which is widely criticized for begging the question of what human intelligence is. I like the inclusion of "foresight" though it’s also rather ambiguous for use in a definition that is intended to be precise.	Without a definition of  "rational" and "best" this definition will simply be a source of confusion and disputation. 	I might have given this a 5, except for the inclusion of "common sense" which is another concept which is vague and bound to lead to dispute.	No, this is almost silly. My fridge gives me guidance about when I have left the door open or some of its contents are going out of date but I don’t think we would call that AI.
								
I would add "learning"			Optimality is computationally infeasible even for humans			Rational behavior is only a part of what I would consider "intelligence"		I would say "good" outcome and I do not like the term "rational", which does not have a clear definition. 
				too long and complex	circular definition	much too limited view of intelligence		too limited
	And this is why artificial intelligence is still different than natural intelligence.	Too many claims in a single question. Some are right, some are wrong (e.g., real time).	Same observation as previously. Since some of the affirmations are right and other wrong, I disagree.	Because the capacities of the intelligent machines is still limited by the computational power of the machine, which is different than the way the human brain works.	The goal is not to make machines intelligent in the human sense. It could be simply to develop an expert system that helps us to make diagnostic or forecast a phenomenon.	Because “artificial rationality” could an alternative terminology to “artificial intelligence”.	This is the point. Machines are able to perform specific things better than humans, but they do not have our versatility to deal with unpredictable situations (plural makes the difference).	Yeah, I believe this is one the goals.
AI is not a "study of computations", it is technology	Straightforward	There is a confusion here between "intelligent" and "opportunistic"/"versatile"	There is a confusion here between "intelligent" and "opportunistic"/"versatile"	Intelligent is intelligent, the amount of computational resources is irrelevant	"Appropriately" is too loose a term.	The statement offers 2 non-overlapping definitions : rational and effective	"Common sense" is subjective and a black box. Shouldn't be part of the list.	I'd rather see a more focused term than "rational"
								
								
			I agree with the non-optimal, but this definition places the human as the reference relatively to what should be performed, independently of context, capabilities or resources (that might all limit the need and usefulness of being highly flexible).	Depends on how computational resources are accounted, as the way they are sometimes accounted for IA and natural beings sometimes differ.		Highly depends on the definition of "rational action", for which there is no consensus either based on discipline / approach / goal.		Ok from a "rational" perspective, but simply not in phase with the definitions of intelligence I commit to.
So long as we define computations as rhetorical.	Yes, though goals are often emergent goals.	Yes, since intelligence is best demonstrated when all available facts are known, in order to generate new facts. But we are always working within an availability heuristic.	Intelligence is hardly ever efficient.	Yes, this overview presents intelligence as context-driven.	Foresight is a recursive function.	Intelligence is also prudence--and prudence may not always be the best available choice, pragmatically speaking. Computer intelligence is highly prudent.	Ethical questions and values are always embedded in human and machine learning situations. 	Non-human actors may have their own teleological purposes. 
								
						what's the definition of intelligence in the first place?		
	Logic and abstract reasoning and inference does not require environment per se.				In my opinion, Intelligence cannot be restricted to an environment. 	This definition is somehow restrictive since it does not take into account other aspects like the emotional intelligence.		Guidance is provided in a context, so there will be always some (cultural, gender, age, regional) bias
								
						Because it all depends on how it has been trained		
								
						What does best mean		I don't think AI is unbiased
				I see no purpose in thus version				Too narrow, too PC, dull
This definition has the advantage of simplicity, but intelligence deserves something more... A signature verification program has these properties, does it have intelligence? No...	well, not enough...	Although this definition is not so abstract it gathers a lot of important properties of intelligence	This definition gathers a lot of important properties of intelligence, however, ties it to humans...	A common person cannot understand this definition	to function appropriately... Ambiguous...	It is good, although a little ambiguous when talking about "best possible action"	Although this definition is not so abstract it gathers a lot of important properties of intelligence	intelligence for me is not about only to serve.
			This probably comes closest to my definition of all above.			Intelligent humans can often fail to take the best possible action. Intelligence to me is more about flexibility and adaptiveness than being optimal for every task.	I like this definition more than the ones below for the emphasis on "wide range of ... domains". Intelligence to me is the ability to adapt flexibly to new goals and to set one’s own goals – not just to "function appropriately and with foresight in its environment". There are existing machines that can "function appropriately and with foresight" in very limited environments, but I wouldn't call them intelligent.	Intelligent behavior can be defined whether or not that behavior provides "guidance and service to humans".
				minimizing computational resources seems a good way of avoiding calling brute-force approaches intelligent, but still seems to be a bit outside a core definition of machine intelligence				the ability to provide 'guidance' to me seems not to fit into a core definition of machine intelligence (even though most machine intelligences will have this purpose)
This presumes that intelligence, or some machine subset of it, is inherently computational.	This is sort of onto something in functional terms, but elides "intelligence" with its measurement.	Again, a reasonable functional definition.	This takes "intelligence" to be quintessentially a human characteristic, which is imitated in AI/MI development. It's too normative.	This seems a reasonable functional definition.	This elides the research programme of developing AI with the object, AI, itself.	It may be right in broad terms, but it seems _very_ broad, and there is no criterion given for the "goodness" of an action here.	This may be an epistemic criterion for how we might recognize MI if we met it, but again, it's not a definition.	This seems to describe a possible use for MI, rather than defining MI per se.
This sounds not bad, I would maybe replace "computations" with "processes", to avoid a round of debate of what is computation and what not. 	...meaning intelligence implies presence of goals and agents; the definitions hide a lot "under the rug".  	This sounds correct... This could be the "ultimate goal" of AI, whereas I would count intermediate steps as AI as well. 	I neither think such general-purpose systems are needed or possible. We also find quite different capabilities in different humans... 	Seems too specific to a particular (probabilistic, computational) framework; hardly insightful for real-world systems...	This is not very impressive, but sounds right	Well, I consider myself more intelligent than a rock and I definitely don't always take the best possible action (in contrast to the rock). What about exploration / exploitation trade-off? Seems a bit too narrow...  	I believe there's a lot intelligence, even in biological systems, that is not "general intelligence of humans": ability to orient and move in an environment like a bee or an ant is also a valuable goal for MI, e.g. 	I find this definition a bit too narrow. Outcomes not obligatory need to be optimal -- could just be better than without MI, could also be for a single task / circumstance, don't need to be rational and unbiased, just serve the task.  
								
	Same as previously.	I like that learning is included.	Again too focused on comparing to humans.			Too focused on action. Should include also other kinds of decisions.	Pretty ok except for the part about “matching humans”.	Too vague, Mainly about the outcome and not much about the process.
Studying "computations that make it possible to perceive, reason, and act" is studying AI, not AI itself.	It is important to have the ability to transfer learning from one environment to another and operate in known and unknown spaces	Perhaps also needs some reference to transfer of knowledge from other tasks	I don't agree with the human centric pose of this definition	Probably too narrow for general AI. There likely many are systems out there which in a confined goal space already meet this.		This view is too based in standard economic theory. It has been shown the vast majority of humans do not act rationally in many situations. Does this not make them intelligent? See "Choices, Values and Frames", by Kahneman and Tversky.	We should not define machine intelligence from a human centric view point. Whilst a machine that can match humans could be deemed intelligent, this is not the only possible route.	Whether or not machines are defined as intelligent has nothing to do with their relationship to humans. That is important for defining the goals we wish to build into AI systems such that if/when they become intelligent, is their creators we make sure they are beneficial for humankind. It should not be used to define whether they are intelligent.
								
					Defining the development of AI as AI is counter-intuitive and confusing.			
Make possible by whom?	Too narrow and needs defining an agent (human, artificial and what is an agent?)	Good except the last sentence, what is relative rationality? Do not make a definition in terms that are not well defined and understood	Good except but it does not need to generate optimal behavior	Too confusing	It is somehow a cyclic definition	Too narrow		
								
								
								
Simple, crisp, and clear.				I don't like the "prior distributions" (would change that into prior knowledge/training data) and the second part of the efficient .pragmatic general intelligence...	AI is not an "activity".		This is a (semi) definition of human intelligence. 	Requires definitions of "agents", and "optimal outcomes"
								
Agree with the definition, but it is a bit too general.	I generally agree with this, but still too general.	a too complex definition, although the elements said are correct.	What is said is correct, but this again is a too complex definition.	The definition is too much tied with some modeling approaches. It should be more general.	I agree with this definition, but it is too vague.	I would not exclude non-rational (human) intelligence.	I do believe that there are differences between human intelligence and machine intelligence. There are strengths in human intelligence and vice versa. Machine intelligence does not have to match human intelligence on all aspects, especially in common sense reasoning.	I agree with the statement that machine intelligence provide services to humans, but the services are not restricted to rational and unbiased ones.
								
Perceive and act are important, "reason" has the connotation of logical reasoning, I think the inference is to be seen much broader.								should intelligence be rational?
Too general - it would cover also non-AI systems.	Much too general - applies to all kinds of technical systems.	Even if the "real time" requirement appears to me a bit strong, this definition covers the essence of the AI systems I know of except that it lacks a goal orientation.	This is too anthropomorphic.	It doesn't take "knowledge" into account.	There are too general terms: appropriately, with foresight, environment	Then there would be almost no AI solution of a problem, where we are often satisfied with near-optimal solutions. 	If "common sense" would be absent I would agree - we are still far away from common sense in general...	Rational agent, but "optimal" is wrong.
								
too vague	general intelligence seems unnecessary; no human makes only intelligent decisions	gets close to the core of how I understand human intelligence	don't like "general" intelligence requirement	sounds good except for "general"	This goes in the right direction, could be expanded a bit	"Rational" seems like a good direction; maybe "provide reasons for a decision" is needed?	"matching" seems to restrictive; what is "general" intelligence?	Too vague
good but the reasoning is not essential for me	It is quite broad, but if there is more consensus, it is fine with me.	Quite specific; but if there is more consensus, it is fine with me.	Too specific and too much referring to human intelligence	too many elements in the definition. It might be good for researchers but not for a broader audience	short and meaningful definition, that I like	this definition is based on the notion of 'rational action'. This is not simple to define. Moreover it is too restrictive for me.	too restrictive if it  includes common sense explicitly	good but too specific for me in order to be my best choice
This is way too general	And how does one measure an agent's ability to do that?	Many of these definitions seem to overlap and it's hard to tell why there are 3 versions of the same thing and if there is meant to be a difference between them or not				Many of these definitions are too general to be of use		
This describes AI as a "study" rather than as a thing. A better statement would be "the study of AI is the study of ..."	This definition is basically the same as a prior one, less the "pragmatic" modifier. A person or machine may be very good at achieving goals, without , e.g., understanding.	This seems more like a definition of intuition and inference. That is, intelligence is the ability to draw inferences from the least amount of resources. But in common use, intelligence connotes having a high level of knowledge.	This definition seems to come closest (of those provided) to describing what we commonly mean by intelligence. I would drop the first sentence and generalize the description to say "an intelligent [thing?]", that is, whether organic or inorganic.	Not sure why the qualifier "pragmatic" is there. Skirts the issue of general intelligence by simply equating it with goal achievement.	Not sure how to define "appropriately" (again, normative). Vague. First part seems superfluous.	Intelligence is not the same as rationality. People act irrationally, but predictably. Psychopaths may be highly intelligent, but not highly rational.	Mixes normative concepts (common sense) with skill and knowledge based concepts.	MI is not limited to the service of humans and "optimal" is vague-for example, it could be an irrational outcome that does not optimize across all factors. This is a judgment call, not "intelligence" (measuring ability).
"Make it possible" seems vague.						By this definition, many humans are "unintelligent."  Is that what we want?  Rationality seems to overly constrict the scope.  It excludes many creative endeavors that fall under intelligence.		Why would intelligence have "service to humans" as part of its definition?  This relates to how the intelligence would be used, not what the intelligence is.
This seems broad enough to encompass the definition but also feels too broad	 A bit too simplistic			Planners do this purely - are they intelligent? Is DFS intelligent?	Too narrow a definition	Clearly humans are not intelligent then	Close - although does not necessarily have to match humans in every aspect	Not necessarily unbiased, not necessarily optimal
				A good description of AI today. But general intelligence should be more than this.	Not wrong, but nonspecific		"Common sense" is too vague and I don't think general intelligence should be necessary to define machine intelligence.	The words "rational", "unbiased", and "optimal" explain our desired use of machine intelligence well, and for better or worse highlight the subjective nature of deciding what "unbiased", "service", and "optimal" really mean.
			I like that it doesn’t require generating optimal behavior	Way too complicated	Environments vary	Don’t like the agent actually tasking actions		
								
	Does not define AI per se.	OK, but too long, lack the strength of a crisp clear definition of a field.	I disapprove of the reference to human	Same remark about "general intelligence". Further, this definition is unnecessarily complicated, not very readable and does not shed light on the research field being defined.	This is a more neutral definition, but it decomposes into a definition of "intelligence", which is not needed for defining AI.	OK, but does not define AI per se.	The reference to human intelligence is a bias; it also leads into more ideological and much less scientific concerns. The qualifier "general" for intelligence is misleading and unclear.	This sentence is too vague to provide a good definition
								
	Maybe. Current AIs are pretty good at specific, narrow tasks and can act with intelligence within those domains. That makes them pragmatic and useful, but not generally intelligent.	I like the emphasis on adaptivity			I worry about how "environment" is often narrowly defined in modern AI research.	Humans often act irrationally.		That's more of a definition of what makes computing useful.
Still too vague	Too vague		Like not optimality.	Sounds like gobbledygook	Too vague	"best" is relative.	Don't like word "match"	"optimal" is ridiculous.
Acting does not require intelligence 	Goal complexity not defined	Evolution is intelligent by this definition 	Human reference irrelevant 	Goal complexity not included	Appropriately irrelevant 	Complexity not included, and need not be related to rational action	Matching irrelevant 	Human guidance and service does not require intelligence nor is necessary 
This simply shifts the definition of intelligence to the definition of reason. Arguably one could hard-code a rules-based system to achieve the above goals in its domain, yet it would be easily classified as non-intelligent.	It is acceptable for a system to be brilliant in a single domain but unable to significantly adapt to new domains, yet still be intelligent. Many humans satisfy this description. See definitions for "crystallized" and "fluid" intelligence for more information.	Excellent definition. Needs more work to define "adapting" (likely requiring the inclusion of goals). Insufficient knowledge and resources is key, and likely the very motivating force behind our own evolution of intelligence. Relative rationality removes the problems with above definitions requiring "rational and unbiased" before one may be intelligent.	This is a wonderful goal, but building a system that is absolutely alien to human thought, works only in a specific domain, and cannot (for instance) acquire new languages/communication forms could definitely still meet intelligence tests and seem indistinguishable from our current (N=1) definition of intelligence as human. Far too restrictive. We should not even be seeking to just emulate ourselves; we are not the sole definition of intelligence. Reference all the worlds' creatures.	Achieving goals is indeed a key feature of intelligence (as well as achieving the goal of setting useful next-step goals i.e. self-direction). Efficiency is a key element to bring into the discussion: a slime-mold can solve a maze, but it does so by exploring every possible state and down-selecting the states that lead to the solution. In this way, it may appear to be solving problems, but in reality it is simply brute-forcing its way to an optimum while consuming unnecessary resources. Evolution itself can be said to be "intelligent" in this same way; however, a simple study of the fossil record shows it is absolutely not. 	This isn't a bad definition, though it focuses unnecessarily on prediction. A system that could brilliantly reason only on the data it's been trained on would still be enormously useful and would seem quite intelligent indeed. "Function appropriately" while vague, is getting closer to the real issue.	Again, just shifts the definition of intelligence to the definition of rational and "best possible." Clumsy and unenlightening.	Matching humans is an absurd goal. If we could make a machine intelligence that could even function as highly as a dog would be a societally-transformative technology with unbounded, wonderful applications. The term "common sense" is actually offensive and usually refers to a set of common behaviors in humans that have caused untold damage. "Common sense" has included racism, sexism, and sacrificing one's children to a volcano. 	Rational and unbiased guidance is a very loaded phrase. If we are to hold humans as the mark to achieve, we are absolutely not entirely rational and not even close to unbiased. We cannot even often agree about what is bias vs. what is a learned observation. Yet we provide excellent service to the community and are measurably useful. I believe "rational and unbiased" here is an expression of a desired quality of the final product by this individual, not at all a definition. If we had a roughly human-level artificial intelligence that expressed "typical human" biases, this would be incredibly useful and transcendentally transformative to our culture. Humans would fail this definition of intelligence!
could add to this, but good avoidance of how and open to perceive/act being speech/language modes	would like to define range but ok on need for adaptability, learning etc.	well, resources are always limited so am not convinced this view adds much	not convinced that this must be human-centered (and human itself covers a very wide range not defined here: babies, drunk, comatose etc.) but there is a point hidden in here on specialist vs generalist	don’t see why computational resources have a place here: isn’t that an implementation issue or are there deeper computability concerns?		rational here are at least two possible meanings: logical, and explainable. there are time and places when "irrational" action is appropriate (creative, fast etc.)	are we making assumptions about human to mean white male western etc.? what  about social intelligence, non-human (ape, etc.)	too many undefined terms: in what way rational? bias has it uses, why at the service of humans (only) transparency is missing, what about learning and adaptation?
						It could be if a decision always implies an action (think it is not necessarily the case)	Actual architectures are far to show general intelligence. 	Human is the center of the concept. Outcomes of any (Intelligent) Machine must be seen as a tool (assistance).
								
Direct, encompassing and simple, with an explicit recognition of the computational aspect. 	Goal pursuit is too restrictive. 	More a definition of how rather than what. 	"Intelligence" is just one, and not the most prominent of "cognitive abilities we find in humans"	Not crisp	Vague	Begs the question of rationality. 	"Matching human intelligence" is not an operationalizable, testable claim	
A photoelectric detector perceive, "reason" and act, but not so intelligently. Same goes for any detector-actuator circuit. Not fair to use "reason" while defining intelligence, btw.	First, intelligence is not a measuring device, rather "can be measured as". Also, "Common sense" is a kind of intelligence that doesn't need goals (except "keep being alive" if any). Last, suicide as a policy gives the agent to achieve a goal (being dead) in a wide range of environments.	It is not a true definition but a wish-list of what it should be able to do.	"Exhibits the full range of the cognitive capabilities we found in humans" is using "intelligence is being as intelligent as we are". Not a definition for me.	Not sure what you mean with "Efficient pragmatic general intelligence".	Saying "an engine enables the car to move" doesn't define the engine itself but what it is good at, but I agree with what is said here, but it doesn't define intelligence.	Just add a definition of "best possible action" as the one giving the agent access to a set of available futures with the higher entropy and you almost have it all! (Alexander Wissner-Gross, 2013, Causal Entropic Forces).	It does challenge a lot of disciplines, but at the same time, the answer would be so simple and general to unify them in a simple form, like the 2nd law of thermodynamics or the least action principles can -and do- define physic system evolution as a whole.	Base a definition on "service to human" is too narrow sighted.
			Again: not as much as a definition as properties that intelligent systems must have. 			Intelligent agents take what they believe to be the best possible action in a situation. 	I don't see this as a definition. Rather, as a property that intelligent machines should be able to fulfill. 	This definitions presumes that a machine is intelligent only as far as it services humans. Consistently with previous answers in this questionnaire, I think that a unified definition is possible both for artificial and human intelligence (the difference between both being that the former is carried out by machines designed by us, and the latter by Homo sapiens). With this in mind, I think that this definition by Nilsson is a very bad starting point. 
				Too technical/convoluted		Too vague		
								
Perception as subjective experience may not be necessary in some implementations of AI	The focus in intelligence should be on the process and not in the result. An entity that has been taught the right answers may perform better than one that finds the answers by itself	Computational resource limitations are per se included in any real world activity, how important is to make it explicit?	It seems to imply the presence of emotional component of human behavior (dealing with social environments). That should not be a goal in AI. 		“appropriately and with foresight” too vague terms. The entities should act ‘consequently’ with acquired knowledge. How far into the future we need to look for the consequences of our actions is a very difficult question (at which humans are mostly failing)	Agree, with the caveat that the best possible action may be computationally unreachable 	Same mechanisms of information processing apply, in principle, to general intelligence for all	The goal to serve humans adds unnecessary confusion, AI should be aligned with human values but focused on the concrete task assigned
This definition specifies computation instead of allowing a dependency on the natural world, and the functions to perform are reasonably narrow. I would include a demand for recursion around these 3, including something like 'perceiving the results of its previous actions', thus requiring the possibility to learn at any meta level.	An atomic bomb can achieve its goals without much regard for its environment. There is no 'adapt' in this definition.	This aspect of a finite agent in a potentially infinite environment is very important to distinguish brute force from intelligent strategies. The 'learn from experience' is also essential to adapt to environments that evolve into uncharted territory.	Describing the cognitive abilities we're after by referring to 'human cognitive abilities' and listing a few examples of human behavior does not get us much further to narrow down the scope.	This definition using 'the capability of an agent to achieve goals in environments, relative to prior distributions over goal and environment space' would make much of the evolving biological world qualify as such as a pragmatic general intelligence.	Because 'enabling an entity to function appropriately and with foresight in its environment' includes gigantic parts of the animal world, unless you narrow the definition of foresight and/or have that definition include concepts that are similarly difficult to define as intelligence.	Best possible action' cannot be defined rationally. Bayesian reasoning always depends on previous assumptions and 'best' is always according to someone's agenda.	The question is if our intelligence can be defined by the ability to meet complex information-processing challenges or only by our successful survival thus far. Also, any modern C++ compiler is vastly better at planning to meet complex information-processing challenges than most humans, although arguably not across a wide range of domains.	It is unclear 'if rational, unbiased guidance [...] to help humans achieve optimal outcomes [...]' can exist at all. If it is important to have a definition of machine intelligence, it should not be dependent on something as fuzzy as 'optimal outcomes for humans'.
	Intelligence alone may not be sufficient for wide range problem solving.							This definition implies that machine intelligence is solely for the purpose of service to humans. I do not agree with this. Intelligence for the agent should subject to its own goals irrespective of how this goals serve humans or not.
							Does not allow for a spectrum of intelligence.	Does not allow for autonomy.
learning is missing, and knowledge accumulation/update, though maybe thinks of those as part of reasoning and perceiving and acting..	achieving goals' is part of what (artificial) intelligence is, but not the whole.	I'd not say 'insufficient' but limited.		Focusing on goals/actions seems too limiting, but it's good to stress limited resources.	The use of 'foresight' seems weird and 'appropriately' requires more elaboration.	Not necessarily.  I would consider a conversation or a good question answering system intelligent, and there is still much work to be done to get them to human level, but I wouldn't characterize such tasks as taking rational actions.	This is ultimate goal of AI research.	unbiased is not necessary, nor optimal, nor perhaps service or guidance to humans, and 'rational' could  be too restrictive (depends on exactly what rational means) ..
								
Concise			Falling asleep reading it, you missed timeliness in the definition	Too wordy	Too vague	Intelligence  without "soul" - talking about outputs without consideration of inputs	It does not cater for highly specific subsets of intelligence	I do believe having a definition gives one a reference without discouraging slightly altered definitions. The drawback is we cannot define human intelligence particularly well.
No, it is not "the study". I.e., that would be the science of AI, not AI itself				should not so narrowly refer to prior distributions		too narrowly focused on only "rational" action.	Incomplete language	Too narrow
				too much to unpack here. Prior distributions? Seems too niche to certain types of AI systems.	Not sure I'd characterize AI as an activity. But this one is both tidy and broad enough to work. Also, I think prediction is a necessary aspect of the definition. "Foresight" captures the spirit there for me.			intelligence can still be biased (not ideal, but not mutually exclusive). Also, intelligence is agnostic to its service to humans. Human ability to benefit from it should be coupled,  but it is not one and the same.
reductive	vague	on to something but too high-brow	indeed! focus on replicating human intelligence	process description in mathematical terms nice	process oriented and too vague	reductive		Too limited a definition. Seems like a definition of machine science rather than intelligence. 
What does reason mean? If you cannot act are you still intelligent?	Not everything is goal driven	Not sure about the relative rationality part	Parts could also be intelligent	Not everything is goal-driven though, or is the absence of goals a goal itself?	I like it, but this also means that a plant is intelligent, which they may be	Are irrational agents not also intelligent?	Doesn't have to match humans, but a notion of generality and common sense (base self-preservation intelligence at least) is valid	Service to humans and optimal outcomes should not be a requirement
								
I appreciate the fact that this definition includes perception and action, which are  central to what the brain was evolved to do. 	This definition highlights an agent's ability to adapt to diverse circumstances.	This incorporates learning, adaptation, and working in real time.  As a roboticist, I appreciate the focus on the constraint of finite processing capacity. 			This definition doesn't explicitly mention learning and adaptation, though they are implied in the ability to act "appropriately". 	Humans are not rational most of the time, yet we view them as intelligent. 	An ability to learn and adapt are essential components in any definition of AI.	All guidance is biased in some way.
								
								
								
Human thinking is also a computation of some higher intelligence.					Intelligence depends on the outcome of an action. For example, if all the info is processed yet, the outcome is bad for humanity/nature, it means there is no intelligence.		if (General Intelligence == Routine work) Yes; else if  (General Intelligence == Creative work) No; else Maybe, it depends on level of complexity, plus, ability to process information and arrive at a optimal decision	Humans are present to evolve their consciousness and machines are here to help to humans in accomplishing this task.
								
Human perception, reason, and action are based on more than objective computation.  To use these terms that already have a meaning in human terms, without further supplying delimiting characteristics, will lead to faulty understanding and unrealistic expectation.  	Good overall definition, but too broad to be useful.  Easily misused.	It keeps intelligence within an objective boundary that can be applied to human, animal, and machine behavior.  	Just because we can, it does not mean we should?  What would be the purpose of such a system, besides just proving we can?	It is effective because it maintains the question in the realm of objective goals and measures.  	This definition is effective except for the term "appropriately" which assumes that appropriate function is based only on cognitive (intelligence) characteristics.	The first part is good in that it first defines the limit of "intelligence." The second part falls apart as it assumes that rational action with by definition be the "best" action.  	This assumes that those that program them have the sum total of all types of common sense and effective ability to learn and reason.  The perspectives and "common sense" of those who do not design and program the systems are ignored.  	Machine intelligence can calculate odds, but not predict future outcome.
Better, but this threatens to be a little bit too instrumental, remains a little bit vague.	Yes, good, fits the one-intelligence view.  You know, I'm going to go 5 on this.  "Achieve goals" is still a little teleological, but we're getting close.	Getting very close now!!!	Getting close!  However, I don't even hold that it needs to be successful!  Nor does it need a full range.  I believe in little bits of intelligence, too.	"Achieve goals" is a little too teleological for my taste.  I sympathize, but feel it needs to be broken down more than that.	These functional answers just push the question around without answering it.	This is true, but the terms here are all loose, "rational", "best", even "situation".	I agree, but an AI simply as intelligent as a dog or a housefly, would still be a great step forward!	Has to be more to it than advising humans, and I'm not even sure you can be rational and unbiased, or would want to be!  Anyway I'm one-intelligence guy, not separate AI and human variations.
Not bad, but partial - lacks a connection to goals. 	I find this the best definition considering its compactness. 	Would be a perfect 5 except it is missing a reference to goals. 	Too wordy. 	General while specific enough to separate it from more broad concepts such as "adaptation"; I like the emphasis on pragmatics - this is very important but often-ignored in discussions about intelligence. 	The first part of this is spot-on, the second singles out "foresight"; while foresight is part of intelligence it is not one of the necessary ones (learning is more important, for instance). 	Rationality is not a useful term with respect to intelligence - too strong a focus on normativeness. 	Anchoring to human intelligence is too restrictive. 	AI should not be restricted to "service to humans" - too restrictive. 
